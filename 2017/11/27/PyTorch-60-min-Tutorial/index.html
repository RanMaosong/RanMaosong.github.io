<!DOCTYPE html>
<html style="display: none;" lang="zh">
    <head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <!--
        © Material Theme
        https://github.com/viosey/hexo-theme-material
        Version: 1.5.0 -->
    <script>
        window.materialVersion = "1.5.0"
        // Delete localstorage with these tags
        window.oldVersion = [
            'codestartv1',
            '1.3.4',
            '1.4.0',
            '1.4.0b1'
        ]
    </script>

    <!-- dns prefetch -->
    <meta http-equiv="x-dns-prefetch-control" content="on">



    <link rel="dns-prefetch" href="https://busuanzi.ibruce.info"/>












    <!-- Title -->
    
    <title>
        
            PyTorch 深度学习:60分钟快速入门 | 
        
        技术闲谈
    </title>

    <!-- Meta & Info -->
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="format-detection" content="telephone=no"/>
    <meta name="theme-color" content="#0097A7">
    <meta name="author" content="茂松">
    <meta name="description" itemprop="description" content="Through studying you will gain success and fortunes will follow. ,">
    <meta name="keywords" content=",PyTorch">

    <!-- Import lsloader -->
    <script>(function(){window.lsloader={jsRunSequence:[],jsnamemap:{},cssnamemap:{}};lsloader.removeLS=function(key){try{localStorage.removeItem(key)}catch(e){}};lsloader.setLS=function(key,val){try{localStorage.setItem(key,val)}catch(e){}};lsloader.getLS=function(key){var val="";try{val=localStorage.getItem(key)}catch(e){val=""}return val};versionString="/*"+(window.materialVersion||"unknownVersion")+"*/";lsloader.clean=function(){try{var keys=[];for(var i=0;i<localStorage.length;i++){keys.push(localStorage.key(i))}keys.forEach(function(key){var data=lsloader.getLS(key);if(window.oldVersion){var remove=window.oldVersion.reduce(function(p,c){return p||data.indexOf("/*"+c+"*/")!==-1},false);if(remove){lsloader.removeLS(key)}}})}catch(e){}};lsloader.clean();lsloader.load=function(jsname,jspath,cssonload,isJs){if(typeof cssonload==="boolean"){isJs=cssonload;cssonload=undefined}isJs=isJs||false;cssonload=cssonload||function(){};var code;code=this.getLS(jsname);if(code&&code.indexOf(versionString)===-1){this.removeLS(jsname);this.requestResource(jsname,jspath,cssonload,isJs);return}if(code){var versionNumber=code.split(versionString)[0];if(versionNumber!=jspath){console.log("reload:"+jspath);this.removeLS(jsname);this.requestResource(jsname,jspath,cssonload,isJs);return}code=code.split(versionString)[1];if(isJs){this.jsRunSequence.push({name:jsname,code:code});this.runjs(jspath,jsname,code)}else{document.getElementById(jsname).appendChild(document.createTextNode(code));cssonload()}}else{this.requestResource(jsname,jspath,cssonload,isJs)}};lsloader.requestResource=function(name,path,cssonload,isJs){var that=this;if(isJs){this.iojs(path,name,function(path,name,code){that.setLS(name,path+versionString+code);that.runjs(path,name,code)})}else{this.iocss(path,name,function(code){document.getElementById(name).appendChild(document.createTextNode(code));that.setLS(name,path+versionString+code)},cssonload)}};lsloader.iojs=function(path,jsname,callback){var that=this;that.jsRunSequence.push({name:jsname,code:""});try{var xhr=new XMLHttpRequest;xhr.open("get",path,true);xhr.onreadystatechange=function(){if(xhr.readyState==4){if(xhr.status>=200&&xhr.status<300||xhr.status==304){if(xhr.response!=""){callback(path,jsname,xhr.response);return}}that.jsfallback(path,jsname)}};xhr.send(null)}catch(e){that.jsfallback(path,jsname)}};lsloader.iocss=function(path,jsname,callback,cssonload){var that=this;try{var xhr=new XMLHttpRequest;xhr.open("get",path,true);xhr.onreadystatechange=function(){if(xhr.readyState==4){if(xhr.status>=200&&xhr.status<300||xhr.status==304){if(xhr.response!=""){callback(xhr.response);cssonload();return}}that.cssfallback(path,jsname,cssonload)}};xhr.send(null)}catch(e){that.cssfallback(path,jsname,cssonload)}};lsloader.iofonts=function(path,jsname,callback,cssonload){var that=this;try{var xhr=new XMLHttpRequest;xhr.open("get",path,true);xhr.onreadystatechange=function(){if(xhr.readyState==4){if(xhr.status>=200&&xhr.status<300||xhr.status==304){if(xhr.response!=""){callback(xhr.response);cssonload();return}}that.cssfallback(path,jsname,cssonload)}};xhr.send(null)}catch(e){that.cssfallback(path,jsname,cssonload)}};lsloader.runjs=function(path,name,code){if(!!name&&!!code){for(var k in this.jsRunSequence){if(this.jsRunSequence[k].name==name){this.jsRunSequence[k].code=code}}}if(!!this.jsRunSequence[0]&&!!this.jsRunSequence[0].code&&this.jsRunSequence[0].status!="failed"){var script=document.createElement("script");script.appendChild(document.createTextNode(this.jsRunSequence[0].code));script.type="text/javascript";document.getElementsByTagName("head")[0].appendChild(script);this.jsRunSequence.shift();if(this.jsRunSequence.length>0){this.runjs()}}else if(!!this.jsRunSequence[0]&&this.jsRunSequence[0].status=="failed"){var that=this;var script=document.createElement("script");script.src=this.jsRunSequence[0].path;script.type="text/javascript";this.jsRunSequence[0].status="loading";script.onload=function(){that.jsRunSequence.shift();if(that.jsRunSequence.length>0){that.runjs()}};document.body.appendChild(script)}};lsloader.tagLoad=function(path,name){this.jsRunSequence.push({name:name,code:"",path:path,status:"failed"});this.runjs()};lsloader.jsfallback=function(path,name){if(!!this.jsnamemap[name]){return}else{this.jsnamemap[name]=name}for(var k in this.jsRunSequence){if(this.jsRunSequence[k].name==name){this.jsRunSequence[k].code="";this.jsRunSequence[k].status="failed";this.jsRunSequence[k].path=path}}this.runjs()};lsloader.cssfallback=function(path,name,cssonload){if(!!this.cssnamemap[name]){return}else{this.cssnamemap[name]=1}var link=document.createElement("link");link.type="text/css";link.href=path;link.rel="stylesheet";link.onload=link.onerror=cssonload;var root=document.getElementsByTagName("script")[0];root.parentNode.insertBefore(link,root)};lsloader.runInlineScript=function(scriptId,codeId){var code=document.getElementById(codeId).innerText;this.jsRunSequence.push({name:scriptId,code:code});this.runjs()};lsloader.loadCombo=function(jslist){var updateList="";var requestingModules={};for(var k in jslist){var LS=this.getLS(jslist[k].name);if(!!LS){var version=LS.split(versionString)[0];var code=LS.split(versionString)[1]}else{var version=""}if(version==jslist[k].path){this.jsRunSequence.push({name:jslist[k].name,code:code,path:jslist[k].path})}else{this.jsRunSequence.push({name:jslist[k].name,code:null,path:jslist[k].path,status:"comboloading"});requestingModules[jslist[k].name]=true;updateList+=(updateList==""?"":";")+jslist[k].path}}var that=this;if(!!updateList){var xhr=new XMLHttpRequest;xhr.open("get",combo+updateList,true);xhr.onreadystatechange=function(){if(xhr.readyState==4){if(xhr.status>=200&&xhr.status<300||xhr.status==304){if(xhr.response!=""){that.runCombo(xhr.response,requestingModules);return}}else{for(var i in that.jsRunSequence){if(requestingModules[that.jsRunSequence[i].name]){that.jsRunSequence[i].status="failed"}}that.runjs()}}};xhr.send(null)}this.runjs()};lsloader.runCombo=function(comboCode,requestingModules){comboCode=comboCode.split("/*combojs*/");comboCode.shift();for(var k in this.jsRunSequence){if(!!requestingModules[this.jsRunSequence[k].name]&&!!comboCode[0]){this.jsRunSequence[k].status="comboJS";this.jsRunSequence[k].code=comboCode[0];this.setLS(this.jsRunSequence[k].name,this.jsRunSequence[k].path+versionString+comboCode[0]);comboCode.shift()}}this.runjs()}})();</script>

    <!-- Import queue -->
    <script>function Queue(){this.dataStore=[];this.offer=b;this.poll=d;this.execNext=a;this.debug=false;this.startDebug=c;function b(e){if(this.debug){console.log("Offered a Queued Function.")}if(typeof e==="function"){this.dataStore.push(e)}else{console.log("You must offer a function.")}}function d(){if(this.debug){console.log("Polled a Queued Function.")}return this.dataStore.shift()}function a(){var e=this.poll();if(e!==undefined){if(this.debug){console.log("Run a Queued Function.")}e()}}function c(){this.debug=true}}var queue=new Queue();</script>

    <!-- Favicons -->
    <link rel="icon shortcut" type="image/ico" href="/img/favicon.png">
    <link rel="icon" sizes="192x192" href="/img/favicon.png">
    <link rel="apple-touch-icon" href="/img/favicon.png">

    <!--iOS -->
    <meta name="apple-mobile-web-app-title" content="Title">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="480">

    <!-- Add to homescreen for Chrome on Android -->
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Add to homescreen for Safari on iOS -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="技术闲谈">

    <!-- Site Verification -->
    
    

    <!-- RSS -->
    

    <!--[if lte IE 9]>
        <link rel="stylesheet" href="/css/ie-blocker.css">

        
            <script src="/js/ie-blocker.zhCN.js"></script>
        
    <![endif]-->

    <!-- Import CSS -->
    
        <style id="material_css"></style><script>if(typeof window.lsLoadCSSMaxNums === "undefined")window.lsLoadCSSMaxNums = 0;window.lsLoadCSSMaxNums++;lsloader.load("material_css","/css/material.min.css?Z7a72R1E4SxzBKR/WGctOA==",function(){if(typeof window.lsLoadCSSNums === "undefined")window.lsLoadCSSNums = 0;window.lsLoadCSSNums++;if(window.lsLoadCSSNums == window.lsLoadCSSMaxNums)document.documentElement.style.display="";}, false)</script>
        <style id="style_css"></style><script>if(typeof window.lsLoadCSSMaxNums === "undefined")window.lsLoadCSSMaxNums = 0;window.lsLoadCSSMaxNums++;lsloader.load("style_css","/css/style.min.css?MKetZV3cUTfDxvMffaOezg==",function(){if(typeof window.lsLoadCSSNums === "undefined")window.lsLoadCSSNums = 0;window.lsLoadCSSNums++;if(window.lsLoadCSSNums == window.lsLoadCSSMaxNums)document.documentElement.style.display="";}, false)</script>

        

    

    

    <!-- Config CSS -->

<!-- Other Styles -->
<style>
  body, html {
    font-family: Roboto, "Helvetica Neue", Helvetica, "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", "微软雅黑", Arial, sans-serif;
    overflow-x: hidden !important;
  }
  
  code {
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  }

  a {
    color: #00838F;
  }

  .mdl-card__media,
  #search-label,
  #search-form-label:after,
  #scheme-Paradox .hot_tags-count,
  #scheme-Paradox .sidebar_archives-count,
  #scheme-Paradox .sidebar-colored .sidebar-header,
  #scheme-Paradox .sidebar-colored .sidebar-badge{
    background-color: #0097A7 !important;
  }

  /* Sidebar User Drop Down Menu Text Color */
  #scheme-Paradox .sidebar-colored .sidebar-nav>.dropdown>.dropdown-menu>li>a:hover,
  #scheme-Paradox .sidebar-colored .sidebar-nav>.dropdown>.dropdown-menu>li>a:focus {
    color: #0097A7 !important;
  }

  #post_entry-right-info,
  .sidebar-colored .sidebar-nav li:hover > a,
  .sidebar-colored .sidebar-nav li:hover > a i,
  .sidebar-colored .sidebar-nav li > a:hover,
  .sidebar-colored .sidebar-nav li > a:hover i,
  .sidebar-colored .sidebar-nav li > a:focus i,
  .sidebar-colored .sidebar-nav > .open > a,
  .sidebar-colored .sidebar-nav > .open > a:hover,
  .sidebar-colored .sidebar-nav > .open > a:focus,
  #ds-reset #ds-ctx .ds-ctx-entry .ds-ctx-head a {
    color: #0097A7 !important;
  }

  .toTop {
    background: #757575 !important;
  }

  .material-layout .material-post>.material-nav,
  .material-layout .material-index>.material-nav,
  .material-nav a {
    color: #757575;
  }

  #scheme-Paradox .MD-burger-layer {
    background-color: #757575;
  }

  #scheme-Paradox #post-toc-trigger-btn {
    color: #757575;
  }

  .post-toc a:hover {
    color: #00838F;
    text-decoration: underline;
  }

</style>


<!-- Theme Background Related-->

    <style>
      body{
        background-color: #F5F5F5;
      }

      /* blog_info bottom background */
      #scheme-Paradox .material-layout .something-else .mdl-card__supporting-text{
        background-color: #fff;
      }
    </style>




<!-- Fade Effect -->

    <style>
      .fade {
        transition: all 800ms linear;
        -webkit-transform: translate3d(0,0,0);
        -moz-transform: translate3d(0,0,0);
        -ms-transform: translate3d(0,0,0);
        -o-transform: translate3d(0,0,0);
        transform: translate3d(0,0,0);
        opacity: 1;
      }

      .fade.out{
        opacity: 0;
      }
    </style>


<!-- Import Font -->
<!-- Import Roboto -->

    <style>
        @font-face {
            font-family: Roboto;
            font-style: normal;
            font-weight: 300;
            src: url(https://lib.baomitu.com/fonts/roboto/roboto-v15-latin-300.eot);
            src: local('Roboto'),local('Roboto-Normal'),url(https://lib.baomitu.com/fonts/roboto/roboto-v15-latin-300.eot?#iefix) format('embedded-opentype'),url(https://lib.baomitu.com/fonts/roboto/roboto-v15-latin-300.woff2) format('woff2'),url(https://lib.baomitu.com/fonts/roboto/roboto-v15-latin-300.woff) format('woff'),url(https://lib.baomitu.com/fonts/roboto/roboto-v15-latin-300.ttf) format('truetype'),url(https://lib.baomitu.com/fonts/roboto/roboto-v15-latin-300.svg#Roboto) format('svg')
        }

        @font-face {
            font-family: Roboto;
            font-style: normal;
            font-weight: regular;
            src: url(https://lib.baomitu.com/fonts/roboto/roboto-v15-latin-regular.eot);
            src: local('Roboto'),local('Roboto-Normal'),url(https://lib.baomitu.com/fonts/roboto/roboto-v15-latin-regular.eot?#iefix) format('embedded-opentype'),url(https://lib.baomitu.com/fonts/roboto/roboto-v15-latin-regular.woff2) format('woff2'),url(https://lib.baomitu.com/fonts/roboto/roboto-v15-latin-regular.woff) format('woff'),url(https://lib.baomitu.com/fonts/roboto/roboto-v15-latin-regular.ttf) format('truetype'),url(https://lib.baomitu.com/fonts/roboto/roboto-v15-latin-regular.svg#Roboto) format('svg')
        }

        @font-face {
            font-family: Roboto;
            font-style: normal;
            font-weight: 500;
            src: url(https://lib.baomitu.com/fonts/roboto/roboto-v15-latin-500.eot);
            src: local('Roboto'),local('Roboto-Normal'),url(https://lib.baomitu.com/fonts/roboto/roboto-v15-latin-500.eot?#iefix) format('embedded-opentype'),url(https://lib.baomitu.com/fonts/roboto/roboto-v15-latin-500.woff2) format('woff2'),url(https://lib.baomitu.com/fonts/roboto/roboto-v15-latin-500.woff) format('woff'),url(https://lib.baomitu.com/fonts/roboto/roboto-v15-latin-500.ttf) format('truetype'),url(https://lib.baomitu.com/fonts/roboto/roboto-v15-latin-500.svg#Roboto) format('svg')
        }
    </style>


<!-- Import Material Icon -->

    <style>
        @font-face {
            font-family: 'Material Icons';
            font-style: normal;
            font-weight: 400;
            src: local('Material Icons'),
            local('MaterialIcons-Regular'),
            url(/fonts/MaterialIcons-Regular.woff2) format('woff2'),
            url(/fonts/MaterialIcons-Regular.woff) format('woff'),
            url(/fonts/MaterialIcons-Regular.ttf) format('truetype');
        }
    </style>




    <!-- Import jQuery -->
    
        <script>lsloader.load("jq_js","/js/jquery.min.js?qcusAULNeBksqffqUM2+Ig==", true)</script>
    

    <!-- The Open Graph protocol -->
    <meta property="og:url" content="http://yoursite.com">
    <meta property="og:type" content="blog">
    <meta property="og:title" content="PyTorch 深度学习:60分钟快速入门 | 技术闲谈">
    <meta property="og:image" content="http://yoursite.com/img/favicon.png" />
    <meta property="og:description" content="Through studying you will gain success and fortunes will follow. ,">
    <meta property="og:article:tag" content="PyTorch"> 

    
        <meta property="article:published_time" content="Mon Nov 27 2017 00:20:22 GMT+0800" />
        <meta property="article:modified_time" content="Tue Mar 20 2018 22:03:01 GMT+0800" />
    

    <!-- The Twitter Card protocol -->
    <meta name="twitter:title" content="PyTorch 深度学习:60分钟快速入门 | 技术闲谈">
    <meta name="twitter:description" content="Through studying you will gain success and fortunes will follow. ,">
    <meta name="twitter:image" content="http://yoursite.com/img/favicon.png">
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:url" content="http://yoursite.com" />

    <!-- Add canonical link for SEO -->
    
        <link rel="canonical" href="http://yoursite.com/2017/11/27/PyTorch-60-min-Tutorial/index.html" />
    

    <!-- Structured-data for SEO -->
    
        


<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "mainEntityOfPage": "http://yoursite.com/2017/11/27/PyTorch-60-min-Tutorial/index.html",
    "headline": "PyTorch 深度学习:60分钟快速入门",
    "datePublished": "Mon Nov 27 2017 00:20:22 GMT+0800",
    "dateModified": "Tue Mar 20 2018 22:03:01 GMT+0800",
    "author": {
        "@type": "Person",
        "name": "茂松",
        "image": {
            "@type": "ImageObject",
            "url": "/img/avatar.png"
        },
        "description": "吾生也有涯，而知也无涯."
    },
    "publisher": {
        "@type": "Organization",
        "name": "技术闲谈",
        "logo": {
            "@type":"ImageObject",
            "url": "/img/favicon.png"
        }
    },
    "keywords": ",PyTorch",
    "description": "Through studying you will gain success and fortunes will follow. ,",
}
</script>


    

    <!-- Analytics -->
    
    
    

    <!-- Custom Head --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    

    

</head>


    
        <body id="scheme-Paradox" class="lazy">
            <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="material-layout  mdl-js-layout has-drawer is-upgraded">
                

                <!-- Main Container -->
                <main class="material-layout__content" id="main">

                    <!-- Top Anchor -->
                    <div id="top"></div>

                    
                        <!-- Hamburger Button -->
                        <button class="MD-burger-icon sidebar-toggle">
                            <span class="MD-burger-layer"></span>
                        </button>
                    

                    <!-- Post TOC -->

    
    <!-- Back Button -->
    <!--
    <div class="material-back" id="backhome-div" tabindex="0">
        <a class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon"
           href="#" onclick="window.history.back();return false;"
           target="_self"
           role="button"
           data-upgraded=",MaterialButton,MaterialRipple">
            <i class="material-icons" role="presentation">arrow_back</i>
            <span class="mdl-button__ripple-container">
                <span class="mdl-ripple"></span>
            </span>
        </a>
    </div>
    -->


    <!-- Left aligned menu below button -->
    
    
    <button id="post-toc-trigger-btn"
        class="mdl-button mdl-js-button mdl-button--icon">
        <i class="material-icons">format_list_numbered</i>
    </button>

    <ul class="post-toc-wrap mdl-menu mdl-menu--bottom-left mdl-js-menu mdl-js-ripple-effect" for="post-toc-trigger-btn" style="max-height:80vh; overflow-y:scroll;">
        <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#一、PyTorch-是什么"><span class="post-toc-number">1.</span> <span class="post-toc-text">一、PyTorch 是什么</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#开始"><span class="post-toc-number">2.</span> <span class="post-toc-text">开始</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#张量（Tensors"><span class="post-toc-number">2.0.1.</span> <span class="post-toc-text">张量（Tensors)</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#操作"><span class="post-toc-number">2.0.2.</span> <span class="post-toc-text">操作</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#numpy桥"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">numpy桥</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#把Torch张量转换为numpy数组"><span class="post-toc-number">2.1.1.</span> <span class="post-toc-text">把Torch张量转换为numpy数组</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#把numpy数组转换为torch张量"><span class="post-toc-number">2.1.2.</span> <span class="post-toc-text">把numpy数组转换为torch张量</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#CUDA张量"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">CUDA张量</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#二、Autograd-自动求导-automatic-differentiation"><span class="post-toc-number">3.</span> <span class="post-toc-text">二、Autograd: 自动求导(automatic differentiation)</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#变量-Variable"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">变量(Variable)</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#梯度-Gradients"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">梯度(Gradients)</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#三、神经网络"><span class="post-toc-number">4.</span> <span class="post-toc-text">三、神经网络</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#定义网络"><span class="post-toc-number">4.1.</span> <span class="post-toc-text">定义网络</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#损失函数"><span class="post-toc-number">4.2.</span> <span class="post-toc-text">损失函数</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#反向传播"><span class="post-toc-number">4.3.</span> <span class="post-toc-text">反向传播</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#更新权重"><span class="post-toc-number">4.4.</span> <span class="post-toc-text">更新权重</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#四、训练一个分类器"><span class="post-toc-number">5.</span> <span class="post-toc-text">四、训练一个分类器</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#关于数据"><span class="post-toc-number">5.1.</span> <span class="post-toc-text">关于数据</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#训练一个图像分类器"><span class="post-toc-number">5.2.</span> <span class="post-toc-text">训练一个图像分类器</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#1-加载和归一化CIFAR0"><span class="post-toc-number">5.2.1.</span> <span class="post-toc-text">1. 加载和归一化CIFAR0</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-定义一个卷积神经网络"><span class="post-toc-number">5.2.2.</span> <span class="post-toc-text">2. 定义一个卷积神经网络</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3-定义损失函数和优化器"><span class="post-toc-number">5.2.3.</span> <span class="post-toc-text">3. 定义损失函数和优化器</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#4-训练网络"><span class="post-toc-number">5.2.4.</span> <span class="post-toc-text">4. 训练网络</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#5-在测试集上测试网络"><span class="post-toc-number">5.2.5.</span> <span class="post-toc-text">5. 在测试集上测试网络</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#在GPU上训练"><span class="post-toc-number">5.3.</span> <span class="post-toc-text">在GPU上训练</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#在多GPU上训练"><span class="post-toc-number">5.4.</span> <span class="post-toc-text">在多GPU上训练</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#接下来做什么"><span class="post-toc-number">5.5.</span> <span class="post-toc-text">接下来做什么?</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#五、数据并行-选读"><span class="post-toc-number">6.</span> <span class="post-toc-text">五、数据并行(选读)</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#导入和参数"><span class="post-toc-number">6.1.</span> <span class="post-toc-text">导入和参数</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#虚拟数据集"><span class="post-toc-number">6.2.</span> <span class="post-toc-text">虚拟数据集</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#简单模型"><span class="post-toc-number">6.3.</span> <span class="post-toc-text">简单模型</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#创建一个模型和数据并行"><span class="post-toc-number">6.4.</span> <span class="post-toc-text">创建一个模型和数据并行</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#运行模型"><span class="post-toc-number">6.5.</span> <span class="post-toc-text">运行模型</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#结果"><span class="post-toc-number">6.6.</span> <span class="post-toc-text">结果</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#总结"><span class="post-toc-number">6.7.</span> <span class="post-toc-text">总结</span></a></li></ol></li></ol>
    </ul>
    




<!-- Layouts -->

    <!-- Post Module -->
    <div class="material-post_container">

        <div class="material-post mdl-grid">
            <div class="mdl-card mdl-shadow--4dp mdl-cell mdl-cell--12-col">

                <!-- Post Header(Thumbnail & Title) -->
                
    <!-- Paradox Post Header -->
    
        
            <!-- Random Thumbnail -->
            <div class="post_thumbnail-random mdl-card__media mdl-color-text--grey-50">
            <script type="text/ls-javascript" id="post-thumbnail-script">
    var randomNum = Math.floor(Math.random() * 19 + 1);

    $('.post_thumbnail-random').attr('data-original', '/img/random/material-' + randomNum + '.png');
    $('.post_thumbnail-random').addClass('lazy');
</script>

        
    
            <p class="article-headline-p">
                PyTorch 深度学习:60分钟快速入门
            </p>
        </div>





                
                    <!-- Paradox Post Info -->
                    <div class="mdl-color-text--grey-700 mdl-card__supporting-text meta">

    <!-- Author Avatar -->
    <div id="author-avatar">
        <img src="/img/avatar.png" width="44px" height="44px" alt="Author Avatar"/>
    </div>
    <!-- Author Name & Date -->
    <div>
        <strong>茂松</strong>
        <span>11月 27, 2017</span>
    </div>

    <div class="section-spacer"></div>

    <!-- Favorite -->
    <!--
        <button id="article-functions-like-button" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon btn-like">
            <i class="material-icons" role="presentation">favorite</i>
            <span class="visuallyhidden">favorites</span>
        </button>
    -->

    <!-- Qrcode -->
    

    <!-- Tags (bookmark) -->
    
    <button id="article-functions-viewtags-button" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon">
        <i class="material-icons" role="presentation">bookmark</i>
        <span class="visuallyhidden">bookmark</span>
    </button>
    <ul class="mdl-menu mdl-menu--bottom-right mdl-js-menu mdl-js-ripple-effect" for="article-functions-viewtags-button">
        <li class="mdl-menu__item">
        <a class="post_tag-link" href="/tags/PyTorch/">PyTorch</a>
    </ul>
    

    <!-- Share -->
    
        <button id="article-fuctions-share-button" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon">
    <i class="material-icons" role="presentation">share</i>
    <span class="visuallyhidden">share</span>
</button>
<ul class="mdl-menu mdl-menu--bottom-right mdl-js-menu mdl-js-ripple-effect" for="article-fuctions-share-button">
    

    
        
            <!-- Busuanzi Views -->
            <a class="post_share-link" href="#">
                <li class="mdl-menu__item">
                    <span id="busuanzi_container_page_pv">
                        <span id="busuanzi_value_page_pv"></span>&nbsp;浏览量
                    </span>
                </li>
            </a>
        
    

    <!-- Share Weibo -->
    
        <a class="post_share-link" href="http://service.weibo.com/share/share.php?appkey=&title=PyTorch 深度学习:60分钟快速入门&url=http://yoursite.com/2017/11/27/PyTorch-60-min-Tutorial/index.html&pic=http://yoursite.com/img/favicon.png&searchPic=false&style=simple" target="_blank">
            <li class="mdl-menu__item">
                分享到微博
            </li>
        </a>
    

    <!-- Share Twitter -->
    
        <a class="post_share-link" href="https://twitter.com/intent/tweet?text=PyTorch 深度学习:60分钟快速入门&url=http://yoursite.com/2017/11/27/PyTorch-60-min-Tutorial/index.html&via=茂松" target="_blank">
            <li class="mdl-menu__item">
                分享到 Twitter
            </li>
        </a>
    

    <!-- Share Facebook -->
    
        <a class="post_share-link" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2017/11/27/PyTorch-60-min-Tutorial/index.html" target="_blank">
            <li class="mdl-menu__item">
                分享到 Facebook
            </li>
        </a>
    

    <!-- Share Google+ -->
    
        <a class="post_share-link" href="https://plus.google.com/share?url=http://yoursite.com/2017/11/27/PyTorch-60-min-Tutorial/index.html" target="_blank">
            <li class="mdl-menu__item">
                分享到 Google+
            </li>
        </a>
    

    <!-- Share LinkedIn -->
    
        <a class="post_share-link" href="https://www.linkedin.com/shareArticle?mini=true&url=http://yoursite.com/2017/11/27/PyTorch-60-min-Tutorial/index.html&title=PyTorch 深度学习:60分钟快速入门" target="_blank">
            <li class="mdl-menu__item">
                分享到 LinkedIn
            </li>
        </a>
    

    <!-- Share QQ -->
    
        <a class="post_share-link" href="http://connect.qq.com/widget/shareqq/index.html?site=技术闲谈&title=PyTorch 深度学习:60分钟快速入门&summary=Through studying you will gain success and fortunes will follow. ,&pics=http://yoursite.com/img/favicon.png&url=http://yoursite.com/2017/11/27/PyTorch-60-min-Tutorial/index.html" target="_blank">
            <li class="mdl-menu__item">
                分享到 QQ
            </li>
        </a>
    

    <!-- Share Telegram -->
    
        <a class="post_share-link" href="https://telegram.me/share/url?url=http://yoursite.com/2017/11/27/PyTorch-60-min-Tutorial/index.html&text=PyTorch 深度学习:60分钟快速入门" target="_blank">
            <li class="mdl-menu__item">
                分享到 Telegram
            </li>
        </a>
    
</ul>

    

    
        
        <!-- Busuanzi Views -->
        <a class="post_share-link" href="#">
                <span id="busuanzi_container_page_pv">
                    hexo busuanzi<span id="busuanzi_value_page_pv"></span>&nbsp;
                    浏览量
                </span>
        </a>
        
    
</div>

                

                <!-- Post Content -->
                <div id="post-content" class="mdl-color-text--grey-700 mdl-card__supporting-text fade out">
    
        <p>此教程为翻译<a href="http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="noopener">官方地址</a></p>
<p><a href="https://ranmaosong.github.io/2017/11/27/PyTorch-60-min-Tutorial/" target="_blank" rel="noopener">Github 地址</a><br><a href="http://www.jianshu.com/p/889dbc684622" target="_blank" rel="noopener">简书地址</a><br><a href="http://blog.csdn.net/u014630987/article/details/78669051" target="_blank" rel="noopener">CSDN地址</a></p>
<p><strong>作者:</strong><a href="http://soumith.ch/" target="_blank" rel="noopener">Soumith Chintala</a></p>
<p>本教程的目标:</p>
<ul>
<li>深入理解PyTorch张量库和神经网络</li>
<li>训练一个小的神经网络来分类图片</li>
</ul>
<p>这个教程假设你熟悉numpy的基本操作。</p>
<p><strong>注意</strong></p>
<p>请确保<code>torch</code>和<code>torchvision</code>包已经安装。</p>
<h2 id="一、PyTorch-是什么"><a href="#一、PyTorch-是什么" class="headerlink" title="一、PyTorch 是什么"></a>一、PyTorch 是什么</h2><p>他是一个基于Python的科学计算包，目标用户有两类</p>
<ul>
<li>为了使用GPU来替代numpy</li>
<li>一个深度学习援救平台：提供最大的灵活性和速度</li>
</ul>
<h2 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h2><h4 id="张量（Tensors"><a href="#张量（Tensors" class="headerlink" title="张量（Tensors)"></a>张量（Tensors)</h4><p>张量类似于numpy的ndarrays，不同之处在于张量可以使用GPU来加快计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>
<p>构建一个未初始化的5*3的矩阵：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.Tensor(5, 3)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<p>输出 ：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1.00000e-10 *</span><br><span class="line"> -1.1314  0.0000 -1.1314</span><br><span class="line">  0.0000  0.0000  0.0000</span><br><span class="line">  0.0000  0.0000  0.0000</span><br><span class="line">  0.0000  0.0000  0.0000</span><br><span class="line">  0.0000  0.0000  0.0000</span><br><span class="line">[torch.FloatTensor of size 5x3]</span><br></pre></td></tr></table></figure>
<p>构建一个随机初始化的矩阵</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(5, 3)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0.2836  0.6710  0.5146</span><br><span class="line"> 0.8842  0.2821  0.7768</span><br><span class="line"> 0.3409  0.0428  0.6726</span><br><span class="line"> 0.1982  0.6950  0.6040</span><br><span class="line"> 0.0272  0.6586  0.3555</span><br><span class="line">[torch.FloatTensor of size 5x3]</span><br></pre></td></tr></table></figure>
<p>获取矩阵的大小：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x.size())</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([5, 3])</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong></p>
<p><code>torch.Size</code>实际上是一个元组，所以它支持元组相同的操作。</p>
<h4 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h4><p>张量上的操作有多重语法形式，下面我们以加法为例进行讲解。</p>
<p><strong>语法1</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = torch.rand(5, 3)</span><br><span class="line">print(x + y)</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0.9842  1.5171  0.8148</span><br><span class="line"> 1.1334  1.6540  1.5739</span><br><span class="line"> 0.9804  1.1647  0.4759</span><br><span class="line"> 0.6232  0.2689  1.0596</span><br><span class="line"> 1.0777  1.1705  0.3206</span><br><span class="line">[torch.FloatTensor of size 5x3]</span><br></pre></td></tr></table></figure>
<p><strong>语法二</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(torch.add(x, y))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0.9842  1.5171  0.8148</span><br><span class="line"> 1.1334  1.6540  1.5739</span><br><span class="line"> 0.9804  1.1647  0.4759</span><br><span class="line"> 0.6232  0.2689  1.0596</span><br><span class="line"> 1.0777  1.1705  0.3206</span><br><span class="line">[torch.FloatTensor of size 5x3]</span><br></pre></td></tr></table></figure>
<p><strong>语法三</strong>：给出一个输出向量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">result = torch.Tensor(5, 3)</span><br><span class="line">torch.add(x, y, out=result)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0.9842  1.5171  0.8148</span><br><span class="line"> 1.1334  1.6540  1.5739</span><br><span class="line"> 0.9804  1.1647  0.4759</span><br><span class="line"> 0.6232  0.2689  1.0596</span><br><span class="line"> 1.0777  1.1705  0.3206</span><br><span class="line">[torch.FloatTensor of size 5x3]</span><br></pre></td></tr></table></figure>
<p><strong>语法四：</strong>原地操作（in-place）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 把x加到y上</span><br><span class="line">y.add_(x)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0.9842  1.5171  0.8148</span><br><span class="line"> 1.1334  1.6540  1.5739</span><br><span class="line"> 0.9804  1.1647  0.4759</span><br><span class="line"> 0.6232  0.2689  1.0596</span><br><span class="line"> 1.0777  1.1705  0.3206</span><br><span class="line">[torch.FloatTensor of size 5x3]</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong></p>
<p>任何在原地(in-place)改变张量的操作都有一个’_‘后缀。例如<code>x.copy_(y), x.t_()</code>操作将改变<code>x</code>.</p>
<p>你可以使用所有的numpy索引操作。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x[:, 1])</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1.5171</span><br><span class="line">1.6540</span><br><span class="line">1.1647</span><br><span class="line">0.2689</span><br><span class="line">1.1705</span><br><span class="line">[torch.FloatTensor of size 5]</span><br></pre></td></tr></table></figure>
<p><strong>稍后阅读</strong></p>
<p><a href="http://pytorch.org/docs/torch" target="_blank" rel="noopener">这里</a>描述了一百多种张量操作，包括转置，索引，数学运算，线性代数，随机数等。</p>
<h3 id="numpy桥"><a href="#numpy桥" class="headerlink" title="numpy桥"></a>numpy桥</h3><p>把一个torch张量转换为numpy数组或者反过来都是很简单的。</p>
<p>Torch张量和numpy数组将共享潜在的内存，改变其中一个也将改变另一个。</p>
<h4 id="把Torch张量转换为numpy数组"><a href="#把Torch张量转换为numpy数组" class="headerlink" title="把Torch张量转换为numpy数组"></a>把Torch张量转换为numpy数组</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(5)</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line"> 1</span><br><span class="line"> 1</span><br><span class="line"> 1</span><br><span class="line"> 1</span><br><span class="line">[torch.FloatTensor of size 5]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b = a.numpy()</span><br><span class="line">print(b)</span><br><span class="line">print(type(b))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ 1.  1.  1.  1.  1.]</span><br><span class="line">&lt;class &apos;numpy.ndarray&apos;&gt;</span><br></pre></td></tr></table></figure>
<p>通过如下操作，我们看一下numpy数组的值如何在改变。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a.add_(<span class="number">1</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line">[torch.FloatTensor of size 5]</span><br><span class="line"></span><br><span class="line">[ 2.  2.  2.  2.  2.]</span><br></pre></td></tr></table></figure>
<h4 id="把numpy数组转换为torch张量"><a href="#把numpy数组转换为torch张量" class="headerlink" title="把numpy数组转换为torch张量"></a>把numpy数组转换为torch张量</h4><p>看看改变numpy数组如何自动改变torch张量。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">np.add(a, <span class="number">1</span>, out=a)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[ 2.  2.  2.  2.  2.]</span><br><span class="line"></span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line">[torch.DoubleTensor of size 5]</span><br></pre></td></tr></table></figure>
<p>所有在CPU上的张量，除了字符张量，都支持在numpy之间转换。</p>
<h3 id="CUDA张量"><a href="#CUDA张量" class="headerlink" title="CUDA张量"></a>CUDA张量</h3><p>使用<code>.cuda</code>函数可以将张量移动到GPU上。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># let us run this cell only if CUDA is available</span><br><span class="line">if torch.cuda.is_available():</span><br><span class="line">	x = x.cuda()</span><br><span class="line">	y = y.cuda()</span><br><span class="line">	x + y</span><br></pre></td></tr></table></figure>
<p><strong>脚本总运行时间:</strong>0.003秒</p>
<p><a href="http://pytorch.org/tutorials/_downloads/tensor_tutorial.py" target="_blank" rel="noopener">Python源码</a></p>
<p><a href="http://pytorch.org/tutorials/_downloads/tensor_tutorial.ipynb" target="_blank" rel="noopener">Jupyter源码</a></p>
<h2 id="二、Autograd-自动求导-automatic-differentiation"><a href="#二、Autograd-自动求导-automatic-differentiation" class="headerlink" title="二、Autograd: 自动求导(automatic differentiation)"></a>二、Autograd: 自动求导(automatic differentiation)</h2><p>PyTorch 中所有神经网络的核心是<code>autograd</code>包.我们首先简单介绍一下这个包,然后训练我们的第一个神经网络.</p>
<p><code>autograd</code>包为张量上的所有操作提供了自动求导.它是一个运行时定义的框架,这意味着反向传播是根据你的代码如何运行来定义,并且每次迭代可以不同.</p>
<p>接下来我们用一些简单的示例来看这个包</p>
<h3 id="变量-Variable"><a href="#变量-Variable" class="headerlink" title="变量(Variable)"></a>变量(Variable)</h3><p><em><code>autograd.Variable</code></em>是<code>autograd</code>包的核心类.它包装了<em>张量</em>(<code>Tensor</code>),支持几乎所有的张量上的操作.一旦你完成你的前向计算,可以通过<code>.backward()</code>方法来自动计算所有的梯度.</p>
<p>你可以通过<code>.data</code>属性来访问变量中的原始张量,关于这个变量的梯度被计算放入<code>.grad</code>属性中</p>
<p><img src="/images/Variable.png" alt="Variable"></p>
<p>对自动求导的实现还有一个非常重要的类,即<em>函数</em>(<code>Function</code>).</p>
<p><em>变量</em>(<code>Variable</code>)和<em>函数</em>(<code>Function</code>)是相互联系的,并形成一个非循环图来构建一个完整的计算过程.每个变量有一个<code>.grad_fn</code>属性,它指向创建该变量的一个<code>Function</code>,用户自己创建的变量除外,它的<code>grad_fn</code>属性为None.</p>
<p>如果你想计算导数,可以在一个变量上调用<code>.backward()</code>.如果一个<code>Variable</code>是一个标量(它只有一个元素值),你不必给该方法指定任何的参数,但是该<code>Variable</code>有多个值,你需要指定一个和该变量相同形状的的<code>grad_output</code>参数(查看API发现实际为<code>gradients</code>参数).</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.autograd import Variable</span><br></pre></td></tr></table></figure>
<p>创建一个变量:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.ones(2, 2), requires_grad=True)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Variable containing:</span><br><span class="line"> 1  1</span><br><span class="line"> 1  1</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br></pre></td></tr></table></figure>
<p>在变量上执行操作:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = x + 2</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Variable containing:</span><br><span class="line"> 3  3</span><br><span class="line"> 3  3</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br></pre></td></tr></table></figure>
<p>因为<code>y</code>是通过一个操作创建的,所以它有<code>grad_fn</code>,而<code>x</code>是由用户创建,所以它的<code>grad_fn</code>为None.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(y.grad_fn)</span><br><span class="line">print(x.grad_fn)</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;torch.autograd.function.AddConstantBackward object at 0x7faa6f3bdd68&gt;</span><br><span class="line">None</span><br></pre></td></tr></table></figure>
<p>在y上执行操作</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">z = y * y * 3</span><br><span class="line">out = z.mean()</span><br><span class="line"></span><br><span class="line">print(z, out)</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Variable containing:</span><br><span class="line"> 27  27</span><br><span class="line"> 27  27</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br><span class="line"> Variable containing:</span><br><span class="line"> 27</span><br><span class="line">[torch.FloatTensor of size 1]</span><br></pre></td></tr></table></figure>
<h3 id="梯度-Gradients"><a href="#梯度-Gradients" class="headerlink" title="梯度(Gradients)"></a>梯度(Gradients)</h3><p>现在我们来执行反向传播,<code>out.backward()</code>相当于执行<code>out.backward(torch.Tensor([1.0]))</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out.backward()</span><br></pre></td></tr></table></figure>
<p>输出<code>out</code>对<code>x</code>的梯度d(out)/dx:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Variable containing:</span><br><span class="line"> 4.5000  4.5000</span><br><span class="line"> 4.5000  4.5000</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br></pre></td></tr></table></figure>
<p>你应该得到一个值全为4.5的矩阵,我们把变量<code>out</code>称为$o$,则$o=\frac{1}{4}\sum_iz_i,z_i=3(x_i + 2)^2,z_i\vert_{x_i=1}=27$,因此$\frac{\partial o}{\partial x_i}=\frac{3}{2}(x_i+2),\frac{\partial o}{\partial x_i}=\frac{9}{2}=4.5$</p>
<p>我们还可以用自动求导做更多有趣的事!</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(3)</span><br><span class="line">x = Variable(x, requires_grad=True)</span><br><span class="line"></span><br><span class="line">y = x * 2</span><br><span class="line">while y.data.norm() &lt; 1000:</span><br><span class="line">    y = y * 2</span><br><span class="line"></span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Variable containing:</span><br><span class="line"> 682.4722</span><br><span class="line">-598.8342</span><br><span class="line"> 692.9528</span><br><span class="line">[torch.FloatTensor of size 3]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">gradients = torch.FloatTensor([0.1, 1.0, 0.0001])</span><br><span class="line">y.backward(gradients)</span><br><span class="line"></span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Variable containing:</span><br><span class="line">  102.4000</span><br><span class="line"> 1024.0000</span><br><span class="line">    0.1024</span><br><span class="line">[torch.FloatTensor of size 3]</span><br></pre></td></tr></table></figure>
<p>稍后阅读:</p>
<p>关于<code>Variable</code>和<code>Function</code>的文档在<a href="http://pytorch.org/docs/autograd" target="_blank" rel="noopener">http://pytorch.org/docs/autograd</a>.</p>
<p>以上脚本的总的运行时间为0.003秒.</p>
<p><a href="http://pytorch.org/tutorials/_downloads/autograd_tutorial.py" target="_blank" rel="noopener">Python源码</a></p>
<p><a href="http://pytorch.org/tutorials/_downloads/autograd_tutorial.ipynb" target="_blank" rel="noopener">Jupyter源码</a></p>
<h2 id="三、神经网络"><a href="#三、神经网络" class="headerlink" title="三、神经网络"></a>三、神经网络</h2><p>可以使用<code>torch.nn</code>包来构建神经网络.</p>
<p>你已知道<code>autograd</code>包,<code>nn</code>包依赖<code>autograd</code>包来定义模型并求导.一个<code>nn.Module</code>包含各个层和一个<code>forward(input)</code>方法,该方法返回<code>output</code>.</p>
<p>例如,我们来看一下下面这个分类数字图像的网络.</p>
<p><img src="/images/mnist.png" alt="mnist"></p>
<p align="center">convnet</p>

<p>他是一个简单的前馈神经网络,它接受一个输入,然后一层接着一层的输入,直到最后得到结果.</p>
<p>神经网络的典型训练过程如下:</p>
<ol>
<li>定义神经网络模型,它有一些可学习的参数(或者权重);</li>
<li>在数据集上迭代;</li>
<li>通过神经网络处理输入;</li>
<li>计算损失(输出结果和正确值的差距大小)</li>
<li>将梯度反向传播会网络的参数;</li>
<li>更新网络的参数,主要使用如下简单的更新原则:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weight = weight - learning_rate * gradient</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="定义网络"><a href="#定义网络" class="headerlink" title="定义网络"></a>定义网络</h3><p>我们先定义一个网络</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line">import torch.nn </span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line">class Net(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        # 1 input image channel, 6 output channels, 5*5 square convolution</span><br><span class="line">        # kernel</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(1, 6, 5)</span><br><span class="line">        self.conv2 = nn.Conv2d(6, 16, 5)</span><br><span class="line">        # an affine operation: y = Wx + b</span><br><span class="line">        self.fc1 = nn.Linear(16 * 5 * 5, 120)</span><br><span class="line">        self.fc2 = nn.Linear(120, 84)</span><br><span class="line">        self.fc3 = nn.Linear(84, 10)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        # max pooling over a (2, 2) window</span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))</span><br><span class="line">        # If size is a square you can only specify a single number</span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), 2)</span><br><span class="line">        x = x.view(-1, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        return x</span><br><span class="line">    </span><br><span class="line">    def num_flat_features(self, x):</span><br><span class="line">        size = x.size()[1:] # all dimensions except the batch dimension</span><br><span class="line">        num_features = 1</span><br><span class="line">        for s in size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        return num_features</span><br><span class="line">net = Net()</span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Net (</span><br><span class="line">  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="line">  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="line">  (fc1): Linear (400 -&gt; 120)</span><br><span class="line">  (fc2): Linear (120 -&gt; 84)</span><br><span class="line">  (fc3): Linear (84 -&gt; 10)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>你只需定义<code>forward</code>函数,<code>backward</code>函数(计算梯度)在使用<code>autograd</code>时自动为你创建.你可以在<code>forward</code>函数中使用<code>Tensor</code>的任何操作.</p>
<p><code>net.parameters()</code>返回模型需要学习的参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">params = list(net.parameters())</span><br><span class="line">print(len(params))</span><br><span class="line">for param in params:</span><br><span class="line">	print(param.size())</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">10</span><br><span class="line">torch.Size([6, 1, 5, 5])</span><br><span class="line">torch.Size([6])</span><br><span class="line">torch.Size([16, 6, 5, 5])</span><br><span class="line">torch.Size([16])</span><br><span class="line">torch.Size([120, 400])</span><br><span class="line">torch.Size([120])</span><br><span class="line">torch.Size([84, 120])</span><br><span class="line">torch.Size([84])</span><br><span class="line">torch.Size([10, 84])</span><br><span class="line">torch.Size([10])</span><br></pre></td></tr></table></figure>
<p><code>forward</code>的输入和输出都是<code>autograd.Variable</code>.<em>注意</em>:这个网络(LeNet)期望的输入大小是32*32.如果使用MNIST数据集来训练这个网络,请把图片大小重新调整到32*32.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input = Variable(torch.randn(1, 1, 32, 32))</span><br><span class="line">out = net(input)</span><br><span class="line">print(out)</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Variable containing:</span><br><span class="line">-0.0536 -0.0548 -0.1079  0.0030  0.0521 -0.1061 -0.1456 -0.0095  0.0704  0.0259</span><br><span class="line">[torch.FloatTensor of size 1x10]</span><br></pre></td></tr></table></figure>
<p>将所有参数的梯度缓存清零,然后进行随机梯度的的反向传播.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(1, 10))</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong></p>
<ol>
<li><code>torch.nn</code> 只支持小批量输入,整个<code>torch.nn</code>包都只支持小批量样本,而不支持单个样本</li>
<li>例如,<code>nn.Conv2d</code>将接受一个4维的张量,每一维分别是sSamples <em> nChannels </em> Height <em> Width(样本数\</em>通道数*高*宽).</li>
<li>如果你有单个样本,只需使用<code>input.unsqueeze(0)</code>来添加其它的维数.</li>
</ol>
<p>在继续之前,我们回顾一下到目前为止见过的所有类.</p>
<p><strong>回顾</strong></p>
<ul>
<li><code>torch.Tensor</code>-一个多维数组</li>
<li><code>autograd.Variable</code>-包装一个<code>Tensor</code>,记录在其上执行过的操作.除了拥有<code>Tensor</code>拥有的API,还有类似<code>backward()</code>的API.也保存关于这个向量的梯度.</li>
<li><code>nn.Module</code>-神经网络模块.封装参数,移动到GPU上运行,导出,加载等</li>
<li><code>nn.Parameter</code>-一种变量,当把它赋值给一个<code>Module</code>时,被自动的注册为一个参数.</li>
<li><code>autograd.Function</code>-实现一个自动求导操作的前向和反向定义,每个变量操作至少创建一个函数节点,(Every <code>Variable</code> operation, creates at least a single <code>Function</code> node, that connects to functions that created a <code>Variable</code> and <em>encodes its history</em>.)</li>
</ul>
<p>现在,我们包含了如下内容:</p>
<ul>
<li>定义一个神经网络</li>
<li>处理输入和调用<code>backward</code></li>
</ul>
<p>剩下的内容:</p>
<ul>
<li>计算损失值</li>
<li>更新神经网络的权值</li>
</ul>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>一个损失函数接受一对<code>(output, target)</code>作为输入(output为网络的输出,target为实际值),计算一个值来估计网络的输出和目标值相差多少.</p>
<p>在<code>nn</code>包中有几种不同的损失函数.一个简单的损失函数是:<code>nn.MSELoss</code>,他计算输入(个人认为是网络的输出)和目标值之间的均方误差.</p>
<p>例如:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">out = net(input)</span><br><span class="line">target = Variable(torch.arange(1, 11))  # a dummy target, for example</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(out, target)</span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Variable containing:</span><br><span class="line"> 38.1365</span><br><span class="line">[torch.FloatTensor of size 1]</span><br></pre></td></tr></table></figure>
<p>现在,你反向跟踪<code>loss</code>,使用它的<code>.grad_fn</code>属性,你会看到向下面这样的一个计算图:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d</span><br><span class="line">      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear</span><br><span class="line">      -&gt; MSELoss</span><br><span class="line">      -&gt; loss</span><br></pre></td></tr></table></figure>
<p>所以, 当你调用<code>loss.backward()</code>,整个图关于损失被求导,图中所有变量将拥有<code>.grad</code>变量来累计他们的梯度.</p>
<p>为了说明,我们反向跟踪几步:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(loss.grad_fn)  # MSELoss</span><br><span class="line">print(loss.grad_fn.next_functions[0][0])  # Linear</span><br><span class="line">print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;torch.autograd.function.MSELossBackward object at 0x7fb3c0dcf4f8&gt;</span><br><span class="line">&lt;torch.autograd.function.AddmmBackward object at 0x7fb3c0dcf408&gt;</span><br><span class="line">&lt;AccumulateGrad object at 0x7fb3c0db79e8&gt;</span><br></pre></td></tr></table></figure>
<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>为了反向传播误差,我们所需做的是调用<code>loss.backward()</code>.你需要清除已存在的梯度,否则梯度将被累加到已存在的梯度.</p>
<p>现在,我们将调用<code>loss.backward()</code>,并查看conv1层的偏置项在反向传播前后的梯度.</p>
<p>输出(官网的例子)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">conv1.bias.grad before backward</span><br><span class="line">Variable containing:</span><br><span class="line"> 0numpy</span><br><span class="line"> 0</span><br><span class="line"> 0</span><br><span class="line"> 0</span><br><span class="line"> 0</span><br><span class="line"> 0</span><br><span class="line">[torch.FloatTensor of size 6]</span><br><span class="line"></span><br><span class="line">conv1.bias.grad after backward</span><br><span class="line">Variable containing:</span><br><span class="line">-0.0317</span><br><span class="line">-0.1682</span><br><span class="line">-0.0158</span><br><span class="line"> 0.2276</span><br><span class="line">-0.0148</span><br><span class="line">-0.0254</span><br><span class="line">[torch.FloatTensor of size 6]</span><br></pre></td></tr></table></figure>
<p>本人运行输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">conv1.bias.grad before backward</span><br><span class="line">None</span><br><span class="line">conv1.bias.grad after backward</span><br><span class="line">Variable containing:</span><br><span class="line"> 0.0011</span><br><span class="line"> 0.1170</span><br><span class="line">-0.0012</span><br><span class="line">-0.0204</span><br><span class="line">-0.0325</span><br><span class="line">-0.0648</span><br><span class="line">[torch.FloatTensor of size 6]</span><br></pre></td></tr></table></figure>
<p>不同之处在于backward之前不同,官网示例的梯度为0,而实际运行出来却是None.</p>
<p>现在我们已知道如何使用损失函数.</p>
<p><strong>稍后阅读</strong></p>
<p>神经网络包包含了各种用来构成深度神经网络构建块的模块和损失函数,一份完整的文档查看<a href="http://pytorch.org/docs/nn" target="_blank" rel="noopener">这里</a></p>
<p><strong>唯一剩下的内容:</strong></p>
<ul>
<li>更新网络的权重</li>
</ul>
<h3 id="更新权重"><a href="#更新权重" class="headerlink" title="更新权重"></a>更新权重</h3><p>实践中最简单的更新规则是随机梯度下降(SGD)．</p>
<script type="math/tex; mode=display">weight = weight - learning\_rate * gradient</script><p>我们可以使用简单的Python代码实现这个规则.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = 0.01</span><br><span class="line">for f in net.parameters():</span><br><span class="line">	f.data.sub_(f.grad.data * learning_rate)</span><br></pre></td></tr></table></figure>
<p>然而,当你使用神经网络是,你想要使用各种不同的更新规则,比如SGD,Nesterov-SGD,Adam, RMSPROP等.为了能做到这一点,我们构建了一个包<code>torch.optim</code>实现了所有的这些规则.使用他们非常简单:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import torch.optim as optim</span><br><span class="line"></span><br><span class="line"># create your optimizer</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=0.01)</span><br><span class="line"></span><br><span class="line">#in your trainning loop:</span><br><span class="line">optimizer.zero_grad()  # zero the gradient buffers</span><br><span class="line">output = net(input)</span><br><span class="line">loss = criter(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step() # does the update</span><br></pre></td></tr></table></figure>
<p><strong>脚本总运行时间: 0.367秒</strong></p>
<p><a href="http://pytorch.org/tutorials/_downloads/neural_networks_tutorial.py" target="_blank" rel="noopener">Python源码</a></p>
<p><a href="http://pytorch.org/tutorials/_downloads/neural_networks_tutorial.ipynb" target="_blank" rel="noopener">Jupyter源码</a></p>
<h2 id="四、训练一个分类器"><a href="#四、训练一个分类器" class="headerlink" title="四、训练一个分类器"></a>四、训练一个分类器</h2><p>你已经看到如何去定义一个神经网络,计算损失值和更新网络的权重.</p>
<p>你现在可能在思考.</p>
<h3 id="关于数据"><a href="#关于数据" class="headerlink" title="关于数据"></a>关于数据</h3><p>通常，当你处理图像，文本，音频和视频数据时，你可以使用标准的Python包来加载数据到一个numpy数组中.然后把这个数组转换成<code>torch.*Tensor</code>.</p>
<ul>
<li>对于图像,有诸如Pillow,OpenCV包.</li>
<li>对于音频,有诸如scipy和librosa包</li>
<li>对于文本,原始Python和Cython来加载,或者NLTK和SpaCy是有用的.</li>
</ul>
<p>对于视觉,我们创建了一个<code>torchvision</code>包,包含常见数据集的数据加载,比如Imagenet,CIFAR10,MNIST等,和图像转换器,也就是<code>torchvision.datasets</code>和<code>torch.utils.data.DataLoader</code>.</p>
<p>这提供了巨大的便利,也避免了代码的重复.</p>
<p>在这个教程中,我们使用CIFAR10数据集,它有如下10个类别:’airplane’,’automobile’,’bird’,’cat’,’deer’,’dog’,’frog’,’horse’,’ship’,’truck’.这个数据集中的图像大小为3*32*32,即,3通道,32*32像素.</p>
<p><img src="/images/cifar10.png" alt="cifar10"></p>
<h3 id="训练一个图像分类器"><a href="#训练一个图像分类器" class="headerlink" title="训练一个图像分类器"></a>训练一个图像分类器</h3><p>我们将一次按照下列顺序进行:</p>
<ol>
<li>使用<code>torchvision</code>加载和归一化CIFAR10训练集和测试集.</li>
<li>定义一个卷积神经网络</li>
<li>定义损失函数</li>
<li>在训练集上训练网络</li>
<li>在测试机上测试网络</li>
</ol>
<h4 id="1-加载和归一化CIFAR0"><a href="#1-加载和归一化CIFAR0" class="headerlink" title="1. 加载和归一化CIFAR0"></a>1. 加载和归一化CIFAR0</h4><p>使用<code>torchvision</code>加载CIFAR10是非常容易的.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torchvision</span><br><span class="line">import torchvision.transforms as transforms</span><br></pre></td></tr></table></figure>
<p>torchvision的输出是[0,1]的PILImage图像,我们把它转换为归一化范围为[-1, 1]的张量.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])</span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=&apos;./data&apos;, train=True,</span><br><span class="line">                                        download=True, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,</span><br><span class="line">                                          shuffle=True, num_workers=2)</span><br><span class="line"></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=&apos;./data&apos;, train=False,</span><br><span class="line">                                       download=True, transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=4,</span><br><span class="line">                                         shuffle=False, num_workers=2)</span><br><span class="line"></span><br><span class="line">classes = (&apos;plane&apos;, &apos;car&apos;, &apos;bird&apos;, &apos;cat&apos;,</span><br><span class="line">           &apos;deer&apos;, &apos;dog&apos;, &apos;frog&apos;, &apos;horse&apos;, &apos;ship&apos;, &apos;truck&apos;)</span><br></pre></td></tr></table></figure></p>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Files already downloaded and verified</span><br><span class="line">Files already downloaded and verified</span><br></pre></td></tr></table></figure>
<p>为了好玩,我们展示一些训练图像.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># functions to show an image</span><br><span class="line"></span><br><span class="line">def imshow(img):</span><br><span class="line">    img = img / 2 + 0.5     # unnormalize</span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (1, 2, 0)))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># get some random training images</span><br><span class="line">dataiter = iter(trainloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"># show images</span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"># print labels</span><br><span class="line">print(&apos; &apos;.join(&apos;%5s&apos; % classes[labels[j]] for j in range(4)))</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">truck   cat   car plane</span><br></pre></td></tr></table></figure>
<p><img src="/images/sphx_glr_cifar10_tutorial_001.png" alt="sphx_glr_cifar10_tutorial_001"></p>
<h4 id="2-定义一个卷积神经网络"><a href="#2-定义一个卷积神经网络" class="headerlink" title="2. 定义一个卷积神经网络"></a>2. 定义一个卷积神经网络</h4><p>从之前的神经网络一节复制神经网络代码,并修改为接受3通道图像取代之前的接受单通道图像.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">from torch.autograd import Variable</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Net(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(3, 6, 5)</span><br><span class="line">        self.pool = nn.MaxPool2d(2, 2)</span><br><span class="line">        self.conv2 = nn.Conv2d(6, 16, 5)</span><br><span class="line">        self.fc1 = nn.Linear(16 * 5 * 5, 120)</span><br><span class="line">        self.fc2 = nn.Linear(120, 84)</span><br><span class="line">        self.fc3 = nn.Linear(84, 10)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(-1, 16 * 5 * 5)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br></pre></td></tr></table></figure>
<h4 id="3-定义损失函数和优化器"><a href="#3-定义损失函数和优化器" class="headerlink" title="3. 定义损失函数和优化器"></a>3. 定义损失函数和优化器</h4><p>我们使用交叉熵作为损失函数,使用带动量的随机梯度下降.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import torch.optim as optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)</span><br></pre></td></tr></table></figure>
<h4 id="4-训练网络"><a href="#4-训练网络" class="headerlink" title="4. 训练网络"></a>4. 训练网络</h4><p>这是开始有趣的时刻.我们只需在数据迭代器上循环,听歌数据输入给网络,并优化.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">for epoch in range(2):  # loop over the dataset multiple times</span><br><span class="line"></span><br><span class="line">    running_loss = 0.0</span><br><span class="line">    for i, data in enumerate(trainloader, 0):</span><br><span class="line">        # get the inputs</span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        # wrap them in Variable</span><br><span class="line">        inputs, labels = Variable(inputs), Variable(labels)</span><br><span class="line"></span><br><span class="line">        # zero the parameter gradients</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        # forward + backward + optimize</span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        # print statistics</span><br><span class="line">        running_loss += loss.data[0]</span><br><span class="line">        if i % 2000 == 1999:    # print every 2000 mini-batches</span><br><span class="line">            print(&apos;[%d, %5d] loss: %.3f&apos; %</span><br><span class="line">                  (epoch + 1, i + 1, running_loss / 2000))</span><br><span class="line">            running_loss = 0.0</span><br><span class="line"></span><br><span class="line">print(&apos;Finished Training&apos;)</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[1,  2000] loss: 2.191</span><br><span class="line">[1,  4000] loss: 1.866</span><br><span class="line">[1,  6000] loss: 1.696</span><br><span class="line">[1,  8000] loss: 1.596</span><br><span class="line">[1, 10000] loss: 1.502</span><br><span class="line">[1, 12000] loss: 1.496</span><br><span class="line">[2,  2000] loss: 1.422</span><br><span class="line">[2,  4000] loss: 1.370</span><br><span class="line">[2,  6000] loss: 1.359</span><br><span class="line">[2,  8000] loss: 1.321</span><br><span class="line">[2, 10000] loss: 1.311</span><br><span class="line">[2, 12000] loss: 1.275</span><br><span class="line">Finished Training</span><br></pre></td></tr></table></figure>
<h4 id="5-在测试集上测试网络"><a href="#5-在测试集上测试网络" class="headerlink" title="5. 在测试集上测试网络"></a>5. 在测试集上测试网络</h4><p>我们在整个训练集上训练了两次网络,但是我们需要检查网络是否从数据集中学习到东西.</p>
<p>我们通过预测神经网络输出的类别标签并根据实际情况进行检测.如果预测正确,我们把该样本添加到正确预测列表.</p>
<p>第一步,显示测试集中的图片一遍熟悉图片内容.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dataiter = iter(testloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"># print images</span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line">print(&apos;GroundTruth: &apos;, &apos; &apos;.join(&apos;%5s&apos; % classes[labels[j]] for j in range(4)))</span><br></pre></td></tr></table></figure>
<p><img src="/images/sphx_glr_cifar10_tutorial_002.png" alt="sphx_glr_cifar10_tutorial_002"></p>
<p>输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GroundTruth:    cat  ship  ship plane</span><br></pre></td></tr></table></figure>
<p>现在我们来看看神经网络认为以上图片是什么?</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">outputs = net(Variable(images))</span><br></pre></td></tr></table></figure>
<p>输出是10个标签的能量.一个类别的能量越大,神经网络越认为他是这个类别.所以让我们得到最高能量的标签.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">_, predicted = torch.max(outputs.data, 1)</span><br><span class="line"></span><br><span class="line">print(&apos;Predicted: &apos;, &apos; &apos;.join(&apos;%5s&apos; % classes[predicted[j]]</span><br><span class="line">                              for j in range(4)))</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Predicted:    cat  ship   car plane</span><br></pre></td></tr></table></figure>
<p>这结果看起来非常的好.</p>
<p>接下来让我们看看网络在整个测试集上的结果如何.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">correct = 0</span><br><span class="line">total = 0</span><br><span class="line">for data in testloader:</span><br><span class="line">    images, labels = data</span><br><span class="line">    outputs = net(Variable(images))</span><br><span class="line">    _, predicted = torch.max(outputs.data, 1)</span><br><span class="line">    total += labels.size(0)</span><br><span class="line">    correct += (predicted == labels).sum()</span><br><span class="line"></span><br><span class="line">print(&apos;Accuracy of the network on the 10000 test images: %d %%&apos; % (</span><br><span class="line">    100 * correct / total))</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy of the network on the 10000 test images: 55 %</span><br></pre></td></tr></table></figure>
<p>结果看起来好于偶然,偶然的正确率为10%,似乎网络学习到了一些东西.</p>
<p>那在什么类上预测较好,什么类预测结果不好呢.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">class_correct = list(0. for i in range(10))</span><br><span class="line">class_total = list(0. for i in range(10))</span><br><span class="line">for data in testloader:</span><br><span class="line">    images, labels = data</span><br><span class="line">    outputs = net(Variable(images))</span><br><span class="line">    _, predicted = torch.max(outputs.data, 1)</span><br><span class="line">    c = (predicted == labels).squeeze()</span><br><span class="line">    for i in range(4):</span><br><span class="line">        label = labels[i]</span><br><span class="line">        class_correct[label] += c[i]</span><br><span class="line">        class_total[label] += 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for i in range(10):</span><br><span class="line">    print(&apos;Accuracy of %5s : %2d %%&apos; % (</span><br><span class="line">        classes[i], 100 * class_correct[i] / class_total[i]))</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Accuracy of plane : 60 %</span><br><span class="line">Accuracy of   car : 46 %</span><br><span class="line">Accuracy of  bird : 44 %</span><br><span class="line">Accuracy of   cat : 35 %</span><br><span class="line">Accuracy of  deer : 38 %</span><br><span class="line">Accuracy of   dog : 43 %</span><br><span class="line">Accuracy of  frog : 57 %</span><br><span class="line">Accuracy of horse : 76 %</span><br><span class="line">Accuracy of  ship : 71 %</span><br><span class="line">Accuracy of truck : 74 %</span><br></pre></td></tr></table></figure>
<p>接下来干什么?</p>
<p>我们如何在GPU上运行神经网络呢?</p>
<h3 id="在GPU上训练"><a href="#在GPU上训练" class="headerlink" title="在GPU上训练"></a>在GPU上训练</h3><p>你是如何把一个<code>Tensor</code>转换GPU上,你就如何把一个神经网络移动到GPU上训练.这个操作会递归遍历有所模块,并将其参数和缓冲区转换为CUDA张量.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.cuda()</span><br></pre></td></tr></table></figure>
<p>请记住,你也必须在每一步中把你的输入和目标值转换到GPU上:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputs, labels = Variable(inputs.cuda()), Variable(target.cuda())</span><br></pre></td></tr></table></figure>
<p>为什么我们没注意到GPU的速度提升很多?那是因为网络非常的小.</p>
<p><strong>实践</strong>:尝试增加你的网络的宽度(第一个<code>nn.Conv2d</code>的第2个参数, 第二个<code>nn.Conv2d</code>的第一个参数,他们需要是相同的数字),看看你得到了什么样的加速.</p>
<p><strong>实现的目标:</strong></p>
<ul>
<li>深入了解了PyTorch的张量库和神经网络.</li>
<li>训练了一个小网络来分类图片.</li>
</ul>
<h3 id="在多GPU上训练"><a href="#在多GPU上训练" class="headerlink" title="在多GPU上训练"></a>在多GPU上训练</h3><p>如果你希望使用所有GPU来更大的加快速度,请查看<a href="http://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html" target="_blank" rel="noopener">选读:数据并行</a></p>
<h3 id="接下来做什么"><a href="#接下来做什么" class="headerlink" title="接下来做什么?"></a>接下来做什么?</h3><ul>
<li><a href="http://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html" target="_blank" rel="noopener">训练神经网络玩电子游戏</a></li>
<li><a href="https://github.com/pytorch/examples/tree/master/imagenet" target="_blank" rel="noopener">在ImageNet上训练最好的ResNet</a></li>
<li><a href="https://github.com/pytorch/examples/tree/master/dcgan" target="_blank" rel="noopener">使用对抗生成网络来训练一个人脸生成器</a></li>
<li><a href="https://github.com/pytorch/examples/tree/master/word_language_model" target="_blank" rel="noopener">使用LSTM网络训练一个字符级的语言模型</a></li>
<li><a href="https://github.com/pytorch/examples" target="_blank" rel="noopener">更多示例</a></li>
<li><a href="https://github.com/pytorch/tutorials" target="_blank" rel="noopener">更多教程</a></li>
<li><a href="https://discuss.pytorch.org/" target="_blank" rel="noopener">在论坛上讨论PyTorch</a></li>
<li><a href="[Chat with other users on Slack](http://pytorch.slack.com/messages/beginner/">在Slack上与其他用户聊天</a>)</li>
</ul>
<p><strong>脚本总运行时间:</strong>3:24.484</p>
<p><a href="http://pytorch.org/tutorials/_downloads/cifar10_tutorial.py" target="_blank" rel="noopener">Python源码</a></p>
<p><a href="http://pytorch.org/tutorials/_downloads/cifar10_tutorial.ipynb" target="_blank" rel="noopener">Jupyter源码</a></p>
<h2 id="五、数据并行-选读"><a href="#五、数据并行-选读" class="headerlink" title="五、数据并行(选读)"></a>五、数据并行(选读)</h2><p><strong>作者</strong>:<a href="https://github.com/hunkim" target="_blank" rel="noopener">Sung Kim</a>和<a href="https://github.com/jennykang" target="_blank" rel="noopener">Jenny Kang</a></p>
<p>在这个教程里,我们将学习如何使用<code>DataParallel</code>来使用多GPU.</p>
<p>PyTorch非常容易的就可以使用GPU,你可以用如下方式把一个模型防盗GPU上:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.gpu()</span><br></pre></td></tr></table></figure>
<p>然后你可以复制所有的张量到GPU上:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mytensor = mytensor.gpu()</span><br></pre></td></tr></table></figure>
<p>请注意,只调用<code>mytensor.gpu()</code>并没有复制张量到GPU上。你需要把它赋值给一个新的张量并在GPU上使用这个张量。</p>
<p>在多GPU上执行前向和反向传播是自然而然的事。然而，PyTorch默认将只是用一个GPU。你可以使用<code>DataParallel</code>让模型并行运行来轻易的让你的操作在多个GPU上运行。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = nn.DataParallel(model)</span><br></pre></td></tr></table></figure>
<p>这是这篇教程背后的核心，我们接下来将更详细的介绍它。</p>
<h3 id="导入和参数"><a href="#导入和参数" class="headerlink" title="导入和参数"></a>导入和参数</h3><p>导入PyTorch模块和定义参数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line">from torch.utils.data import Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"># Parameters and DataLoaders</span><br><span class="line">input_size = 5</span><br><span class="line">output_size = 2</span><br><span class="line"></span><br><span class="line">batch_size = 30</span><br><span class="line">data_size = 100</span><br></pre></td></tr></table></figure>
<h3 id="虚拟数据集"><a href="#虚拟数据集" class="headerlink" title="虚拟数据集"></a>虚拟数据集</h3><p>制作一个虚拟（随机）数据集，你只需实现<code>__getitem__</code>.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class RandomDataset(Dataset):</span><br><span class="line"></span><br><span class="line">    def __init__(self, size, length):</span><br><span class="line">        self.len = length</span><br><span class="line">        self.data = torch.randn(length, size)</span><br><span class="line"></span><br><span class="line">    def __getitem__(self, index):</span><br><span class="line">        return self.data[index]</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        return self.len</span><br><span class="line"></span><br><span class="line">rand_loader = DataLoader(dataset=RandomDataset(input_size, 100),</span><br><span class="line">                         batch_size=batch_size, shuffle=True)</span><br></pre></td></tr></table></figure>
<h3 id="简单模型"><a href="#简单模型" class="headerlink" title="简单模型"></a>简单模型</h3><p>作为演示，我们的模型只接受一个输入，执行一个线性操作，然后得到结果。然而，你能在任何模型（CNN，RNN，Capsule Net等）上使用<code>DataParallel</code>。</p>
<p>我们在模型内部放置了一条打印语句来检测输入和输出向量的大小。请注意批等级为0时打印的内容。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class Model(nn.Module):</span><br><span class="line">    # Our model</span><br><span class="line"></span><br><span class="line">    def __init__(self, input_size, output_size):</span><br><span class="line">        super(Model, self).__init__()</span><br><span class="line">        self.fc = nn.Linear(input_size, output_size)</span><br><span class="line"></span><br><span class="line">    def forward(self, input):</span><br><span class="line">        output = self.fc(input)</span><br><span class="line">        print(&quot;  In Model: input size&quot;, input.size(),</span><br><span class="line">              &quot;output size&quot;, output.size())</span><br><span class="line"></span><br><span class="line">        return output</span><br></pre></td></tr></table></figure>
<h3 id="创建一个模型和数据并行"><a href="#创建一个模型和数据并行" class="headerlink" title="创建一个模型和数据并行"></a>创建一个模型和数据并行</h3><p>这是本教程的核心部分。首先，我们需要创建一个模型实例和检测我们是否有多个GPU。如果我们有多个GPU，我们使用<code>nn.DataParallel</code>来包装我们的模型。然后通过<code>model.gpu()</code>（看代码实际是<code>model.cuda()</code>)把模型放到GPU上。</p>
<h3 id="运行模型"><a href="#运行模型" class="headerlink" title="运行模型"></a>运行模型</h3><p>现在我们可以看输入和输出张量的大小。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">for data in rand_loader:</span><br><span class="line">    if torch.cuda.is_available():</span><br><span class="line">        input_var = Variable(data.cuda())</span><br><span class="line">    else:</span><br><span class="line">        input_var = Variable(data)</span><br><span class="line"></span><br><span class="line">    output = model(input_var)</span><br><span class="line">    print(&quot;Outside: input size&quot;, input_var.size(),</span><br><span class="line">          &quot;output_size&quot;, output.size())</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">  In Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">  In Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">  In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</span><br></pre></td></tr></table></figure>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>当我们对30个输入和输出进行批处理时，我们和期望的一样得到30个输入和输出，但是如果你有多个GPU，你得到如下的结果。</p>
<p><strong>2个GPU</strong></p>
<p>如果你有2个GPU，你将看到：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># on 2 GPUs</span><br><span class="line">Let&apos;s use 2 GPUs!</span><br><span class="line">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])</span><br><span class="line">    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])</span><br><span class="line">Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</span><br></pre></td></tr></table></figure>
<p><strong>3个GPU</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Let&apos;s use 3 GPUs!</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</span><br></pre></td></tr></table></figure>
<p><strong>8个GPU</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">Let&apos;s use 8 GPUs!</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</span><br></pre></td></tr></table></figure>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>DataParallel自动的划分数据，并将作业发送到多个GPU上的多个模型。在每个模型完成作业后，DataParallel收集并合并结果返回给你。</p>
<p>更多信息请看这里：</p>
<p><a href="http://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html" target="_blank" rel="noopener">http://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html</a></p>
<p><strong>脚本总运行时间：</strong>0.0003秒</p>
<p><a href="http://pytorch.org/tutorials/_downloads/data_parallel_tutorial.py" target="_blank" rel="noopener">Python源码</a></p>
<p><a href="http://pytorch.org/tutorials/_downloads/data_parallel_tutorial.ipynb" target="_blank" rel="noopener">Jupyter源码</a></p>

        
    

    
</div>


                

                <!-- Post Comments -->
                
                    
                
            </div>

            <!-- Post Prev & Next Nav -->
            <nav class="material-nav mdl-color-text--grey-50 mdl-cell mdl-cell--12-col">
    <!-- Prev Nav -->
    
        <a href="/2017/11/29/hexo-support-mathjax/" id="post_nav-newer" class="prev-content">
            <button class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon mdl-color--white mdl-color-text--grey-900" role="presentation">
                <i class="material-icons">arrow_back</i>
            </button>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            新篇
        </a>
    

    <!-- Section Spacer -->
    <div class="section-spacer"></div>

    <!-- Next Nav -->
    
        <a href="/2017/11/27/hexo-configuration/" id="post_nav-older" class="next-content">
            旧篇
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            <button class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon mdl-color--white mdl-color-text--grey-900" role="presentation">
                <i class="material-icons">arrow_forward</i>
            </button>
        </a>
    
</nav>

        </div>
    </div>



                    
                        <!-- Overlay For Active Sidebar -->
<div class="sidebar-overlay"></div>

<!-- Material sidebar -->
<aside id="sidebar" class="sidebar sidebar-colored sidebar-fixed-left" role="navigation">
    <div id="sidebar-main">
        <!-- Sidebar Header -->
        <div class="sidebar-header header-cover" style="background-image: url(/img/sidebar_header.png);">
    <!-- Top bar -->
    <div class="top-bar"></div>

    <!-- Sidebar toggle button -->
    <button type="button" class="sidebar-toggle mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon" style="display: initial;" data-upgraded=",MaterialButton,MaterialRipple">
        <i class="material-icons">clear_all</i>
        <span class="mdl-button__ripple-container">
            <span class="mdl-ripple">
            </span>
        </span>
    </button>

    <!-- Sidebar Avatar -->
    <div class="sidebar-image">
        <img src="/img/avatar.png" alt="茂松's avatar">
    </div>

    <!-- Sidebar Email -->
    <a data-toggle="dropdown" class="sidebar-brand" href="#settings-dropdown">
        maosongran@gmail.com
        <b class="caret"></b>
    </a>
</div>


        <!-- Sidebar Navigation  -->
        <ul class="nav sidebar-nav">
    <!-- User dropdown  -->
    <li class="dropdown">
        <ul id="settings-dropdown" class="dropdown-menu">
            
                <li>
                    <a href="mailto: maosongran@gmail.com" target="_blank" title="Email Me">
                        
                            <i class="material-icons sidebar-material-icons sidebar-indent-left1pc-element">email</i>
                        
                        Email Me
                    </a>
                </li>
            
        </ul>
    </li>

    <!-- Homepage -->
    
        <li id="sidebar-first-li">
            <a href="/">
                
                    <i class="material-icons sidebar-material-icons">home</i>
                
                主页
            </a>
        </li>
        
    

    <!-- Archives  -->
    
        <li class="dropdown">
            <a href="#" class="ripple-effect dropdown-toggle" data-toggle="dropdown">
                
                    <i class="material-icons sidebar-material-icons">inbox</i>
                
                    归档
                <b class="caret"></b>
            </a>
            <ul class="dropdown-menu">
            <li>
                <a class="sidebar_archives-link" href="/archives/2018/09/">九月 2018<span class="sidebar_archives-count">2</span></a></li><li><a class="sidebar_archives-link" href="/archives/2018/08/">八月 2018<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/archives/2018/04/">四月 2018<span class="sidebar_archives-count">2</span></a></li><li><a class="sidebar_archives-link" href="/archives/2018/03/">三月 2018<span class="sidebar_archives-count">2</span></a></li><li><a class="sidebar_archives-link" href="/archives/2018/01/">一月 2018<span class="sidebar_archives-count">3</span></a></li><li><a class="sidebar_archives-link" href="/archives/2017/12/">十二月 2017<span class="sidebar_archives-count">3</span></a></li><li><a class="sidebar_archives-link" href="/archives/2017/11/">十一月 2017<span class="sidebar_archives-count">3</span></a>
            </ul>
        </li>
        
    

    <!-- Categories  -->
    
        <li class="dropdown">
            <a href="#" class="ripple-effect dropdown-toggle" data-toggle="dropdown">
                
                    <i class="material-icons sidebar-material-icons">chrome_reader_mode</i>
                
                分类
                <b class="caret"></b>
            </a>
            <ul class="dropdown-menu">
                <li>
                <a class="sidebar_archives-link" href="/categories/Algorithm-4th/">Algorithm(4th)<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/categories/Android/">Android<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/categories/Hexo/">Hexo<span class="sidebar_archives-count">2</span></a></li><li><a class="sidebar_archives-link" href="/categories/PyTorch/">PyTorch<span class="sidebar_archives-count">5</span></a></li><li><a class="sidebar_archives-link" href="/categories/Spring/">Spring<span class="sidebar_archives-count">2</span></a></li><li><a class="sidebar_archives-link" href="/categories/Tutorial/">Tutorial<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/categories/Weekly-Algorithm/">Weekly Algorithm<span class="sidebar_archives-count">1</span></a></li><li><a class="sidebar_archives-link" href="/categories/机器学习100天/">机器学习100天<span class="sidebar_archives-count">2</span></a>
            </ul>
        </li>
        
            <li class="divider"></li>
        
    

    <!-- Pages  -->
    
        <li>
            <a href="/about" title="关于">
                
                    <i class="material-icons sidebar-material-icons">person</i>
                
                关于
            </a>
        </li>
        
    
        <li>
            <a href="/tags" title="标签云">
                
                    <i class="material-icons sidebar-material-icons">bookmark</i>
                
                标签云
            </a>
        </li>
        
    
        <li>
            <a href="/timeline" title="时间轴">
                
                    <i class="material-icons sidebar-material-icons">visibility</i>
                
                时间轴
            </a>
        </li>
        
    
        <li>
            <a href="/gallery" title="图库">
                
                    <i class="material-icons sidebar-material-icons">image</i>
                
                图库
            </a>
        </li>
        
            <li class="divider"></li>
        
    

    <!-- Article Number  -->
    
        <li>
            <a href="/archives">
                文章总数
                <span class="sidebar-badge">16</span>
            </a>
        </li>
        
    
</ul>


        <!-- Sidebar Footer -->
        <!--
I'm glad you use this theme, the development is no so easy, I hope you can keep the copyright, I will thank you so much.
If you still want to delete the copyrights, could you still retain the first one? Which namely "Theme Material"
It will not impact the appearance and can give developers a lot of support :)

很高兴您使用并喜欢该主题，开发不易 十分谢谢与希望您可以保留一下版权声明。
如果您仍然想删除的话 能否只保留第一项呢？即 "Theme Material"
它不会影响美观并可以给开发者很大的支持和动力。 :)
-->

<!-- Sidebar Divider -->


<!-- Theme Material -->


<!-- Help & Support -->
<!--

-->

<!-- Feedback -->
<!--

-->

<!-- About Theme -->
<!--

-->

    </div>

    <!-- Sidebar Image -->
    

</aside>

                    

                    
                        <!-- Footer Top Button -->
                        <div id="back-to-top" class="toTop-wrap">
    <a href="#top" class="toTop">
        <i class="material-icons footer_top-i">expand_less</i>
    </a>
</div>

                    

                    <!--Footer-->
<footer class="mdl-mini-footer" id="bottom">
    
        <!-- Paradox Footer Left Section -->
        <div class="mdl-mini-footer--left-section sns-list">
    <!-- Twitter -->
    
        <a href="https://twitter.com/twitter" target="_blank">
            <button class="mdl-mini-footer--social-btn social-btn footer-sns-twitter">
                <span class="visuallyhidden">Twitter</span>
            </button><!--
     --></a>
    

    <!-- Facebook -->
    
        <a href="https://www.facebook.com/facebook" target="_blank">
            <button class="mdl-mini-footer--social-btn social-btn footer-sns-facebook">
                <span class="visuallyhidden">Facebook</span>
            </button><!--
     --></a>
    

    <!-- Google + -->
    
        <a href="https://www.google.com/" target="_blank">
            <button class="mdl-mini-footer--social-btn social-btn footer-sns-gplus">
                <span class="visuallyhidden">Google Plus</span>
            </button><!--
     --></a>
    

    <!-- Weibo -->
    

    <!-- Instagram -->
    

    <!-- Tumblr -->
    

    <!-- Github -->
    
        <a href="https://github.com/ranmaosong" target="_blank">
            <button class="mdl-mini-footer--social-btn social-btn footer-sns-github">
                <span class="visuallyhidden">Github</span>
            </button><!--
     --></a>
    

    <!-- LinkedIn -->
    

    <!-- Zhihu -->
    

    <!-- Bilibili -->
    

    <!-- Telegram -->
    
    
    <!-- V2EX -->
    
</div>


        <!--Copyright-->
        <div id="copyright">
            Copyright&nbsp;©<script type="text/javascript">var fd = new Date();document.write("&nbsp;" + fd.getFullYear() + "&nbsp;");</script>技术闲谈
            
        </div>
        

        <!-- Paradox Footer Right Section -->

        <!--
        I am glad you use this theme, the development is no so easy, I hope you can keep the copyright.
        It will not impact the appearance and can give developers a lot of support :)

        很高兴您使用该主题，开发不易，希望您可以保留一下版权声明。
        它不会影响美观并可以给开发者很大的支持。 :)
        -->

        <div class="mdl-mini-footer--right-section">
            <div>
                <div class="footer-develop-div">Powered by <a href="https://hexo.io" target="_blank" class="footer-develop-a">Hexo</a></div>
                <div class="footer-develop-div">Theme - <a href="https://github.com/viosey/hexo-theme-material" target="_blank" class="footer-develop-a">Material</a></div>
            </div>
        </div>
    
</footer>


                    <!-- Import JS File -->

    <script>lsloader.load("lazyload_js","/js/lazyload.min.js?1BcfzuNXqV+ntF6gq+5X3Q==", true)</script>



    <script>lsloader.load("js_js","/js/js.min.js?V/53wGualMuiPM3xoetD5Q==", true)</script>



    <script>lsloader.load("np_js","/js/nprogress.js?pl3Qhb9lvqR1FlyLUna1Yw==", true)</script>


<script type="text/ls-javascript" id="NProgress-script">
    NProgress.configure({
        showSpinner: true
    });
    NProgress.start();
    $('#nprogress .bar').css({
        'background': '#29d'
    });
    $('#nprogress .peg').css({
        'box-shadow': '0 0 10px #29d, 0 0 15px #29d'
    });
    $('#nprogress .spinner-icon').css({
        'border-top-color': '#29d',
        'border-left-color': '#29d'
    });
    setTimeout(function() {
        NProgress.done();
        $('.fade').removeClass('out');
    }, 800);
</script>







    <!-- Busuanzi -->
    <script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>







<!-- UC Browser Compatible -->
<script>
	var agent = navigator.userAgent.toLowerCase();
	if(agent.indexOf('ucbrowser')>0) {
		document.write('<link rel="stylesheet" href="/css/uc.css">');
	   alert('由于 UC 浏览器使用极旧的内核，而本网站使用了一些新的特性。\n为了您能更好的浏览，推荐使用 Chrome 或 Firefox 浏览器。');
	}
</script>

<!-- Import prettify js  -->



<!-- Window Load -->
<!-- add class for prettify -->
<script type="text/ls-javascript" id="window-load">
    $(window).on('load', function() {
        // Post_Toc parent position fixed
        $('.post-toc-wrap').parent('.mdl-menu__container').css('position', 'fixed');
    });

    
    
</script>

<!-- MathJax Load-->

    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
        tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
        TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
        messageStyle: "none"
    });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

    <script src="[object Object]?config=TeX-AMS-MML_HTMLorMML"></script>




<!-- Bing Background -->


<script type="text/ls-javascript" id="lazy-load">
    // Offer LazyLoad
    queue.offer(function(){
        $('.lazy').lazyload({
            effect : 'show'
        });
    });

    // Start Queue
    $(document).ready(function(){
        setInterval(function(){
            queue.execNext();
        },200);
    });
</script>

<!-- Custom Footer -->



<script>
    (function(){
        var scriptList = document.querySelectorAll('script[type="text/ls-javascript"]')

        for (var i = 0; i < scriptList.length; ++i) {
            var item = scriptList[i];
            lsloader.runInlineScript(item.id,item.id);
        }
    })()
console.log('\n %c © Material Theme | Version: 1.5.0 | https://github.com/viosey/hexo-theme-material %c \n', 'color:#455a64;background:#e0e0e0;padding:5px 0;border-top-left-radius:5px;border-bottom-left-radius:5px;', 'color:#455a64;background:#e0e0e0;padding:5px 0;border-top-right-radius:5px;border-bottom-right-radius:5px;');
</script>

                </main>
            </div>
        <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
    
</html>
