<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>技术闲谈</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-09-13T13:36:00.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>茂松</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>简单线性回归——Day2</title>
    <link href="http://yoursite.com/2018/09/13/ML-100-Days-002/"/>
    <id>http://yoursite.com/2018/09/13/ML-100-Days-002/</id>
    <published>2018-09-13T06:22:04.000Z</published>
    <updated>2018-09-13T13:36:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>相关链接:</strong><br><a href="https://ranmaosong.github.io/2018/09/13/ML-100-Days-002/" target="_blank" rel="noopener">Github地址</a><br><a href="https://www.jianshu.com/p/ff991ff8d4c7" target="_blank" rel="noopener">简书地址</a><br><a href="https://blog.csdn.net/u014630987/article/details/82687507" target="_blank" rel="noopener">CSDN地址</a></p><hr><p>在预测问题中,我们会经常遇到两种常用术语:回归(Regression)和分类(classification),他们的区别是回归算法解决的是预测连续值,而分类问题则是预测的是离散值,因此回归模型的输出是无限的,而分类问题的输出是有限的.</p><p>在统计学中，线性回归是利用称为线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。</p><p>本文首先介绍简单线性回归，下一节介绍多元线性回归。简单线性回归就是根据一个特征 <code>X</code> 来预测一个与其相关的变量 <code>Y</code>。通常我们我们假设这两种变量之间是现行相关的,即可以通过一条直线把这些变量区分开.因此,简单线性回归就是我们试图寻找一个线性函数,该函数以特征 <code>X</code> 为输入,输出一个变量,且在训练集中,使其预测的值尽可能接近目标值.</p><p>在本文我们以一个学生学习时间(hours),来预测的他该门课程的分数(scores).我们假设这两个变量之间存在某种线性关系,如图所示<br><img src="/images/simple_linear_regression.png" alt="simple_linear_regression"></p><p>我们的目标是找到最有的 $b_0$ 和 $b_1$,使我们训练集中的数据,我们的预测值和真实值之间的误差最小.即</p><script type="math/tex; mode=display">b_0^*,b_1^* = argmax_{b_0, b_1} sum\{(y_i - y_p)^2\}</script><p>其中, $y_p$ 为我们预测的值, $y_i$为真是值.</p><h1 id="1-数据预处理"><a href="#1-数据预处理" class="headerlink" title="1. 数据预处理"></a>1. 数据预处理</h1><p>数据预处理,我们将按照第一天介绍的模型进行处理:</p><ol><li>导入相关库</li><li>导入数据集</li><li>检查缺失值</li><li>划分数据集</li><li>特征标准化</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">dataset = pd.read_csv(&quot;studentscores.csv&quot;)</span><br><span class="line">X = dataset.iloc[:, 0].values</span><br><span class="line">Y = dataset.iloc[:, 1].values</span><br><span class="line"></span><br><span class="line">from sklearn.cross_validation import train_test_split</span><br><span class="line">X_train, Y_train, X_test, Y_test = train_test_split(X, Y, test_size=1/4, random_state=0)</span><br></pre></td></tr></table></figure><h1 id="2-训练线性回归模型"><a href="#2-训练线性回归模型" class="headerlink" title="2. 训练线性回归模型"></a>2. 训练线性回归模型</h1><p>sklearn 机器学习库为我们提供了许多的常用机器学习模型,线性回归模型 LinearRegression 存在于 sklearn.linear_model 文件中, 该文件为我们提供了许多的线性模型.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">regressor = LinearRegression()</span><br><span class="line">regressor.fit(X_train, Y_train)</span><br></pre></td></tr></table></figure><p>我们首先通过 LinearRegression() 初始化一个 regressor 实例来表示线性回归模型.然后通过给 <code>.fit</code> 传入我们的训练集的特征和标签来训练 regressor.</p><p><strong>注意:</strong> 在 sklearn 中对训练数据的格式有一个规定,对于输入 X, 要求其格式是 N*M,其中 N 表示样本数, M 表示每个样本的特征数, 此示例中 M=1.对于标签 Y, 其格式是N*F, N表示样本数, F表示输出值的个数,此处F=1.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(regressor.coef_, regressor.intercept_)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[9.94167834]] [1.93220425]</span><br></pre></td></tr></table></figure></p><p>regressor.coef_表示模型的权重, regressor.intercept_ 表示模型的偏执,分别表示上面模型公式的$b_1$ 和 $b_0$.</p><h1 id="3-预测结果"><a href="#3-预测结果" class="headerlink" title="3. 预测结果"></a>3. 预测结果</h1><p>当我们通过 <code>.fit</code> 函数训练好后模型,我们可以通过 <code>.predict</code> 函数来预测我们未知的数据.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Y_pred = regressor.predict(X_test)</span><br></pre></td></tr></table></figure></p><p>我们通过手动预测来验证上面提到的 regressor.coef_, regressor.intercept_ 表示的意义:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">temp = regressor.intercept_[<span class="number">0</span>] + regressor.coef_[<span class="number">0</span>] * X_test[<span class="number">0</span>]</span><br><span class="line">print(temp, Y_pred[<span class="number">0</span>])</span><br></pre></td></tr></table></figure></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[16.84472176] [16.84472176]</span><br></pre></td></tr></table></figure></p><h1 id="4-可视化"><a href="#4-可视化" class="headerlink" title="4. 可视化"></a>4. 可视化</h1><p>首先我们可视化训练集的结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X_train, Y_train, color=&quot;red&quot;)</span><br><span class="line">plt.plot(X_train, regressor.predict(X_train), color=&apos;blue&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="/images/simple_linear_regression_fig1.png" alt="figure1"><br>对测试集进行可视化<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X_test, Y_test, color=&quot;red&quot;)</span><br><span class="line">plt.plot(X_test, regressor.predict(X_test), color=&quot;blue&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="/images/simple_linear_regression_fig2.png" alt="figure2"></p><p>其中 <code>.scatter</code> 用于画散点图, <code>.plot</code> 用于画直线.</p><h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p><a href="https://github.com/RanMaosong/RanMaosong.github.io" target="_blank" rel="noopener">感谢大家的阅读和支持, 欢迎大家上星.</a>.该博客的原始Github项目地址<a href="https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day2_Simple_Linear_Regression.md" target="_blank" rel="noopener">点击这里</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;相关链接:&lt;/strong&gt;&lt;br&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/09/13/ML-100-Days-002/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github地址&lt;/a&gt;&lt;
      
    
    </summary>
    
      <category term="机器学习100天" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    
    
      <category term="机器学习100天" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    
  </entry>
  
  <entry>
    <title>数据预处理——Day 1</title>
    <link href="http://yoursite.com/2018/09/11/ML-100-Days-001/"/>
    <id>http://yoursite.com/2018/09/11/ML-100-Days-001/</id>
    <published>2018-09-11T02:53:21.000Z</published>
    <updated>2018-09-13T13:36:13.568Z</updated>
    
    <content type="html"><![CDATA[<p><strong>相关链接:</strong><br><a href="https://ranmaosong.github.io/2018/09/11/ML-100-Days-001/" target="_blank" rel="noopener">Github地址</a><br><a href="https://www.jianshu.com/p/b7b5512395f8" target="_blank" rel="noopener">简书地址</a><br><a href="https://blog.csdn.net/u014630987/article/details/82669796" target="_blank" rel="noopener">CSDN地址</a></p><hr><p>总所周知,对于机器学习任务,特别是对于深度学习任务,我们需要创建训练集,因此需要收集大量的数据.然而,在实际中,我们收集的数据极易受噪声、缺失值和数据不一致的影响。通常我们对数据进行如下几种预处理：</p><ol><li>缺失值处理</li><li>异常值处理</li><li>冗余数据处理</li><li>数据标准化</li><li>数据离散化</li><li>数据向量化: 我自己总结的，主要针对文本数据。不知道算不算预处理，但这是文本数据的必须操作。</li></ol><p>本文主要针对缺失值处理和数据向量化两种与处理方法进行讲解，整个处理过程分为6步：</p><ol><li>导入相关库</li><li>导入数据集</li><li>处理缺失值</li><li>编码分类数据</li><li>创建训练集和测试集</li><li>特征缩放</li></ol><p>下面我们将通过实例来一步步详细介绍这些操作。</p><h1 id="1-导入相关库"><a href="#1-导入相关库" class="headerlink" title="1. 导入相关库"></a>1. 导入相关库</h1><p><a href="http://www.numpy.org/" target="_blank" rel="noopener">numpy</a> 和 <a href="https://pandas.pydata.org/" target="_blank" rel="noopener">pandas</a> 是利用 Python 进行科学计算时非常重要两个科学计算库。其中 numpy 负责提供基本的数学计算函数，起运算对象事针对张量或矩阵；而 pandas 库则用来导入和管理数据集。起导入规则如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure><h1 id="2-导入数据集"><a href="#2-导入数据集" class="headerlink" title="2. 导入数据集"></a>2. 导入数据集</h1><p>pandas 是一个管理数据的函数库，我为我们提供了许多读取和操作数据的接口:</p><p><img src="/images/pd_read_data.png" alt="pandas_read_data"></p><p><code>.csv</code> 格式文件是我们保存数据的一种常用格式，其格式简单，其默认格式是以 ,(英文逗号)分割数据集，显示效果和 excel 表格类似。 pandas 为我们提供了访问 <code>.csv</code> 的接口，其 <code>pd.read_csv()</code>用来进行读取 <code>.csv</code>格式数据,该函数常用的参数如下：</p><ul><li>filename: 文件名</li><li>header: 表头，默认不为空（第一行为表头），设为 None， 表示无表头</li><li>sep: 指定分隔符。如果不指定参数，则会尝试使用逗号分隔</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataset = pd.read_csv(<span class="string">'Data.csv'</span>)</span><br><span class="line">X = dataset.iloc[ : , :<span class="number">-1</span>].values</span><br><span class="line">Y = dataset.iloc[ : , <span class="number">3</span>].values</span><br></pre></td></tr></table></figure><p><a href="https://github.com/Avik-Jain/100-Days-Of-ML-Code/tree/master/datasets" target="_blank" rel="noopener">Data.csv</a>是我们访问的数据集,其内容如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Country,Age,Salary,Purchased</span><br><span class="line">France,44,72000,No</span><br><span class="line">Spain,27,48000,Yes</span><br><span class="line">Germany,30,54000,No</span><br><span class="line">Spain,38,61000,No</span><br><span class="line">Germany,40,,Yes</span><br><span class="line">France,35,58000,Yes</span><br><span class="line">Spain,,52000,No</span><br><span class="line">France,48,79000,Yes</span><br><span class="line">Germany,50,83000,No</span><br><span class="line">France,37,67000,Yes</span><br></pre></td></tr></table></figure></p><p><code>pd.read_csv()</code>返回的数据类型为DataFrame（把他就像成一张表），其没有原始的下表操作，我们访问内容通过如下几个函数：</p><ol><li>loc: 通过行标签访问数据</li><li>iloc: 通过行索引进行访问</li><li>ix: loc 和 iloc 的混合</li></ol><p><code>.value</code> 属性则是将 DataFrame 转成 numpy类型，以便后续的数值计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(type(dataset), type(X), type(Y))</span><br><span class="line">print(X.shape, Y.shape)</span><br></pre></td></tr></table></figure><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; &lt;class &apos;numpy.ndarray&apos;&gt; &lt;class &apos;numpy.ndarray&apos;&gt;</span><br><span class="line">(10, 3) (10,)</span><br><span class="line">[[&apos;France&apos; 44.0 72000.0]</span><br><span class="line"> [&apos;Spain&apos; 27.0 48000.0]</span><br><span class="line"> [&apos;Germany&apos; 30.0 54000.0]</span><br><span class="line"> [&apos;Spain&apos; 38.0 61000.0]</span><br><span class="line"> [&apos;Germany&apos; 40.0 nan]</span><br><span class="line"> [&apos;France&apos; 35.0 58000.0]</span><br><span class="line"> [&apos;Spain&apos; nan 52000.0]</span><br><span class="line"> [&apos;France&apos; 48.0 79000.0]</span><br><span class="line"> [&apos;Germany&apos; 50.0 83000.0]</span><br><span class="line"> [&apos;France&apos; 37.0 67000.0]]</span><br></pre></td></tr></table></figure></p><h1 id="3-处理缺失值"><a href="#3-处理缺失值" class="headerlink" title="3. 处理缺失值"></a>3. 处理缺失值</h1><p>从数据集我们可以发现，某些数据为空，这时我们需要对这些缺失值进行处理。我么可以对缺失值进行均值和中值处理，即用整个数据集的该列数据的均值或中值来替换缺失的值。在sklearn的preprocessing包中包含了对数据集中缺失值的处理，主要是应用Imputer类进行处理。代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line">imputer = Imputer(missing_values = <span class="string">"NaN"</span>, strategy = <span class="string">"mean"</span>, axis = <span class="number">0</span>)</span><br><span class="line">imputer = imputer.fit(X[ : , <span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line">X[ : , <span class="number">1</span>:<span class="number">3</span>] = imputer.transform(X[ : , <span class="number">1</span>:<span class="number">3</span>])</span><br></pre></td></tr></table></figure></p><p>该代码首先导入 Imputer 类，然后通过一些参数来创建 Imputer 实例，参数意思大致如下：</p><ul><li>missing_values: 我们要处理的缺失值类型，由于 <code>pandas.read_csv()</code> 会将确实的值默认设置为 Nan，所以这里其类型为 <code>Nan</code>。</li><li>strategy： 表示采用何种策略，其取值有 mean, median, most_frequent,默认为 mean，即均值。</li><li>axis: 表示我们处理的方向，由于这里列方向是同一类型数据，即主轴方向，因此其值为0.</li></ul><p><strong>注意</strong>: sklearn 库预处理操作有个特征是，其处理分为两步： fit 和 transform，fit 大致可以理解为我们对那些数据进行操作或统计，transform 表示实际进行操作。这一点和 sklearn 中的机器学习算法类似，两个固定接口: fit 和 predict。</p><p>由于我们数据库中的数据只有第2、3 列才是数值数据，我们也只能对这两列数据进行预处理，所欲第三行我们在 <code>.fit</code> 中指出要操作的数据，在其内部分别计算了这两列的均值，以供后面处理使用。</p><p>第四行代码执行缺失值处理，通过第三行已经计算出 相应的均值，所以这行代码实际执行缺失值处理</p><p>第三行和第四行代码可以通过<code>.fit_transform</code> 函数来一步执行。这也是 sklearn 的一个特点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(X)</span><br></pre></td></tr></table></figure><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[[&apos;France&apos; 44.0 72000.0]</span><br><span class="line"> [&apos;Spain&apos; 27.0 48000.0]</span><br><span class="line"> [&apos;Germany&apos; 30.0 54000.0]</span><br><span class="line"> [&apos;Spain&apos; 38.0 61000.0]</span><br><span class="line"> [&apos;Germany&apos; 40.0 63777.77777777778]</span><br><span class="line"> [&apos;France&apos; 35.0 58000.0]</span><br><span class="line"> [&apos;Spain&apos; 38.77777777777778 52000.0]</span><br><span class="line"> [&apos;France&apos; 48.0 79000.0]</span><br><span class="line"> [&apos;Germany&apos; 50.0 83000.0]</span><br><span class="line"> [&apos;France&apos; 37.0 67000.0]]</span><br></pre></td></tr></table></figure></p><h1 id="4-编码分类数据"><a href="#4-编码分类数据" class="headerlink" title="4. 编码分类数据"></a>4. 编码分类数据</h1><p>对于我们计算机进行科学计算只针对数值，而不能对文本进行数值计算，因此我们需要对文本进行数据向量化。如何用数值表示文本呢? 最常用的的格式就是 one-hot 格式,这种格式将文本表示成一个向量,该向量只有一个元素的值为1,其余全为0.</p><p>比如,我们数据集有”a”, “b”, “c”, “d”四个文本,此时我们的向量长度则为4,则这四个文本表示如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a: [1, 0, 0, 0]</span><br><span class="line">b: [0, 1, 0, 0]</span><br><span class="line">c: [0, 0, 1, 0]</span><br><span class="line">d: [0, 0, 0, 1]</span><br></pre></td></tr></table></figure></p><p>因此我们需要先统计数据库中所有的文本,并对每个文本从 0 开始进行编号.该过程也通过sklearn库来进行实现:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder, OneHotEncoder</span><br><span class="line">labelencoder_X = LabelEncoder()</span><br><span class="line">X[ : , <span class="number">0</span>] = labelencoder_X.fit_transform(X[ : , <span class="number">0</span>])</span><br></pre></td></tr></table></figure></p><p>由于对于特征(数据集中的最后一列是标签),只有第一列是文本,因此我们需要对第一列进行文本统计.上面代码就是将第一列的文本用自己的编号来进行表示.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(X)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[[0 44.0 72000.0]</span><br><span class="line"> [2 27.0 48000.0]</span><br><span class="line"> [1 30.0 54000.0]</span><br><span class="line"> [2 38.0 61000.0]</span><br><span class="line"> [1 40.0 63777.77777777778]</span><br><span class="line"> [0 35.0 58000.0]</span><br><span class="line"> [2 38.77777777777778 52000.0]</span><br><span class="line"> [0 48.0 79000.0]</span><br><span class="line"> [1 50.0 83000.0]</span><br><span class="line"> [0 37.0 67000.0]]</span><br></pre></td></tr></table></figure></p><p>如何将这些编号表示的文本转换成one-hot形式呢?这就需要使用 sklearn.preprocessing 中的 OneHotEncoder 进行编号.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">onehotencoder = OneHotEncoder(categorical_features = [<span class="number">0</span>])</span><br><span class="line">X = onehotencoder.fit_transform(X).toarray()</span><br><span class="line">labelencoder_Y = LabelEncoder()</span><br><span class="line">Y =  labelencoder_Y.fit_transform(Y)</span><br></pre></td></tr></table></figure></p><p>categorical_features 参数指定了我们需要对哪些列进行one-hot编码,这里为0,表示对第一列,然后利用’.fit_transform’ 进行编码.由于对于标签我们只需要其标签类型,因此不需要进行 one-hot 编码.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(X[<span class="number">0</span>])</span><br><span class="line">print(Y)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[1.0e+00 0.0e+00 0.0e+00 4.4e+01 7.2e+04]</span><br><span class="line">[0 1 0 0 1 1 0 1 0 1]</span><br></pre></td></tr></table></figure></p><p>因为第一列总共有三种文本,因此其one-hot向量长度为3,且第一个样本的 index 为0, 故其 one-hot 为 [1, 0, 0],总的就如上所示.</p><h1 id="5-创建训练集和测试集"><a href="#5-创建训练集和测试集" class="headerlink" title="5. 创建训练集和测试集"></a>5. 创建训练集和测试集</h1><p>对于机器学习任务,我们需要将数据集按照比例划分成两个部分: 训练集和测试集.训练接用于训练模型,测试集用于测试我们模型的性能.对于数据的划分,我们使用 sklearn.cross_validation 的 train_test_split 对数据集按比例进行划分.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cross_validation import train_test_split</span><br><span class="line">X_train, X_test, Y_train, Y_test = train_test_split( X , Y , test_size = 0.2, random_state = 0)</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(X.shape, X_train.shape, X_test.shape)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(10, 5) (8, 5) (2, 5)</span><br></pre></td></tr></table></figure></p><h1 id="6-特征缩放"><a href="#6-特征缩放" class="headerlink" title="6. 特征缩放"></a>6. 特征缩放</h1><p>通过我们需要对数据进行标准化处理,这样可以减小噪声的影响.通常将其标准化为:均值为0, 方差为1的数据范围.<br>标准化之前的方差和均值:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(np.mean(X_train), np.std(X_train))</span><br><span class="line">print(np.mean(X_test), np.std(X_test))</span><br></pre></td></tr></table></figure></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">12527.338888888888 25395.495797239782</span><br><span class="line">13708.2 28152.896706378193</span><br></pre></td></tr></table></figure></p><p>进行标准化:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">sc_X = StandardScaler()</span><br><span class="line">X_train = sc_X.fit_transform(X_train)</span><br><span class="line">X_test = sc_X.fit_transform(X_test)</span><br></pre></td></tr></table></figure></p><p>标准化之后的均值和方法:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(np.mean(X_train), np.std(X_train))</span><br><span class="line">print(np.mean(X_test), np.std(X_test))</span><br></pre></td></tr></table></figure></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">7.771561172376095e-17 1.0</span><br><span class="line">0.0 0.6324555320336759</span><br></pre></td></tr></table></figure></p><h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p><a href="https://github.com/RanMaosong/RanMaosong.github.io" target="_blank" rel="noopener">感谢大家的阅读和支持, 欢迎大家上星.</a>.该博客的原始Github项目地址[点击这里]</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;相关链接:&lt;/strong&gt;&lt;br&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/09/11/ML-100-Days-001/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github地址&lt;/a&gt;&lt;
      
    
    </summary>
    
      <category term="机器学习100天" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    
    
      <category term="机器学习100天" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    
  </entry>
  
  <entry>
    <title>序言</title>
    <link href="http://yoursite.com/2018/09/10/ML-100-Days/"/>
    <id>http://yoursite.com/2018/09/10/ML-100-Days/</id>
    <published>2018-09-10T03:16:16.000Z</published>
    <updated>2018-09-13T13:34:39.533Z</updated>
    
    <content type="html"><![CDATA[<p><strong>相关链接:</strong><br><a href="https://ranmaosong.github.io/2018/09/10/ML-100-Days/" target="_blank" rel="noopener">Github地址</a><br><a href="https://www.jianshu.com/p/ee8fc3881c57" target="_blank" rel="noopener">简书地址</a><br><a href="https://blog.csdn.net/u014630987/article/details/82669779" target="_blank" rel="noopener">CSDN地址</a></p><p>该系列博客是基于由<a href="https://github.com/llSourcell" target="_blank" rel="noopener">Siraj Raval</a>提出的<a href="https://github.com/Avik-Jain/100-Days-Of-ML-Code" target="_blank" rel="noopener">100 Days of Machine Learning Coding</a>.而该系列博客的目的是在学习该项目的同事丰富其内容，加入自己查阅的资料和自己的理解。该项目全部基于 Python 语言进行讲解</p><p><strong>内容安排</strong></p><ul><li><a href="https://ranmaosong.github.io/2018/09/11/ML-100-Days-001/" target="_blank" rel="noopener">第1天 数据集预处理</a></li><li><a href="/2018/09/13/ML-100-Days-002/">第2天 简单线性回归</a></li><li><a href="*">第3天 多元线性回归</a></li><li><a href="*">第4天 Logistic回归</a></li><li><a href="*">第5天 Logistic回归</a></li><li><a href="*">第6天 实现 Logistic回归</a></li><li><a href="*">第7天 K 均值</a></li><li><a href="*">第8天 Logistic回归数学原理</a></li><li><a href="*">第9天 支持向量机</a></li><li><a href="*">第10天 实现 K 均值</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;相关链接:&lt;/strong&gt;&lt;br&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/09/10/ML-100-Days/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github地址&lt;/a&gt;&lt;br&gt;&lt;
      
    
    </summary>
    
      <category term="机器学习100天" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    
    
      <category term="机器学习100天" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    
  </entry>
  
  <entry>
    <title>排序算法</title>
    <link href="http://yoursite.com/2018/08/14/Algorithm-sorting/"/>
    <id>http://yoursite.com/2018/08/14/Algorithm-sorting/</id>
    <published>2018-08-14T14:22:27.000Z</published>
    <updated>2018-08-14T14:57:29.883Z</updated>
    
    <content type="html"><![CDATA[<p><strong>排序</strong>就是将一组无序的元素通过某种规则将他们按照某种主键来进行升序或者降序排列，比如，我们考试成绩，按照总分进行降序排列，按照语文成绩进行降序排列，按照数学成绩进行降序排列，此时的<em>总分</em>, <em>语文</em>, <em>数学</em> 就是我们排序的主键,又或者我们打开电脑文件夹,按照名字排序,按照时间排序等,由此可见,排序操作在我们的日常生活中的作用之大,下面我们将讲解一些常见的排序算法,参考书籍为<strong>&lt;&lt;算法 第四版&gt;&gt;</strong>.</p><p><strong>声明</strong>: 本节所有内容均默认以升序排列,同时所有代码遵从以下模板.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Template</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">sort</span><span class="params">(Comparable[] a)</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">less</span><span class="params">(Comparable v, Comparable w)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> v.compareTo(w) &lt; <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">exch</span><span class="params">(Comparable[] a, <span class="keyword">int</span> i, <span class="keyword">int</span> j)</span> </span>&#123;</span><br><span class="line">        Comparable t = a[i];</span><br><span class="line">        a[i] = a[j];</span><br><span class="line">        a[j] = t;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">show</span><span class="params">(Comparable[] a)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; a.length; ++i) &#123;</span><br><span class="line">            System.out.print(a[i] + <span class="string">", "</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isSorted</span><span class="params">(Comparable[] a)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; a.length; ++i) &#123;</span><br><span class="line">            <span class="keyword">if</span> (less(a[i], a[i-<span class="number">1</span>]))</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"><span class="comment">//        String[] a = new String[n];</span></span><br><span class="line">        sort(a);</span><br><span class="line">        <span class="keyword">assert</span> Template.isSorted(a);</span><br><span class="line">        Template.show(a);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="选择排序"><a href="#选择排序" class="headerlink" title="选择排序"></a>选择排序</h1><p><strong>选择排序</strong> 顾名思义就是从乱序的数组中选择最值,其思路如下: 首先,从 N 个数中选择最小的数,放在数组第一个位置,然后从剩下的 N-1 个数中选择最小的数放在数组的第二个位置,迭代此过程直至完成排序.该算法大约进行 $N^2/2$ 次比较和 N 次交换,详细计算如下:</p><p>寻找第一个最小值时,我们需要在余下的 N-1(为什么是 N-1 不是 N?因为数组坐标为0的数是我们目前最小的值,此时我们从剩下的 N-1 个数比较依次找最小值) 个数中进行比较,且交换 1 次,此时进行了 N-1 次比较,寻找第二个最小值时,需要在余下的 N-2 个数中进行比较,此时进行了 N-2 次比较,所以我们总共需要 $(N-1)+(N-2)+\dots +1=N(N-1)/2$ 比较和 N-1 次交换.</p><p>代码如下:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> sort;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> song</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 18-8-14 下午9:38</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Selection</span> <span class="keyword">extends</span> <span class="title">Template</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span>  <span class="title">sort</span><span class="params">(Comparable[] a)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 升序排列</span></span><br><span class="line">        <span class="keyword">int</span> N = a.length;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; ++i) &#123;</span><br><span class="line">            <span class="keyword">int</span> min = i;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = i+<span class="number">1</span>; j &lt; N; ++j) &#123;</span><br><span class="line">                <span class="keyword">if</span> (Template.less(a[j], a[min]))</span><br><span class="line">                    min = j;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            Template.exch(a, i, min);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        String[] a = <span class="keyword">new</span> String[]&#123;<span class="string">"s"</span>, <span class="string">"o"</span>, <span class="string">"r"</span>, <span class="string">"t"</span>, <span class="string">"e"</span>, <span class="string">"x"</span>, <span class="string">"a"</span>, <span class="string">"m"</span>, <span class="string">"p"</span>, <span class="string">"l"</span>, <span class="string">"e"</span>&#125;;</span><br><span class="line">        sort(a);</span><br><span class="line">        <span class="keyword">assert</span> Template.isSorted(a);</span><br><span class="line">        Template.show(a);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;排序&lt;/strong&gt;就是将一组无序的元素通过某种规则将他们按照某种主键来进行升序或者降序排列，比如，我们考试成绩，按照总分进行降序排列，按照语文成绩进行降序排列，按照数学成绩进行降序排列，此时的&lt;em&gt;总分&lt;/em&gt;, &lt;em&gt;语文&lt;/em&gt;, &lt;em&gt;数
      
    
    </summary>
    
      <category term="Algorithm(4th)" scheme="http://yoursite.com/categories/Algorithm-4th/"/>
    
    
      <category term="Algorithm(4th)" scheme="http://yoursite.com/tags/Algorithm-4th/"/>
    
  </entry>
  
  <entry>
    <title>最长回文子串(Longest Palindromic Substring)</title>
    <link href="http://yoursite.com/2018/04/10/LeetCode-001-LongestPalindromicSubstring/"/>
    <id>http://yoursite.com/2018/04/10/LeetCode-001-LongestPalindromicSubstring/</id>
    <published>2018-04-10T13:05:49.000Z</published>
    <updated>2018-04-15T13:04:47.736Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2018/04/10/001-LeetCode-LongestPalindromicSubstring/" target="_blank" rel="noopener">Github地址</a><br><a href="https://www.jianshu.com/p/c0840c4d03be" target="_blank" rel="noopener">简书地址</a><br><a href="https://blog.csdn.net/u014630987/article/details/79948604" target="_blank" rel="noopener">CSDN地址</a></p><p><strong>问题描述</strong></p><p><strong>给定一个字符串 s，找出其中最长的回文子串，假设给定字符串的长度最大维 1000.</strong></p><p>例如:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入: &quot;babad&quot;</span><br><span class="line">输出: &quot;bab&quot;</span><br><span class="line">注意： “aba” 也是正确的解，有多个解返回其中一个即可</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入：&quot;cbbd&quot;</span><br><span class="line">输出：&quot;bb&quot;</span><br></pre></td></tr></table></figure><p>回文串是指一个字符串对称，从最左边和最右边分别往最中间遍历，各个位置的字符都相同。解决这个问题，下面将从四个算法分别进行介绍。</p><p><strong>1、暴力枚举法（不可取）</strong></p><p>暴力枚举法是最简单、最容易想到的方法，其思路是：首先找到字符串 s 的所有子串，然后判断该子串是否是回文字符串，最后返回最长的回文子串。该方法虽然简单明了，但因其计算成本太高，该算法在实际中并<strong>不可取</strong>。</p><p>由于一个长为 n 的字符串，共有 $\frac{n(n+1)}{2}$ 个连续子串，故寻找子串的时间复杂度为 $O(n^2)$, 判断一个字符串是否维回文串的时间复杂度维 $O(n)$,故</p><p><strong>时间复杂度:</strong> $O(n^3)$</p><p><strong>空间复杂度为:</strong> $O(1)$。</p><p>下面是 Java 的实现代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LongestPalindromicSubstring</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">longestPalindrome</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 保存得到的最长回文子串的起始位置</span></span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>, right = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> len = s.length();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;len; ++i) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j=i; j&lt;len; ++j) &#123;</span><br><span class="line">                <span class="comment">// 获取 s 的连续子串</span></span><br><span class="line">                String subStr = s.substring(i, j+<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 判断子串是否是回文字符串</span></span><br><span class="line">                <span class="keyword">if</span> (isPalindrome(subStr)) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (j-i &gt; right-left) &#123;</span><br><span class="line">                        left = i;</span><br><span class="line">                        right = j;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> s.substring(left, right+<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">isPalindrome</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> len = s.length();</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>, right = len-<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (left &lt; right) &#123;</span><br><span class="line">            <span class="keyword">if</span> (s.charAt(left) != s.charAt(right))</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">            ++left;</span><br><span class="line">            --right;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>2、中心扩展法（可取）</strong></p><p>中心扩展法是根据暴力枚举法改进而来，主要是去除了一些不必要的子字符串的判断，<strong>主要思路:</strong>首先从字符串 s 中选择一个字符作为子字符串的中心字符,然后以该字符维中心依次往左右两边扩展,判断该子串的最左边和最右边的字符是否相同,相同则继续向两边扩展,不相同则停止扩展,该子串则是以该字符为中心的最长回文子串,这样就减少了很多在暴力枚举方法中不必要的字符的判断.</p><p>和暴力枚举方法比较,以”abacdfgdcaba”为例,假设我们以第一个字符 ‘c’ 为中心,中心扩展首先比较”acd”,由于 ‘a’ 和 ‘d’不相同,则停止扩展,二暴力枚举还需比较 “bacdf” 和 “abacdfg”.该算法在扩展时需要同时考虑子串是奇数和偶数的情况.</p><p>由于需要依次迭代每个字符串中心,因此该迭代需要 $O(n)$ 时间复杂度,同时从中心向两边扩展的复杂度维 $O(n)$,因此:</p><p><strong>时间复杂度:</strong> $O(n^2)$</p><p><strong>空间复杂度为:</strong> $O(1)$。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LongestPalindromicSubstring</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span>  <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"abacdfgdcaba"</span>));</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"cbbd"</span>));</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"babad"</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">longestPalindrome</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 保存获得的最大回文子串</span></span><br><span class="line">        String maxStr = <span class="string">""</span>;</span><br><span class="line">        <span class="keyword">int</span> len = s.length();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;len; ++i) &#123;</span><br><span class="line">            String subStr1 = isPalindrome(s, i, i);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (subStr1.length() &gt; maxStr.length()) &#123;</span><br><span class="line">                maxStr = subStr1;</span><br><span class="line">            &#125;</span><br><span class="line">            String subStr2 = isPalindrome(s, i, i+<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (subStr2.length() &gt; maxStr.length()) &#123;</span><br><span class="line">                maxStr = subStr2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> maxStr;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> String <span class="title">isPalindrome</span><span class="params">(String s, <span class="keyword">int</span> i, <span class="keyword">int</span> j)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// i 表示中心扩展的左边字符</span></span><br><span class="line">        <span class="comment">// j 表示中心扩展的右边字符</span></span><br><span class="line">        <span class="keyword">int</span> len = s.length();</span><br><span class="line">        <span class="keyword">while</span> (i &gt;= <span class="number">0</span> &amp;&amp; j &lt; len &amp;&amp; s.charAt(i) == s.charAt(j)) &#123;</span><br><span class="line">            --i;</span><br><span class="line">            ++j;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span>  s.substring(i+<span class="number">1</span>, j);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>3、动态规划（可取）</strong></p><p>回文字符串的子串也是回文字符串,我们可以将最长回文子串分解为一些列子问题,使用动态规划.设 f 为状态表,f(i,j)表示字符区间 [i, j](包括j)是否为回文字符串<br>,f(i, j)=false 表示子串 [i, j] 不是回文字符串,f(i, j)=true 表示子串 [i, j] 为回文字符串.当我们判断了字符 [i], [j] 相同时,只需判断 f(i+1, j-1) 是否维 true 即可.</p><p>状态表满足以下关系:</p><script type="math/tex; mode=display">f(i,j)=\begin{cases}true,\quad i=j \\s[i]==s[j], \quad i= j-1 \\s[i] = s[j]\quad and\quad f(i+1, j-1), \quad i< j-1\end{cases}</script><p>由于状态表 f 是一个 n*n 的方阵,且是一个对称方阵,故我们只需判断状态表 f 的右上角的内容,因此:</p><p><strong>时间复杂度:</strong> $O(n^2)$<br><strong>空间复杂度:</strong> $O(n^2)$</p><p>实现代码:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">test</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span>  <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"abacdfgdcaba"</span>));</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"cbbd"</span>));</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"babad"</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">longestPalindrome</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n = s.length();</span><br><span class="line">        <span class="keyword">boolean</span>[][] f = <span class="keyword">new</span> <span class="keyword">boolean</span>[n][n];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> left =<span class="number">0</span>, right=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;n; ++j) &#123;</span><br><span class="line">            f[j][j] = <span class="keyword">true</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;j; ++i) &#123;</span><br><span class="line">                <span class="keyword">if</span> (s.charAt(i) == s.charAt(j) &amp;&amp; (i == j-<span class="number">1</span> || f[i+<span class="number">1</span>][j-<span class="number">1</span>])) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (j-i &gt; right - left) &#123;</span><br><span class="line">                        left = i;</span><br><span class="line">                        right = j;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    f[i][j] = <span class="keyword">true</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> s.substring(left, right+<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>该代码首先将状态表全部初始化为 false, 然后按照从上到下,从左到右的顺序依次判断状态表的值.<br><strong>4、Manacher 算法（马拉车算法）（可取）</strong></p><p>Manacher 算法是一种经典的求取最长回文子串的方法,其基本原理是使用已知回文字符串的左半部分来推导以右半部分的字符为中心的回文字符.</p><p>我们使用 p[i] 表示以第 i 个字符为中心的最长回文半径.可以利用已知p[0],p[1]……p[i-1]的值,来计算 p[i] 的值.我们定义 maxRight 是当前计算 i 位置时所有回文子串所能达到的最右端的位置,且该回文串的中心位置为 k,此时有如下关系: maxRight = k + p[k],此时有两种情况:</p><p><img src="/images/longestPalindromicSubstring.png" alt="pictures001"></p><p><strong>第一种情况</strong>:i &gt; maxRight,此时初始化p[i] = 1, 然后判断s[i+p[i]] == s[i-p[i]],若不相等则停止,若相等,则++p[i]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">if (i &gt; maxRight) &#123;</span><br><span class="line">    p[i] = 1;</span><br><span class="line">    while(s[i+p[i]] == s[i-p[i]]) &#123;</span><br><span class="line">        ++p[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>第二种情况</strong>:i &lt;= maxRight,此时不在给 p[i] 赋值维为1,由回文串的对称性可得, 2k-i 是 i 关于 k 的对称点.此时由两种情况:</p><ol><li>以 2k-i 为中心的回文串的半径(如图蓝色箭头)大于等于 $maxRight - i$(空心箭头),由对称性可知,已知紫色箭头 5 和 6 关于 k 对称,且 2k-i 和 i 关于 k 对称,所以空心箭头 1 和 4 对称,2 和 3 对称,又箭头 7 和 8 对称,且箭头 1 和 2 分别是 7 和 8 的一部分,所以空心箭头 1 和 2 对称,故空心箭头 3 和 4 对称.所以p[i]的对称半径至少为 maxRight - i.所以首先 p[i]=maxRight - i,然后在依次往两边扩展判断是否对称.</li></ol><p><img src="/images/LongestPalindroomicSubstring1.jpg" alt="picture2"></p><ol><li>以 2k-i 为中心的回文半径小于 maxRight-i,根据和上面类似的推导,可以得知 p[i] = p[2k-i],且不在扩展.</li></ol><p><img src="/images/LongestPalindroomicSubstring2.png" alt="picture3"></p><p>复杂度分析:</p><ul><li><strong>时间复杂度:</strong>$O(n)$</li><li><strong>空间复杂度:</strong> $O(n)$ </li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">test</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span>  <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"abacdfgdcaba"</span>));</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"cbbd"</span>));</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"babad"</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">longestPalindrome</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        StringBuilder temp = <span class="keyword">new</span> StringBuilder(<span class="string">"#"</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; s.length(); ++i) &#123;</span><br><span class="line">            temp.append(s.charAt(i));</span><br><span class="line">            temp.append(<span class="string">"#"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        String str = temp.toString();</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        * maxCenter: 保存当前能延伸到最右端的回文字符串的中心位置</span></span><br><span class="line"><span class="comment">        * maxId: 保存当前最长回文子串的中心位置</span></span><br><span class="line"><span class="comment">        * p: 保存以该位置的字符维中心位置的最长回文字符的右边长度</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="keyword">int</span> maxCenter=<span class="number">0</span>, maxId=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> n = str.length();</span><br><span class="line">        <span class="keyword">int</span>[] p = <span class="keyword">new</span> <span class="keyword">int</span>[n];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;n; ++i) &#123;</span><br><span class="line">            <span class="keyword">int</span> syncCenter = <span class="number">2</span> * maxCenter - i;</span><br><span class="line">            p[i] = (i&lt;maxCenter + p[maxCenter) ? Math.min(p[syncCenter], maxCenter + p[maxCenter] - i) : <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span>(i-p[i] &gt;=<span class="number">0</span> &amp;&amp; i+p[i]&lt;n &amp;&amp; str.charAt(i-p[i]) == str.charAt(i+p[i])) &#123;</span><br><span class="line">                ++p[i];</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (i + p[i] &gt;= p[maxCenter] + maxCenter) &#123;</span><br><span class="line">                maxCenter = i;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (p[i] &gt; p[maxId]) &#123;</span><br><span class="line">                maxId = i;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> s.substring((maxId - p[maxId])/<span class="number">2</span>, (maxId + p[maxId]) / <span class="number">2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/04/10/001-LeetCode-LongestPalindromicSubstring/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github地址&lt;/a&gt;&lt;b
      
    
    </summary>
    
      <category term="Weekly Algorithm" scheme="http://yoursite.com/categories/Weekly-Algorithm/"/>
    
    
      <category term="Weekly Algorithm" scheme="http://yoursite.com/tags/Weekly-Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>issue_mathjax</title>
    <link href="http://yoursite.com/2018/04/08/issue-mathjax/"/>
    <id>http://yoursite.com/2018/04/08/issue-mathjax/</id>
    <published>2018-04-08T13:50:26.000Z</published>
    <updated>2018-04-08T13:52:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>$c^{T}x^{<em>}=y^{T}Ax^{</em>}=y^{*T}b$</p><script type="math/tex; mode=display">c^{T}x^{*}=y^{T}Ax^{*}=y^{*T}b</script><p>$c_x=yAx=y_b$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;$c^{T}x^{&lt;em&gt;}=y^{T}Ax^{&lt;/em&gt;}=y^{*T}b$&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;c^{T}x^{*}=y^{T}Ax^{*}=y^{*T}b&lt;/script&gt;&lt;p&gt;$c_x=yAx=y_b$
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Spring（二） —— 核心： 依赖注入</title>
    <link href="http://yoursite.com/2018/03/20/002-Spring-IoC/"/>
    <id>http://yoursite.com/2018/03/20/002-Spring-IoC/</id>
    <published>2018-03-20T14:05:32.000Z</published>
    <updated>2018-04-08T13:36:44.590Z</updated>
    
    <content type="html"><![CDATA[<p>在所有的 Java 程序中，总存在 A 对象调用 B 对象， B 对象调用 C 对象，这种情形被 Spring 称为依赖，即 A 对象依赖 B 对象，B 对象依赖 C 对象。对于整个 Java 项目而言，它是由一些相互调用的对象所构成， Spring 把这种相互调用的关系称为依赖关系。</p><p>Spring 提供的核心功能有两个：</p><ol><li>Spring容器作为一个超级大工厂，负责创建、管理所有的Bean（Java对象）；</li><li>Spring容器管理容器中Bean之间的依赖关系，它使用一种称为“依赖注入”的方式来管理 Bean 之间的依赖关系。</li></ol><p>依赖注入是一种优秀的解耦方法，使用依赖注入，程序中的各个组件不需要以硬编码的方式耦合在一起，同时不再需要工厂模式，我们只需使用配置文件就可将各个组件组织在一起。使用依赖注入，不仅可以为Bean注入普通的属性值，还可以注入其它 Bean 的引用。</p><h2 id="一、理解依赖注入"><a href="#一、理解依赖注入" class="headerlink" title="一、理解依赖注入"></a>一、理解依赖注入</h2><p>依赖注入(Dependency Injection)最开始的时候被称为控制反转(Inversion of Control,IoC).在传统方法中，我们主要有两种方式来实现依赖注入：</p><pre><code>1. 原始方法：调用者通过被调用对象的构造函数主动创建以来对象，在调用被依赖对象的方法；2. 工厂方法：调用者通过被依赖对象的的工厂方法来调用其方法； </code></pre><p>第一种方法，调用者通过“new”来创建被依赖对象，当被依赖对象做出更改时，必须修改调用者中的代码，这种以硬编码方法的耦合，非常不利于项目的升级维护。</p><p>工厂方法，大致分为三点：1、调用者面向被依赖对象的借口编程，2、被依赖对象的创建交给工厂实现，3、 调用者通过工厂来获得被依赖对象。但是这种方法会将工厂方法和被依赖对象耦合在一起。</p><p>由于Spring容器负责将被依赖对象赋值给调用者的成员变量，这样调用者无需主动获取被依赖对象，，这样对象间的耦合就通过配置文件来实现。</p><p>总之，Spring 框架的优点有：</p><pre><code>1. 程序无需主动使用 new 调用构造器来创建依赖对象，所有的java对象可交给 Spring 容器去创建。2. 调用者需要调用被依赖对象的方法时，调用者无需主动获取被依赖对象，Spring 容器会注入依赖对象。</code></pre><p>依赖注入有两种方式：</p><pre><code>1. 设值构造： IoC 容器使用成员变量的 setter 方法来注入被依赖对象。2. 构造注入： IoC 容器使用构造器来注入被依赖对象。</code></pre><h2 id="二、设值注入"><a href="#二、设值注入" class="headerlink" title="二、设值注入"></a>二、设值注入</h2><p>设置注入是指 IoC 容器通过成员变量的 setter 方法来注入被依赖对象，这种方法简单、直观，且大量使用。Spring 容器推荐面向接口编程，不管是调用者还是被依赖对象，都应该定义为借口，程序面向他们的借口编程而不是实现类编程。面向借口编程，可以更好地让规范和实现分离，从而更好的解耦，对于 Java EE，不管是 DAO组件还是应用业务逻辑组件，都应该先定义一个借口，来规范该组件应该实现的功能。</p><p>下面我们先定义两个接口在分别实现这两个接口。</p><p>1、 Person 接口</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> demo1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 定义一个使用斧头的方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">useAxe</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li><p>Axe 接口</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> demo1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Axe</span> </span>&#123;</span><br><span class="line">    <span class="comment">//Axe 借口中定义一个 chop() 方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">chop</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>Chinese 类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> demo1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Chinese</span> <span class="keyword">implements</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Axe axe;</span><br><span class="line">    <span class="comment">// 设值注入所需的setter方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAxe</span><span class="params">(Axe axe)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.axe = axe;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">useAxe</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 调用 axe 的chop方法</span></span><br><span class="line">        <span class="comment">// 表明 Person 对象依赖于 axe 对象</span></span><br><span class="line">        System.out.println(axe.chop());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>StoneAxe 类 </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> demo1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StoneAxe</span> <span class="keyword">implements</span> <span class="title">Axe</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">chop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"石头砍柴好慢"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><p>此时，我们已创建好所有的 Java 类，但是他们都还不是 Bean， Spring 也不知道哪些类相互耦合和相互依赖，这时我们需要使用 XML 配置文件来指定实例之间的依赖关系。Spring 2.0 开始，采用 XML Schema 来定义配置文件的语义约束。</p><p>XML 配置文件的内容如下：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">beans</span> <span class="attr">xmlns</span>=<span class="string">"http://www.springframework.org/schema/beans"</span></span></span><br><span class="line"><span class="tag">       <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">       <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--配置 chinese 实例，其实现类是 Chinese 类--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"chinese"</span> <span class="attr">class</span>=<span class="string">"demo1.Chinese"</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--驱动调用 chinese 的 setAxe 方法，将容器中的stoneAxe 作为参数传入--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"axe"</span> <span class="attr">ref</span>=<span class="string">"stoneAxe"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"stoneAxe"</span> <span class="attr">class</span>=<span class="string">"demo1.StoneAxe"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">beans</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>Spring 配置 Bean 实例时必须指定两个属性：</p><ol><li>id： 该 Bean 的唯一标志，Spring 根据 id 属性来管理 Bean，程序通过 id 属性值来访问该 Bean；</li><li><p>class: 指定该 Bean 的实现类，此处不可一世借口，必须是实现类，因为借口不可以实例化。</p><p>Spring 会自动检测每个 <bean...>定义里面的<property...>元素，Spring 会在自动调用默认构造器创建实例之后，立即调用对应的setter方法维 Bean 的成员变量注入值。</property...></bean...></p><p>下面的代码通过 ApplicationContest 的子类 ClassPathXmlApplicationContext 来获取 Person 实例。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> demo1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.springframework.context.ApplicationContext;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.support.ClassPathXmlApplicationContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BeanTest</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 创建容器</span></span><br><span class="line">        ApplicationContext ctx = <span class="keyword">new</span> ClassPathXmlApplicationContext(<span class="string">"beans1.xml"</span>);</span><br><span class="line">        <span class="comment">//获取 chinese 实例</span></span><br><span class="line">        Person p = ctx.getBean(<span class="string">"chinese"</span>, Chinese.class);</span><br><span class="line">        <span class="comment">// 调用 useAxe() 方法</span></span><br><span class="line">        p.useAxe();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出：</p><blockquote><p>石头砍柴好慢</p></blockquote></li></ol><p>如果某一天，系统需要改变 Axe 的实现，比如 SteelAxe 类，此时只需给出 Axe 的另一个类和更改 XML 配置文件，而Person、Chinese类如需任何改动。</p><p>SteelAxe 类<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> demo1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SteelAxe</span> <span class="keyword">implements</span> <span class="title">Axe</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">chop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"钢斧砍柴真快"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>XML 配置文件更改如下：<br>添加一行<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"steelAxe"</span> <span class="attr">class</span>=<span class="string">"demo1.SteelAxe"</span> /&gt;</span></span><br></pre></td></tr></table></figure></p><p>将<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"axe"</span> <span class="attr">ref</span>=<span class="string">"stoneAxe"</span> /&gt;</span></span><br></pre></td></tr></table></figure></p><p>改成：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"axe"</span> <span class="attr">ref</span>=<span class="string">"steelAxe"</span> /&gt;</span></span><br></pre></td></tr></table></figure></p><p>输出：</p><blockquote><p>钢斧砍柴真快</p></blockquote><p>从上面的变化可以看出，chinese 实例与具体的 Axe 实现类没有任何联系，chinese 实例只与 Axe 借口耦合，这就是 Spring 强调面向借口编程的原因。</p><h2 id="三、构造注入"><a href="#三、构造注入" class="headerlink" title="三、构造注入"></a>三、构造注入</h2><h2 id="四、对比两种方式"><a href="#四、对比两种方式" class="headerlink" title="四、对比两种方式"></a>四、对比两种方式</h2><h2 id="五-总结"><a href="#五-总结" class="headerlink" title="五 总结"></a>五 总结</h2><p>Spring IoC容器的三个基本要点：</p><ol><li><p>应用程序各组件面向借口编程。面向接口编程可以讲组件之间的耦合关系提升到借口层次，从而有利于项目的后期维护。</p></li><li><p>应用程序的各组件不在有程序主动创建，而是由 Spring 容器负责产生并初始化。</p></li><li><p>Spring 采用配置文件或注解来管理 Bean 的实现类、依赖关系， Spring容器则根据配置文件或注解，利用反射机制来创建实例，并为之注入依赖关系</p></li></ol><p>$c^{T}x^{<em>}=y^{T}Ax^{</em>}=y^{*T}b$</p><script type="math/tex; mode=display">c^Tx^*=y^{T}Ax^*=y^{*T}b</script><p>2</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在所有的 Java 程序中，总存在 A 对象调用 B 对象， B 对象调用 C 对象，这种情形被 Spring 称为依赖，即 A 对象依赖 B 对象，B 对象依赖 C 对象。对于整个 Java 项目而言，它是由一些相互调用的对象所构成， Spring 把这种相互调用的关系称
      
    
    </summary>
    
      <category term="Spring" scheme="http://yoursite.com/categories/Spring/"/>
    
    
      <category term="Spring" scheme="http://yoursite.com/tags/Spring/"/>
    
  </entry>
  
  <entry>
    <title>Spring（一） —— 入门</title>
    <link href="http://yoursite.com/2018/03/17/001-Spring-Introduction/"/>
    <id>http://yoursite.com/2018/03/17/001-Spring-Introduction/</id>
    <published>2018-03-17T11:25:00.000Z</published>
    <updated>2018-03-20T14:32:46.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>声明</strong>：本教程均在Spring4.0.4版本中实现</p><p>目前Java EE主流的轻量级开发框架有SSH(Struts+spring+Hibernate)和SSM(Spring+Sprng MVC+Mybatis),从中可以看出，无论是SSH还是SSM，Spring在Java开发中有着重要的作用。</p><p>Spring以IoC、AOP维主要思想，它是一个“一站式”框架，即在 Java EE的三层架构，即表现层（Web层）、业务逻辑层（Service层）、数据访问层（DAO）层中，每一层均提供了不同的解决技术。</p><p>当使用Spring框架时，必须使用Spring Core Container（即Spring容器），它是Spring的核心机制，主要由org.springframework.core、org.springframework.beans、org.springframework.context、prg.springframework.expression四个包及其子包组成，主要提供Spring IoC容器支持。</p><h2 id="一-Maven依赖"><a href="#一-Maven依赖" class="headerlink" title="一 Maven依赖"></a>一 Maven依赖</h2><p>本教程全都基于Maven管理工具来构建基于 Spring 框架的应用，因此基础Maven依赖主要包括三个spring-core、spring-context、spring-beans<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- spring-core --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.0.4.RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- spring-context --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-context<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.0.4.RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Spring-beans --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-beans<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.0.4.RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></p><h2 id="二-Spring-Bean概念"><a href="#二-Spring-Bean概念" class="headerlink" title="二 Spring Bean概念"></a>二 Spring Bean概念</h2><p>Spring 核心容器像是一个超级大工厂，所有的Java对象都会成为 Spring 核心容器的管理对象，Spring把容器中的一切管理对象成为Bean。</p><p>Spring容器中的Bean和Java Bean不同，它不需要像 Java Bean 那样遵守一些规范，Spring对Bean没有任何的要求。<strong>在 Spring 中，一切 Java 对象，都是Bean</strong>。</p><p>下面我们先创建一个简单的 Java 类<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.crazyit.app.service;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Axe</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">chop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"使用斧头劈柴"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>接着我们在创建一个 Person 类，在 Person 类的 useAxe() 方法中使用 Axe 对象的 chop() 方法，这样 Person 对象就依赖于 Axe对象。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.crazyit.app.service;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Axe axe;zhong</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAxe</span><span class="params">(Axe axe)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.axe = axe;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">useAxe</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"我打算去砍点柴火！"</span>);</span><br><span class="line">        <span class="comment">// 调用 axe 的 chop() 方法</span></span><br><span class="line">        <span class="comment">// 表明 Person 对象依赖于 axe 对象</span></span><br><span class="line">        System.out.print(axe.chop());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>我们创建好 Java 类后，如何让它们成为Spring Bean，这就要通过XML配置文件或者注解来让 Spring 管理这些 Bean，该配置文件内容如下：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span><br><span class="line"><span class="comment">&lt;!--  Spring 配置文件的根元素没使用spring-beans-4.0.xsd 语义约束--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">beans</span> <span class="attr">xmlns</span>=<span class="string">"http://www.springframework.org/schema/beans"</span></span></span><br><span class="line"><span class="tag">       <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">       <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--配置名为person的Bean，其实现类为org.crazyit.app.service.Person--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"person"</span> <span class="attr">class</span>=<span class="string">"org.crazyit.app.service.Person"</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--控制调用setAxe()方法，将容器的名为axe的Bean作为参数传入--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"axe"</span> <span class="attr">ref</span>=<span class="string">"axe"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--配置名为axe的Bean，其实现类为org.crazyit.app.service.Axe--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"axe"</span> <span class="attr">class</span>=<span class="string">"org.crazyit.app.service.Axe"</span> /&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--配置名为win的Bean，其实现类为javax.swing.JFrame--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"win"</span> <span class="attr">class</span>=<span class="string">"javax.swing.JFrame"</span> /&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--配置名为date的Bean，其实现类为java.util.Date--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"date"</span> <span class="attr">class</span>=<span class="string">"java.util.Date"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">beans</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>该配置文件的根元素维<bean...>,它包括多个<bean...>元素，每个<bean...>元素定义一个Bean，上面的配置文件共定义了四个Bean，其中两个为我们自定义的 Java 类，后面两个使用了 JDK 标准库中的类，这和前面说的 <strong>Spring中,一切 Java 对象皆可为 Bean</strong>一致。</bean...></bean...></bean...></p><p>我们通过如下元素来将 Java 类变为 Bean，<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--配置名为axe的Bean，其实现类为org.crazyit.app.service.Axe--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"axe"</span> <span class="attr">class</span>=<span class="string">"org.crazyit.app.service.Axe"</span> /&gt;</span></span><br></pre></td></tr></table></figure></p><p>其中 id 属性为该 Bean 指定了唯一标志，class 属性指定该Bean要实现哪个类，这个类可以是任何 Java 类：自定义类和标准库类都可以。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--配置名为person的Bean，其实现类为org.crazyit.app.service.Person--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"person"</span> <span class="attr">class</span>=<span class="string">"org.crazyit.app.service.Person"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--控制调用setAxe()方法，将容器的名为axe的Bean作为参数传入--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"axe"</span> <span class="attr">ref</span>=<span class="string">"axe"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br></pre></td></tr></table></figure><p><bean...> 元素还可以包含多个 <property...> 子元素。它用来设置该 Bean 的属性的值，通过调用setter方法来实现，name 属性指定要设置哪个属性的值，ref或value属性决定使用什么内容来作为参数：</property...></bean...></p><ol><li>ref 表示使用容器中的某个 Bean 来作为 setter 的参数；</li><li>value 指明使用内置对象及其包装类来作为setter参数</li></ol><p><strong>底层浅析：</strong></p><p>1.<bean...> 元素默认以反射方式来调用该类的无参构造函数创建 Bean； 其实现方式类似如下代码，以 id 为 person 的Bean为例：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">String idStr = ...; <span class="comment">// 解析&lt;bean.../&gt;元素的id属性得到该字符串值“person”</span></span><br><span class="line">string classStr = ...; <span class="comment">//解析&lt;bean.../&gt;元素的 class 属性得到该字符串值“org.crazyit.app.service.Person”</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">clazz</span> </span>= Class.forName(classStr);</span><br><span class="line">Object obj = <span class="class"><span class="keyword">class</span>.<span class="title">newInstance</span>()</span>;</span><br><span class="line"><span class="comment">// container 代表 Spring 容器</span></span><br><span class="line">container.put(idStr, obj);</span><br></pre></td></tr></table></figure></bean...></p><ol><li><property...>也是通过反射机制来调用对象的 setter 方法来实现的，实现方式如下，以 id 为 person 的Bean为例：<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">String nameStr = ...; <span class="comment">// 解析&lt;property .../&gt;元素的name属性，得到该属性值“axe”</span></span><br><span class="line">String refStr = ...; <span class="comment">//解析&lt;property.../&gt;元素的ref属性得到字符串“axe”</span></span><br><span class="line">String setterName = <span class="string">"set"</span> + nameStr.substring(<span class="number">0</span>, <span class="number">1</span>).toUpperCase() + nameStr.substring(<span class="number">1</span>); <span class="comment">//生成要调用的setter方法名</span></span><br><span class="line"><span class="comment">//获取容器中名为refStr的Bean，</span></span><br><span class="line">Object paramBean = container.get(refStr);</span><br><span class="line">Method setter = clazz.getMethos(setterName, paramBean,getClass());</span><br><span class="line">setter.incoke(obj, paramBean)</span><br></pre></td></tr></table></figure></property...></li></ol><h2 id="三-访问-Bean"><a href="#三-访问-Bean" class="headerlink" title="三 访问 Bean"></a>三 访问 Bean</h2><p>我们在xml配置文件中配置好Bean和它们的相关依赖后，我们需要通过Spring容器来访问容器中的Bean。<code>ApplicationContext</code>是Spring容器最常用的接口，他有两个实现类：</p><ol><li>ClassPathXmlApplicationContext：该类从类加载路径下搜索配置文件，并根据配置文件来创建Spring容器；</li><li>FileSystemXmlApplicationContext：该类从文件系统的相对路径或者绝对路径下去搜索配置文件；</li></ol><p>对于应用程序，类加载路径总是不变的，因此通常用ClassPathXmlApplicationContext来创建Spring容器 。代码如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> lee;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.crazyit.app.service.Person;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.ApplicationContext;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.support.ClassPathXmlApplicationContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BeanTest</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//创建Spring容器</span></span><br><span class="line">        ApplicationContext ctx = <span class="keyword">new</span> ClassPathXmlApplicationContext(<span class="string">"beans.xml"</span>);</span><br><span class="line">        <span class="comment">//获取id为person的Bean</span></span><br><span class="line">        Person p = ctx.getBean(<span class="string">"person"</span>, Person.class);</span><br><span class="line">        <span class="comment">// 调用useAxe() 方法</span></span><br><span class="line">        p.useAxe();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>通过Spring容器来获取Bean对象主要由两个方法：</p><ol><li>Object getBean(String id): 根据容器中Bean的id来获取Bean，该方法需要强制类型转换。</li><li>T getBean(String id, Class <t> requiredType):根据容器中Bean的id来获取Bean，该方法无需进行强制类型转换。</t></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;声明&lt;/strong&gt;：本教程均在Spring4.0.4版本中实现&lt;/p&gt;
&lt;p&gt;目前Java EE主流的轻量级开发框架有SSH(Struts+spring+Hibernate)和SSM(Spring+Sprng MVC+Mybatis),从中可以看出，无论
      
    
    </summary>
    
      <category term="Spring" scheme="http://yoursite.com/categories/Spring/"/>
    
    
      <category term="Spring" scheme="http://yoursite.com/tags/Spring/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch Deep Learning for NLP with Pytorch</title>
    <link href="http://yoursite.com/2018/01/20/PyTorch-Deep-Learning-for-NLP-with-Pytorch/"/>
    <id>http://yoursite.com/2018/01/20/PyTorch-Deep-Learning-for-NLP-with-Pytorch/</id>
    <published>2018-01-20T14:04:52.000Z</published>
    <updated>2018-03-20T14:03:28.000Z</updated>
    
    <content type="html"><![CDATA[<p>此教程翻译自<a href="http://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html" target="_blank" rel="noopener">官方教程</a></p><p><strong>作者:</strong><a href="https://github.com/rguthrie3/DeepLearningForNLPInPytorch" target="_blank" rel="noopener">Robert Guthrie</a></p><p>本教程将引导您学习在使用 Pytorch 进行深度学习编程中的一些主要思想。许多概念（概念图抽象和autograd）并不是 Pytorch 所特有的，而是与任何深度学习工具有关。</p><p>我写这个教程，专门针对自然语言处理和那些从未用过任何深度学习框架（比如Tensorflow,Theano,Keras, Dynet)的写过自然语言处理的人。该教程假定你知道自然语言处理的核心知识： 词性标注、语言建模等。也假定你熟悉神经网络基础（例如来自 Russel 和 Norvig书）。通常，这些书籍涵盖基础的前馈神经网络的反向传播算法，并指出他们是线性和非线性组合的链。<br>本教程旨在让您开始编写深度学习的代码和让你知道这些必备知识。</p><p>请注意，这是关于模型而不是数据。对于所有的模型，我们只创建一个小维度的小的测试样例，以便您可以看到权重在它训练时是如何变化的。如果有一些真实数据你想去尝试，你应该能够撕掉这个笔记上的任何模型，并使用他们。</p><h1 id="介绍-PyTorch"><a href="#介绍-PyTorch" class="headerlink" title="介绍 PyTorch"></a>介绍 PyTorch</h1><h2 id="介绍-Torch-的张量库"><a href="#介绍-Torch-的张量库" class="headerlink" title="介绍 Torch 的张量库"></a>介绍 Torch 的张量库</h2><p>所有深度学习都是基于张量计算，这是矩阵的推广，它的维度可以超过两维。我们将在后面深入地看到这究竟意味着什么。首先，让我们看看我们可以使用张量做什么。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;此教程翻译自&lt;a href=&quot;http://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官方教程&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;str
      
    
    </summary>
    
      <category term="PyTorch" scheme="http://yoursite.com/categories/PyTorch/"/>
    
    
      <category term="PyTorch" scheme="http://yoursite.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Learn Progress</title>
    <link href="http://yoursite.com/2018/01/20/Learn-Progress/"/>
    <id>http://yoursite.com/2018/01/20/Learn-Progress/</id>
    <published>2018-01-20T05:14:25.000Z</published>
    <updated>2018-01-20T05:21:08.773Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目标检测-Object-Detection"><a href="#目标检测-Object-Detection" class="headerlink" title="目标检测(Object Detection)"></a>目标检测(Object Detection)</h1><ul><li><p>R-CNN 论文笔记 </p><ul><li>论文<a href="https://dl.dropboxusercontent.com/s/293tu0hh9ww08co/r-cnn-cvpr.pdf?dl=0" target="_blank" rel="noopener">《Rich feature hierarchies for accurate object detection and semantic segmentation》</a></li><li><a href="http://www.cnblogs.com/kingstrong/p/4969472.html" target="_blank" rel="noopener">http://www.cnblogs.com/kingstrong/p/4969472.html</a></li><li><a href="http://blog.gater.vip/articles/2015/11/02/1478607351098.html" target="_blank" rel="noopener">http://blog.gater.vip/articles/2015/11/02/1478607351098.html</a></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;目标检测-Object-Detection&quot;&gt;&lt;a href=&quot;#目标检测-Object-Detection&quot; class=&quot;headerlink&quot; title=&quot;目标检测(Object Detection)&quot;&gt;&lt;/a&gt;目标检测(Object Detection)
      
    
    </summary>
    
      <category term="Tutorial" scheme="http://yoursite.com/categories/Tutorial/"/>
    
    
      <category term="Tutorial" scheme="http://yoursite.com/tags/Tutorial/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch:数据加载和预处理</title>
    <link href="http://yoursite.com/2018/01/05/PyTorch-Data-Loading-and-Processing-TUtorial/"/>
    <id>http://yoursite.com/2018/01/05/PyTorch-Data-Loading-and-Processing-TUtorial/</id>
    <published>2018-01-05T11:56:09.000Z</published>
    <updated>2018-04-10T12:25:58.335Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2018/01/05/PyTorch-Data-Loading-and-Processing-TUtorial/" target="_blank" rel="noopener">Github地址</a><br><a href="https://www.jianshu.com/p/8b4cdd36a1b3" target="_blank" rel="noopener">简书地址</a><br><a href="https://blog.csdn.net/u014630987/article/details/79886956" target="_blank" rel="noopener">CSDN地址</a></p><p>此教程翻译自<a href="http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#" target="_blank" rel="noopener">PyTorch官方教程</a></p><p><strong>作者</strong>: <a href="https://chsasank.github.io/" target="_blank" rel="noopener">Sasank Chilamkurthy</a></p><p>在解决任何机器学习问题上，在准备数据上会付出很大努力。PyTorch 提供了许多工具， 使数据加载变得简单，希望能使你的代码更具可读性。本教程中，我们将看到图和从一个不重要的数据集中加载和预处理/增强数据。</p><p>要运行本教程，请确保已安装一下软件包：</p><ol><li><code>scikit-image</code>: 用于图像 IO 和 变换</li><li><code>pandas</code>： 更简单的 csv 解析</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function, division</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io, transform</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, utils</span><br><span class="line"></span><br><span class="line"><span class="comment"># Ignore warnings</span></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</span><br><span class="line"></span><br><span class="line">plt.ion()   <span class="comment"># interactive mode</span></span><br></pre></td></tr></table></figure><p>我们将要处理的数据集是面部姿势，意味着一张人脸将像下面这样被标注：<br><img src="/images/landmarked_face2.png" alt="landmarked_face2"></p><p>每张人脸总共有68个不同的地方被标注。</p><p><strong>注意</strong>:<br>数据下载地址为 <a href="https://download.pytorch.org/tutorial/faces.zip，" target="_blank" rel="noopener">https://download.pytorch.org/tutorial/faces.zip，</a> 图像位于名为“faces/“的目录中。这个数据集实际上是通过对来自 imagenet 的几张标注为 ‘face’ 的图片应用优秀的 <code>dlib</code> 的姿态估计来生成的。</p><p>数据集带有一个 csv 标注文件，里面的标注内容看起来像下面这样：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">image_name,part_0_x,part_0_y,part_1_x,part_1_y,part_2_x, ... ,part_67_x,part_67_y</span><br><span class="line">0805personali01.jpg,27,83,27,98, ... 84,134</span><br><span class="line">1084239450_e76e00b7e7.jpg,70,236,71,257, ... ,128,312</span><br></pre></td></tr></table></figure></p><p>让我们快速读取 csv 文件，并把标记数据保存在一个(N, 2)的数组中，其中 N 是特征点的数量。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">landmarks_frame = pd.read_csv(<span class="string">"./data//faces/face_landmarks.csv"</span>)</span><br><span class="line">n = <span class="number">65</span></span><br><span class="line">img_name = landmarks_frame.ix[n, <span class="number">0</span>]</span><br><span class="line">landmarks = landmarks_frame.ix[n, <span class="number">1</span>:].as_matrix().astype(<span class="string">'float'</span>)</span><br><span class="line">landmarks = landmarks.reshape(<span class="number">-1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Image name: &#123;&#125;"</span>.format(img_name))</span><br><span class="line">print(<span class="string">"Landmarks shape: &#123;&#125;"</span>.format(landmarks.shape))</span><br><span class="line">print(<span class="string">"First 4 Landmarks: &#123;&#125;"</span>.format(landmarks[:<span class="number">4</span>]))</span><br></pre></td></tr></table></figure></p><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Image name: person-7.jpg</span><br><span class="line">Landmarks shape: (68, 2)</span><br><span class="line">First 4 Landmarks: [[ 32.  65.]</span><br><span class="line"> [ 33.  76.]</span><br><span class="line"> [ 34.  86.]</span><br><span class="line"> [ 34.  97.]]</span><br></pre></td></tr></table></figure></p><p> 让我们写一个简单的帮主函数来显示图像及其特征点，并用他来显示一个样本。<br> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_landmarks</span><span class="params">(image, landmarks)</span>:</span></span><br><span class="line">    <span class="string">"""SHow image with landmarks"""</span></span><br><span class="line">    plt.imshow(image)</span><br><span class="line">    plt.scatter(landmarks[:, <span class="number">0</span>], landmarks[:, <span class="number">1</span>], s=<span class="number">10</span>, marker=<span class="string">"."</span>, c=<span class="string">"r"</span>)</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">img = io.imread(os.path.join(<span class="string">"./data/faces/"</span>, img_name))</span><br><span class="line">show_landmarks(io.imread(os.path.join(<span class="string">"./data/faces/"</span>, img_name)), landmarks)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p> 输出：<br> <img src="/images/sphx_glr_data_loading_tutorial_001.png" alt="sphx_glr_data_loading_tutorial_001"></p><p><strong>注意</strong>：要得到以上结果，请把 <code>plt.ion()</code> 注释掉。</p><h1 id="Dataset-类"><a href="#Dataset-类" class="headerlink" title="Dataset 类"></a>Dataset 类</h1><p><code>torch.utils.data.Dataset</code> 是一个表示数据集的抽象类。你自定义的数据集类应该继承自 <code>Dataset</code> 并重写如下方法：</p><ul><li><code>__len__</code>: 返回数据集的大小， <code>len(dataset)</code></li><li><code>__getitem__</code>: 是数据集支持索引操作, <code>dataset[i]</code></li></ul><p>让我们维我们的人脸特征点数据集创建一个数据集类。我们将在 <code>__init__</code> 中读取 csv， 但是让读取图片的操作在 <code>__getitem__</code> 中进行。这是内存高效的，因为所有的图像不是一次存储在内存中，而是根据需要进行读取。</p><p>我们数据集的样本将是一个字典<code>{&#39;image&#39;: image, &#39;landmarks&#39;: landmarks}</code>。我们的数据集将接受一个可选参数<code>transform’ 以便可以对样本应用任何需要的处理。我们将在下一节看到</code>transform` 的好处。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FaceLandmarksDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="string">"""Face Landmarks dataset."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, csv_file, root_dir, transform=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            csv_file (string): Path to the csv file with annotations.</span></span><br><span class="line"><span class="string">            root_dir (string): Directory with all the images.</span></span><br><span class="line"><span class="string">            transform (callable, optional): Optional transform to be applied</span></span><br><span class="line"><span class="string">                on a sample.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.landmarks_frame = pd.read_csv(csv_file)</span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.transform = transform</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.landmarks_frame)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        img_name = os.path.join(self.root_dir, self.landmarks_frame.ix[idx, <span class="number">0</span>])</span><br><span class="line">        image = io.imread(img_name)</span><br><span class="line">        landmarks = self.landmarks_frame.ix[idx, <span class="number">1</span>:].as_matrix().astype(<span class="string">'float'</span>)</span><br><span class="line">        landmarks = landmarks.reshape(<span class="number">-1</span>, <span class="number">2</span>)</span><br><span class="line">        sample = &#123;<span class="string">'image'</span>: image, <span class="string">'landmarks'</span>: landmarks&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            sample = self.transform(sample)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sample</span><br></pre></td></tr></table></figure><p>让我们初始化这个类的实例，并在数据样本上迭代。我们讲打印开始4个样本的大小并显示他们的特征点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">face_dataset = FaceLandmarksDataset(csv_file=<span class="string">'faces/face_landmarks.csv'</span>,</span><br><span class="line">                                    root_dir=<span class="string">'faces/'</span>)</span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(face_dataset)):</span><br><span class="line">    sample = face_dataset[i]</span><br><span class="line"></span><br><span class="line">    print(i, sample[<span class="string">'image'</span>].shape, sample[<span class="string">'landmarks'</span>].shape)</span><br><span class="line"></span><br><span class="line">    ax = plt.subplot(<span class="number">1</span>, <span class="number">4</span>, i + <span class="number">1</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    ax.set_title(<span class="string">'Sample #&#123;&#125;'</span>.format(i))</span><br><span class="line">    ax.axis(<span class="string">'off'</span>)</span><br><span class="line">    show_landmarks(**sample)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">3</span>:</span><br><span class="line">        plt.show()</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p><img src="/images/sphx_glr_data_loading_tutorial_002.png" alt="sphx_glr_data_loading_tutorial_002"></p><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">0 (324, 215, 3) (68, 2)</span><br><span class="line">1 (500, 333, 3) (68, 2)</span><br><span class="line">2 (250, 258, 3) (68, 2)</span><br><span class="line">3 (434, 290, 3) (68, 2)</span><br></pre></td></tr></table></figure></p><h1 id="Transform-变换"><a href="#Transform-变换" class="headerlink" title="Transform (变换)"></a>Transform (变换)</h1><p>从上面的例子我们可以看到一个问题：样本的尺寸不一样。大部分的神经网络希望一个固定大小的图像。因此，我们需要写一些预处理代码。让我们来创建三种变换：</p><ul><li><code>Rescale</code>: 缩放图像</li><li><code>RandomCrop</code>: 随机剪裁图像，这是一种数据增强的方法</li><li><code>ToTensor</code>: 把 numpy 图像转换为 PyTorch 图像（我们需要交换轴）</li></ul><p>我们将把它们写成一个可调用的类而不是函数，所以变换所需的参数不必在每次调用时都传递。为此，我们只需实现 <code>__call__</code> 方法，如果需要可以实现 <code>__init__</code> 方法。我们可以向下面这样使用他们：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tsfm = Transform(params)</span><br><span class="line">transformed_sample = tsfm(sample)</span><br></pre></td></tr></table></figure></p><p>请观察下面的变换是如何应用在图像和特征点上的。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Rescale</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Rescale the image in a sample to a given size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        output_size (tuple or int): Desired output size. If tuple, output is</span></span><br><span class="line"><span class="string">            matched to output_size. If int, smaller of image edges is matched</span></span><br><span class="line"><span class="string">            to output_size keeping aspect ratio the same.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, output_size)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> isinstance(output_size, (int, tuple))</span><br><span class="line">        self.output_size = output_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, sample)</span>:</span></span><br><span class="line">        image, landmarks = sample[<span class="string">'image'</span>], sample[<span class="string">'landmarks'</span>]</span><br><span class="line"></span><br><span class="line">        h, w = image.shape[:<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">if</span> isinstance(self.output_size, int):</span><br><span class="line">            <span class="keyword">if</span> h &gt; w:</span><br><span class="line">                new_h, new_w = self.output_size * h / w, self.output_size</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                new_h, new_w = self.output_size, self.output_size * w / h</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            new_h, new_w = self.output_size</span><br><span class="line"></span><br><span class="line">        new_h, new_w = int(new_h), int(new_w)</span><br><span class="line"></span><br><span class="line">        img = transform.resize(image, (new_h, new_w))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># h and w are swapped for landmarks because for images,</span></span><br><span class="line">        <span class="comment"># x and y axes are axis 1 and 0 respectively</span></span><br><span class="line">        landmarks = landmarks * [new_w / w, new_h / h]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'image'</span>: img, <span class="string">'landmarks'</span>: landmarks&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomCrop</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Crop randomly the image in a sample.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        output_size (tuple or int): Desired output size. If int, square crop</span></span><br><span class="line"><span class="string">            is made.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, output_size)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> isinstance(output_size, (int, tuple))</span><br><span class="line">        <span class="keyword">if</span> isinstance(output_size, int):</span><br><span class="line">            self.output_size = (output_size, output_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">assert</span> len(output_size) == <span class="number">2</span></span><br><span class="line">            self.output_size = output_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, sample)</span>:</span></span><br><span class="line">        image, landmarks = sample[<span class="string">'image'</span>], sample[<span class="string">'landmarks'</span>]</span><br><span class="line"></span><br><span class="line">        h, w = image.shape[:<span class="number">2</span>]</span><br><span class="line">        new_h, new_w = self.output_size</span><br><span class="line"></span><br><span class="line">        top = np.random.randint(<span class="number">0</span>, h - new_h)</span><br><span class="line">        left = np.random.randint(<span class="number">0</span>, w - new_w)</span><br><span class="line"></span><br><span class="line">        image = image[top: top + new_h,</span><br><span class="line">                      left: left + new_w]</span><br><span class="line"></span><br><span class="line">        landmarks = landmarks - [left, top]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'image'</span>: image, <span class="string">'landmarks'</span>: landmarks&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ToTensor</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Convert ndarrays in sample to Tensors."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, sample)</span>:</span></span><br><span class="line">        image, landmarks = sample[<span class="string">'image'</span>], sample[<span class="string">'landmarks'</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># swap color axis because</span></span><br><span class="line">        <span class="comment"># numpy image: H x W x C</span></span><br><span class="line">        <span class="comment"># torch image: C X H X W</span></span><br><span class="line">        image = image.transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'image'</span>: torch.from_numpy(image),</span><br><span class="line">                <span class="string">'landmarks'</span>: torch.from_numpy(landmarks)&#125;</span><br></pre></td></tr></table></figure></p><h2 id="组合变换"><a href="#组合变换" class="headerlink" title="组合变换"></a>组合变换</h2><p>现在，我们应用这些变换到我们的样本上。</p><p>假如我们想先把图像的较短的一边缩放到256，然后从中随机剪裁一个224*224大小的图像。即我们想要组合 <code>Rescale</code> 和 <code>RandomCrop</code> 两个变换。</p><p><code>torchvision.transforms.Compose</code> 是一个简单的可调用类，允许我们来组合多个变换<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scale = Rescale(<span class="number">256</span>)</span><br><span class="line">crop = RandomCrop(<span class="number">128</span>)</span><br><span class="line">composed = transforms.Compose([Rescale(<span class="number">256</span>),</span><br><span class="line">                               RandomCrop(<span class="number">224</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply each of the above transforms on sample.</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">sample = face_dataset[<span class="number">65</span>]</span><br><span class="line"><span class="keyword">for</span> i, tsfrm <span class="keyword">in</span> enumerate([scale, crop, composed]):</span><br><span class="line">    transformed_sample = tsfrm(sample)</span><br><span class="line"></span><br><span class="line">    ax = plt.subplot(<span class="number">1</span>, <span class="number">3</span>, i + <span class="number">1</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    ax.set_title(type(tsfrm).__name__)</span><br><span class="line">    show_landmarks(**transformed_sample)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="/images/sphx_glr_data_loading_tutorial_003.png" alt="sphx_glr_data_loading_tutorial_003"></p><h2 id="迭代数据集"><a href="#迭代数据集" class="headerlink" title="迭代数据集"></a>迭代数据集</h2><p>我们把这些放在一个来创建一个包含组合变换的数据集。总之，每当这个数据集被采样时执行一下操作：</p><ul><li>即时从文件中读取图像。</li><li>对图像应用变换。</li><li>由于其中一个变换是随机的，因此数据的采样得到增强。</li></ul><p>我们可以使用和之前一样的 <code>for i in range</code> 循环来迭代创建的数据集。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">transformed_dataset = FaceLandmarksDataset(csv_file=<span class="string">'faces/face_landmarks.csv'</span>,</span><br><span class="line">                                           root_dir=<span class="string">'faces/'</span>,</span><br><span class="line">                                           transform=transforms.Compose([</span><br><span class="line">                                               Rescale(<span class="number">256</span>),</span><br><span class="line">                                               RandomCrop(<span class="number">224</span>),</span><br><span class="line">                                               ToTensor()</span><br><span class="line">                                           ]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(transformed_dataset)):</span><br><span class="line">    sample = transformed_dataset[i]</span><br><span class="line"></span><br><span class="line">    print(i, sample[<span class="string">'image'</span>].size(), sample[<span class="string">'landmarks'</span>].size())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">3</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure></p><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">0 torch.Size([3, 224, 224]) torch.Size([68, 2])</span><br><span class="line">1 torch.Size([3, 224, 224]) torch.Size([68, 2])</span><br><span class="line">2 torch.Size([3, 224, 224]) torch.Size([68, 2])</span><br><span class="line">3 torch.Size([3, 224, 224]) torch.Size([68, 2])</span><br></pre></td></tr></table></figure></p><p>但是，通过使用简单的for循环遍历数据，我们将失去许多功能。特别是我们错过了：</p><ul><li>批处理数据</li><li>打乱数据</li><li>使用多线程并行加载数据</li></ul><p><code>torch.utils.data.DataLoader</code> 是一个提供以上所有的功能的迭代器。下面使用的参数应该是清楚的。其中一个又去的参数是 <code>collate_fn</code>。你可以指定如何使用 collate_fn 对样本进行批处理。但是，对大多数情况来说，默认的自动分页应该可以正常工作的很好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">dataloader = DataLoader(transformed_dataset, batch_size=<span class="number">4</span>,</span><br><span class="line">                        shuffle=<span class="keyword">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Helper function to show a batch</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_landmarks_batch</span><span class="params">(sample_batched)</span>:</span></span><br><span class="line">    <span class="string">"""Show image with landmarks for a batch of samples."""</span></span><br><span class="line">    images_batch, landmarks_batch = \</span><br><span class="line">            sample_batched[<span class="string">'image'</span>], sample_batched[<span class="string">'landmarks'</span>]</span><br><span class="line">    batch_size = len(images_batch)</span><br><span class="line">    im_size = images_batch.size(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    grid = utils.make_grid(images_batch)</span><br><span class="line">    plt.imshow(grid.numpy().transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size):</span><br><span class="line">        plt.scatter(landmarks_batch[i, :, <span class="number">0</span>].numpy() + i * im_size,</span><br><span class="line">                    landmarks_batch[i, :, <span class="number">1</span>].numpy(),</span><br><span class="line">                    s=<span class="number">10</span>, marker=<span class="string">'.'</span>, c=<span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line">        plt.title(<span class="string">'Batch from dataloader'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i_batch, sample_batched <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">    print(i_batch, sample_batched[<span class="string">'image'</span>].size(),</span><br><span class="line">          sample_batched[<span class="string">'landmarks'</span>].size())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># observe 4th batch and stop.</span></span><br><span class="line">    <span class="keyword">if</span> i_batch == <span class="number">3</span>:</span><br><span class="line">        plt.figure()</span><br><span class="line">        show_landmarks_batch(sample_batched)</span><br><span class="line">        plt.axis(<span class="string">'off'</span>)</span><br><span class="line">        plt.ioff()</span><br><span class="line">        plt.show()</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p><img src="/images/sphx_glr_data_loading_tutorial_004.png" alt="sphx_glr_data_loading_tutorial_004"></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">0 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])</span><br><span class="line">1 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])</span><br><span class="line">2 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])</span><br><span class="line">3 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])</span><br></pre></td></tr></table></figure></p><h1 id="后记：torchvision"><a href="#后记：torchvision" class="headerlink" title="后记：torchvision"></a>后记：torchvision</h1><p>在此教程中，我们看到了如何写和使用数据集（dataset），变换（transform）和数据加载器（dataloader）。<br><code>torchvision</code> 包提供了一些常见数据集和变换。你甚至可能不需要编写自定义类。<code>torchvision</code> 提供了一个更通过的数据集： ImageFolder。它假设图像按以下方式组织：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">root/ants/xxx.png</span><br><span class="line">root/ants/xxy.jpeg</span><br><span class="line">root/ants/xxz.png</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">root/bees/123.jpg</span><br><span class="line">root/bees/nsdf3.png</span><br><span class="line">root/bees/asd932_.png</span><br></pre></td></tr></table></figure></p><p>其中 ‘ant’,’bees’等是类别标签，类似的通用的操作于 <code>PIL.Image</code> 的变换，如<code>RandomHOrizontalFlip</code>， <code>Scale</code> 也是可以获取的。你可以向下面一样使用这些变换来写一个数据加载器。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, datasets</span><br><span class="line"></span><br><span class="line">data_transform = transforms.Compose([</span><br><span class="line">        transforms.RandomSizedCrop(<span class="number">224</span>),</span><br><span class="line">        transforms.RandomHorizontalFlip(),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">                             std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ])</span><br><span class="line">hymenoptera_dataset = datasets.ImageFolder(root=<span class="string">'hymenoptera_data/train'</span>,</span><br><span class="line">                                           transform=data_transform)</span><br><span class="line">dataset_loader = torch.utils.data.DataLoader(hymenoptera_dataset,</span><br><span class="line">                                             batch_size=<span class="number">4</span>, shuffle=<span class="keyword">True</span>,</span><br><span class="line">                                             num_workers=<span class="number">4</span>)</span><br></pre></td></tr></table></figure></p><p>有关训练代码的示例，请阅读 迁移学习章节。</p><p><a href="http://pytorch.org/tutorials/_downloads/data_loading_tutorial.py" target="_blank" rel="noopener">Python 源码</a><br><a href="http://pytorch.org/tutorials/_downloads/data_loading_tutorial.ipynb" target="_blank" rel="noopener">Jupyter</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/01/05/PyTorch-Data-Loading-and-Processing-TUtorial/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github地址&lt;/
      
    
    </summary>
    
      <category term="PyTorch" scheme="http://yoursite.com/categories/PyTorch/"/>
    
    
      <category term="PyTorch" scheme="http://yoursite.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>用例子学习 PyTorch</title>
    <link href="http://yoursite.com/2017/12/27/Learning-PyTorch-with-Examples/"/>
    <id>http://yoursite.com/2017/12/27/Learning-PyTorch-with-Examples/</id>
    <published>2017-12-27T04:59:35.000Z</published>
    <updated>2018-04-10T12:17:02.666Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2017/12/27/Learning-PyTorch-with-Examples/" target="_blank" rel="noopener">Github地址</a><br><a href="https://www.jianshu.com/p/52684285e335" target="_blank" rel="noopener">简书地址</a><br><a href="https://blog.csdn.net/u014630987/article/details/79886956" target="_blank" rel="noopener">CSDN地址</a></p><p>此教程翻译自 <code>PyTorch</code> <a href="http://pytorch.org/tutorials/beginner/pytorch_with_examples.html#tensors" target="_blank" rel="noopener">官方教程</a></p><p><strong>作者</strong><a href="https://github.com/jcjohnson/pytorch-examples" target="_blank" rel="noopener">Justin Johnson</a></p><p>本教程通过 <code>PyTorch</code> 自带的一些实例来介绍它的基本概念。<br><code>PyTorch</code> 核心提供了两个主要特征：</p><ol><li>n 维的张量，类似于 <code>numpy</code> ，但可以在 GPU 上运行</li><li>为构建和训练神经网络提供自动求导</li></ol><p>我们讲使用一个全连接的 <code>ReLU</code> 神经网络来作为我们运行的例子。这个神经网络只有一个隐藏层，并且通过梯度下降来最小化网络输出和真实值的欧几里得距离来训练神经网络拟合随机数据。</p><h1 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h1><h2 id="热身：-numpy"><a href="#热身：-numpy" class="headerlink" title="热身： numpy"></a>热身： numpy</h2><p>在介绍 <code>PyTorch</code> 之前，我们首先使用 <code>numpy</code> 来实现这个神经网络。<br><code>numpy</code> 提供了一个 n 维数组对象和许多操作这些数组的函数。<code>numpy</code> 是一个通用的科学计算框架；它没有计算图、深度学习和梯度的概念。然而我们可以很容易的使用 <code>numpy</code> 的操作手动实现前向传播和反向传播让一个两层的网络来拟合随机数据：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random input and output data</span></span><br><span class="line">x = np.random.randn(N, D_in)</span><br><span class="line">y = np.random.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Randomly initialize weights</span></span><br><span class="line">w1 = np.random.randn(D_in, H)</span><br><span class="line">w2 = np.random.randn(H, D_out)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y</span></span><br><span class="line">    h = x.dot(w1)</span><br><span class="line">    h_relu = np.maximum(h, <span class="number">0</span>)</span><br><span class="line">    y_pred = h_relu.dot(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = np.square(y_pred - y).sum()</span><br><span class="line">    print(t, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.T.dot(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.dot(w2.T)</span><br><span class="line">    grad_h = grad_h_relu.copy()</span><br><span class="line">    grad_h_relu[h&lt;<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.T.dot(grad_h)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># update weights</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure></p><h2 id="PyTorch-Tensors"><a href="#PyTorch-Tensors" class="headerlink" title="PyTorch: Tensors"></a>PyTorch: Tensors</h2><p><code>Numpy</code> 是一个伟大的框架，但是他不能利用 GPUs 来加快数值计算。对于现代深度神经网络， GPU 通常提供50倍或更高的加速，所以不幸的是 numpy 不足以用于现代深度学习。</p><p>这里，我介绍 PyTorch 中最基础的概念： 张量（Tensor）。Tensor 在概念上与 numpy 数组相同，它是一个 N 维的数组，并提供很多操作它的函数。和 numpy 类似，PyTorch 中的 Tensors 没有深度学习、计算图和梯度这些内容，他们是科学计算的通用工具。</p><p>然而，PyTorch的Tensor 和 numpy 不同，它可以利用 GPUs 来加快数值计算。为了让 Tensors 在 GPU上运行，你只需把它转型维一个新的类型即可。</p><p>这里我们使用 PyTorch 来训练一个两层的网络让它拟合随机数据。 和上面 numpy 的例子类似，我们需要手动实现网络的前向和反向传播：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">dtype = torch.FloatTensor</span><br><span class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimenssion</span></span><br><span class="line"><span class="comment"># H is hidden demenssion; D_out is output dimension</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create random input and output data</span></span><br><span class="line">x = torch.randn(N, D_in).type(dtype)</span><br><span class="line">y = torch.randn(N, D_out).type(dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Randomly initialize weights</span></span><br><span class="line">w1 = torch.randn(D_in, H).type(dtype)</span><br><span class="line">w2 = torch.randn(H, D_out).type(dtype)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pss: compute predicted y</span></span><br><span class="line">    h = x.mm(w1)</span><br><span class="line">    h_relu = h.clamp(min=<span class="number">0</span>)</span><br><span class="line">    y_pred = h_relu.mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute and print loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</span><br><span class="line">    print(t, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backprop to compute gradients of w1 and w2 with respect to loss</span></span><br><span class="line">    grad_y_pred = <span class="number">3.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.t().mm(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.mm(w2.t())</span><br><span class="line">    grad_h = grad_h_relu.clone()</span><br><span class="line">    grad_h[h&lt;<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.t().mm(grad_h)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># update weights using gradient descent</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure></p><h1 id="自动求导"><a href="#自动求导" class="headerlink" title="自动求导"></a>自动求导</h1><h2 id="PyTorch：-Variables-和-autograd"><a href="#PyTorch：-Variables-和-autograd" class="headerlink" title="PyTorch： Variables 和 autograd"></a>PyTorch： Variables 和 autograd</h2><p> 在上面的例子中，我们自己实现了神经网络的前向和反向传播。手动实现反向传播对于一个小型的两层网络来说并不是什么难题，但是对于大型复杂啊网络，很快就会变得棘手。</p><p>值得庆幸的是，我们可以使用自动微分来自动计算神经网络的反向传播。PyTorch 中的 <code>autograd</code> 包提供了这一功能。当我们使用自动求导时，你的神经网络的前向传播讲定义一个计算图；图中的节点将是 Tensors，边表示从输入张量产生输出张量的函数。通过图反向传播可以让你轻松的计算梯度。</p><p>这听起来很复杂，但是在实际使用却十分简单。我们用 <code>Variable</code> 对象来包装 Tensors。 一个 Variable 表示计算图中的一个节点。如果 <code>x</code> 是一个 <code>Variable</code>，那么 <code>x.data</code> 则是一个 Tensor，<code>x.grad</code> 是另一个用来保存 <code>x</code> 关于某个标量值的梯度。</p><p>PyTorch 的 Variable 拥有和 PyTorch 的 Tensors 一样的 API：几乎所有 Tensors 上的操作都可以在 Variable 上运行；不同之处在于使用 Variables 定义了一个计算图， 允许你自动的计算梯度。</p><p>这里我们使用 Variable 和自动求导来实现我们的两层网络；现在我们不再需要手动地实现反向传播。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dtype = torch.FloatTensor</span><br><span class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimenssion</span></span><br><span class="line"><span class="comment"># H is hidden demenssion; D_out is output dimension</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold input and outpus, and wrap them in Variables.</span></span><br><span class="line"><span class="comment"># Setting requires_grad=False indicates that we do not need to compute gradients</span></span><br><span class="line"><span class="comment"># with respect to these Variables during the backward pass</span></span><br><span class="line">x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=<span class="keyword">False</span>)</span><br><span class="line">y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors for weights, and wrap them in Variables.</span></span><br><span class="line"><span class="comment"># Setting requires_grad=True indicates that we want to compute gradients with</span></span><br><span class="line"><span class="comment"># respect to these Variables during the backward pass.</span></span><br><span class="line">w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=<span class="keyword">True</span>)</span><br><span class="line">w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y using operations on Variables; these</span></span><br><span class="line">    <span class="comment"># are exactly the same operations we used to compute the forward pass using</span></span><br><span class="line">    <span class="comment"># Tensors, but we do not need to keep references to intermediate values since</span></span><br><span class="line">    <span class="comment"># we are not implementing the backward pass by hand.</span></span><br><span class="line">    y_pred = x.mm(w1).clamp(min=<span class="number">0</span>).mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss using operations on Variables.</span></span><br><span class="line">    <span class="comment"># Now loss is a Variable of shape (1,) and loss.data is a Tensor of shape</span></span><br><span class="line">    <span class="comment"># (1,); loss.data[0] is a scalar value holding the loss.</span></span><br><span class="line"></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</span><br><span class="line">    print(t, loss.data[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use autograd to compute the backward pass. This call will compute the</span></span><br><span class="line">    <span class="comment"># gradient of loss with respect to all Variables with requires_grad=True.</span></span><br><span class="line">    <span class="comment"># After this call w1.grad and w2.grad will be Variables holding the gradient</span></span><br><span class="line">    <span class="comment"># of the loss with respect to w1 and w2 respectively.</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using gradient descent; w1.data and w2.data are Tensors,</span></span><br><span class="line">    <span class="comment"># w1.grad and w2.grad are Variables and w1.grad.data and w2.grad.data are</span></span><br><span class="line">    <span class="comment"># Tensors.</span></span><br><span class="line">    w1.data -= learning_rate * w1.grad.data</span><br><span class="line">    w2.data-= learning_rate * w2.grad.data</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Manually zero the gradients after updating weights</span></span><br><span class="line">    w1.grad.data.zero_()</span><br><span class="line">    w2.grad.data.zero_()</span><br></pre></td></tr></table></figure></p><h2 id="PyTorch-定义新的-autograd-函数"><a href="#PyTorch-定义新的-autograd-函数" class="headerlink" title="PyTorch:定义新的 autograd 函数"></a>PyTorch:定义新的 autograd 函数</h2><p>在底层，每个原始 autograd 操作符实际上是在 Tensors 上操作的两个函数。<strong>forward</strong> 函数从输入张量来计算输出张量。<strong>backward</strong>函数接收输出向量关于某个标量的梯度，然后计算输入张量关于关于同一个标量的梯度。</p><p>在 PyTorch 中， 我们可以通过定义一个 <code>torch.autograd.Function</code> 的子类并实现 <code>forward</code> 和 <code>backward</code>两个函数容易地实现我们自己的 autograd 操作符。我们可以通过定义一个该操作符的实例，并像函数一样给它传递一个包含输入数据的 Variable 来调用它，这样就使用我们新定义的 autograd 操作符。</p><p>在这个例子中我们， 我们自己定义了一个 autograd 函数来执行 ReLU 非线性映射，并使用它实现了一个两层的网络：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyReLU</span><span class="params">(torch.autograd.Function)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    We can implement our own custom autograd Function by subclassing</span></span><br><span class="line"><span class="string">    torch.autograd.Function and implementing forward and backward passed</span></span><br><span class="line"><span class="string">    which operate on tensors</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the forward pass we receive a  Tensor containing the input and return a</span></span><br><span class="line"><span class="string">        Tensor containing the output. You can cache arbitrary Tensors for use in the</span></span><br><span class="line"><span class="string">        backward pass using the save_for_backward method.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.save_for_backward(input)</span><br><span class="line">        <span class="keyword">return</span> input.clamp(min=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad_output)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the backward pass we receive a Tensor containing the gradient of loss </span></span><br><span class="line"><span class="string">        with respect to the output, and we need to compute the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the input</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        input, = self.saved_tensors</span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        grad_input[input&lt;<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dtype = torch.FloatTensor</span><br><span class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold input and outputs, and wrap them in Variables.</span></span><br><span class="line">x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=<span class="keyword">False</span>)</span><br><span class="line">y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors for weights, and wrap them in Variables.</span></span><br><span class="line">w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=<span class="keyword">True</span>)</span><br><span class="line">w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Construct an instance of our MyReLU class to use in our network</span></span><br><span class="line">    relu = MyReLU()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y using operations on Variables; we compute</span></span><br><span class="line">    <span class="comment"># ReLU using our custom autograd operation.</span></span><br><span class="line">    y_pred = relu(x.mm(w1)).mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</span><br><span class="line">    print(t, loss.data[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use autograd to compute the backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using gradient descent</span></span><br><span class="line">    w1.data -= learning_rate * w1.grad.data</span><br><span class="line">    w2.data -= learning_rate * w2.grad.data</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Manually zero the gradients after updateing weights</span></span><br><span class="line">    w1.grad.data.zero_()</span><br><span class="line">    w2.grad.data.zero_()</span><br></pre></td></tr></table></figure></p><h2 id="Tensorflow-静态图"><a href="#Tensorflow-静态图" class="headerlink" title="Tensorflow: 静态图"></a>Tensorflow: 静态图</h2><p>PyTorch 的 autograd 看起来很像 Tensorflow： 在两个框架中，我们都定义一个计算图，然后使用自动微分来计算梯度。最大的区别是： Tensorflow 中的计算图是静态的，而 PyTorch 使用的是动态计算图。</p><p>在 Tensorflow 中，我们只定义一次计算图，然后重复执行同一个图，可能将不同的输入数据提供给图。在 PyTorch 中，每次前向传播都会定义一个新的计算图。</p><p>静态图很好，因为您可以预先优化图; 例如，一个框架可能决定为了效率而融合某些图操作，或者想出一个在许多GPU或许多机器上的分布运行计算图的策略。 如果您一遍又一遍地重复使用同一个图，那么这个潜在的昂贵的前期优化可以在同一个图重复运行的情况下分摊。</p><p>静态图和动态图不同的一个方面是控制流。对于某些模型，我们可能希望对数据点执行不同的计算；例如，对于每个数据点，循环网络可以展开不同数量的时间步长，这个展开可以作为一个循环来实现。</p><p>使用静态图，循环结构需要成为图形的一部分；出于这个原因，Tensorflow 提供了像 <code>tf.scan</code> 这样的操作付来将循环嵌入到计算图中。使用动态图，这种情形就变得更简单了：由于我们为每个示例动态地都贱图，我们可以使用普通的命令式流控制来执行每个输入的不同计算。</p><p>为了与上面的 PyTorch 的 autograd 例子对比，这里我们使用 Tensorflow 来拟合一个简单的两层网络：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># First we set up the computational graph:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create placeholders for the input and target data; these will be filled</span></span><br><span class="line"><span class="comment"># with real data when we execute the graph.</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, D_in))</span><br><span class="line">y = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, D_out))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create Variables for the weights and initialize them with random data.</span></span><br><span class="line"><span class="comment"># A TensorFlow Variable persists its value across executions of the graph.</span></span><br><span class="line">w1 = tf.Variable(tf.random_normal((D_in, H)))</span><br><span class="line">w2 = tf.Variable(tf.random_normal((H, D_out)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Forward pass: Compute the predicted y using operations on TensorFlow Tensors.</span></span><br><span class="line"><span class="comment"># Note that this code does not actually perform any numeric operations; it</span></span><br><span class="line"><span class="comment"># merely sets up the computational graph that we will later execute.</span></span><br><span class="line">h = tf.matmul(x, w1)</span><br><span class="line">h_relu = tf.maximum(h, tf.zeros(<span class="number">1</span>))</span><br><span class="line">y_pred = tf.matmul(h_relu, w2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute loss using operations on TensorFlow Tensors</span></span><br><span class="line">loss = tf.reduce_sum((y - y_pred) ** <span class="number">2.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute gradient of the loss with respect to w1 and w2.</span></span><br><span class="line">grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Update the weights using gradient descent. To actually update the weights</span></span><br><span class="line"><span class="comment"># we need to evaluate new_w1 and new_w2 when executing the graph. Note that</span></span><br><span class="line"><span class="comment"># in TensorFlow the the act of updating the value of the weights is part of</span></span><br><span class="line"><span class="comment"># the computational graph; in PyTorch this happens outside the computational</span></span><br><span class="line"><span class="comment"># graph.</span></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line">new_w1 = w1.assign(w1 - learning_rate * grad_w1)</span><br><span class="line">new_w2 = w2.assign(w2 - learning_rate * grad_w2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now we have built our computational graph, so we enter a TensorFlow session to</span></span><br><span class="line"><span class="comment"># actually execute the graph.</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># Run the graph once to initialize the Variables w1 and w2.</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create numpy arrays holding the actual data for the inputs x and targets</span></span><br><span class="line">    <span class="comment"># y</span></span><br><span class="line">    x_value = np.random.randn(N, D_in)</span><br><span class="line">    y_value = np.random.randn(N, D_out)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">        <span class="comment"># Execute the graph many times. Each time it executes we want to bind</span></span><br><span class="line">        <span class="comment"># x_value to x and y_value to y, specified with the feed_dict argument.</span></span><br><span class="line">        <span class="comment"># Each time we execute the graph we want to compute the values for loss,</span></span><br><span class="line">        <span class="comment"># new_w1, and new_w2; the values of these Tensors are returned as numpy</span></span><br><span class="line">        <span class="comment"># arrays.</span></span><br><span class="line">        loss_value, _, _ = sess.run([loss, new_w1, new_w2],</span><br><span class="line">                                    feed_dict=&#123;x: x_value, y: y_value&#125;)</span><br><span class="line">        print(loss_value)</span><br></pre></td></tr></table></figure></p><h1 id="nn-模块"><a href="#nn-模块" class="headerlink" title="nn 模块"></a>nn 模块</h1><h2 id="PyTorch-nn"><a href="#PyTorch-nn" class="headerlink" title="PyTorch:nn"></a>PyTorch:nn</h2><p>计算图和 autograd 是定义复杂运算符和自动求导的的一个非常强大的范例。然而对于大规模的神经网络， 原始的 autograd 可能有点太低级了。</p><p>当构建神经网络时，我们经常想到把计算组织维层级结构，其中一些具有可学习的参数，这些参数将在学习期间被优化。</p><p>在 Tensorflow 中，像 <code>Keras</code>， <code>TensorFlow-Slim</code> 和 <code>TFLearn</code> 这样的软件包提供了对原始图的更高级的抽象，这对于构建神经网络很有用。</p><p>在 PyTorch 中， <code>nn</code> 包提供了同样的功能。 <code>nn</code> 包提供了一组模块，他们大致相当于神经网络层。一个模块接收一个输入变量并计算输出向量，也可能保存内部状态，如包含可学习的参数。 <code>nn</code> 包还定义了一组训练神经网络时有用的损失函数。</p><p>在这个例子中我们使用 <code>nn</code> 包来实现我们的两层网络：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables.</span></span><br><span class="line">x = Variable(torch.randn(N, D_in))</span><br><span class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the nn package to define our model as a sequence of layers. nn.Sequential</span></span><br><span class="line"><span class="comment"># is a Module which contains other Modules, and applies them in sequence to</span></span><br><span class="line"><span class="comment"># produce its output. Each Linear Module computes output from input using a</span></span><br><span class="line"><span class="comment"># linear function, and holds internal Variables for its weight and bias.</span></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(D_in, H),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(H, D_out)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The nn package also contains definitions of popular loss functions; in this</span></span><br><span class="line"><span class="comment"># case we will use Mean Squared Error (MSE) as our loss function.</span></span><br><span class="line">loss_fn = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-4</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y by passing x to the model. Module objects</span></span><br><span class="line">    <span class="comment"># override the __call__ operator so you can call them like functions. When</span></span><br><span class="line">    <span class="comment"># doing so you pass a Variable of input data to the Module and it produces</span></span><br><span class="line">    <span class="comment"># a Variable of output data.</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss. We pass Variables containing the predicted and true</span></span><br><span class="line">    <span class="comment"># values of y, and the loss function returns a Variable containing the</span></span><br><span class="line">    <span class="comment"># loss.</span></span><br><span class="line">    loss = loss_fn(y_pred, y)</span><br><span class="line">    print(t, loss.data[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero the gradients before running the backward pass.</span></span><br><span class="line">    model.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass: compute gradient of the loss with respect to all the learnable</span></span><br><span class="line">    <span class="comment"># parameters of the model. Internally, the parameters of each Module are stored</span></span><br><span class="line">    <span class="comment"># in Variables with requires_grad=True, so this call will compute gradients for</span></span><br><span class="line">    <span class="comment"># all learnable parameters in the model.</span></span><br><span class="line"></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">        param.data -= learning_rate * param.grad.data</span><br></pre></td></tr></table></figure></p><h2 id="PyTorch：-optim"><a href="#PyTorch：-optim" class="headerlink" title="PyTorch： optim"></a>PyTorch： optim</h2><p>到目前为止，我们已经更新了我们模型的权重，通过手动改变需要学习的参数变量的 <code>.data</code> 成员。这对于像随机梯度下降这种简单的优化算法来说不是很困难，但是在实际训练神经网络时，我们常常使用更复杂的优化算法，如 AdaGrad，RMSProp， Adam等。</p><p>PyTorch 的 <code>optim</code> 包抽象了优化算法的思想，并提供了常用算法的实现。</p><p>在这个例子中，我们讲使用 <code>nn</code> 包来重新定义我们之前的模型，但是我们讲使用 <code>optim</code> 包提供的 Adam 算法来优化我们的模型：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span>  torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables.</span></span><br><span class="line">x = Variable(torch.randn(N, D_in))</span><br><span class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the nn package to define our model and loss function</span></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(D_in, H),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(H, D_out)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">loss_fn = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the optim package to define an Optimizer that will update the weights of</span></span><br><span class="line"><span class="comment"># the model for us. Here we will use Adam; the optim package contains many other</span></span><br><span class="line"><span class="comment"># optimization algoriths. The first argument to the Adam constructor tells the</span></span><br><span class="line"><span class="comment"># optimizer which Variables it should update.</span></span><br><span class="line">learning_rate = <span class="number">1e-4</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y by passing x to the model.</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y)</span><br><span class="line">    print(t, loss.data[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Before the backward pass, use the optimizer object to zero all of the</span></span><br><span class="line">    <span class="comment"># gradients for the variables it will update (which are the learnable weights</span></span><br><span class="line">    <span class="comment"># of the model)</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass: compute gradient of the loss with respect to model</span></span><br><span class="line">    <span class="comment"># parameters</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calling the step function on an Optimizer makes an update to its</span></span><br><span class="line">    <span class="comment"># parameters</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure></p><h2 id="PyTorch：-自定义模块"><a href="#PyTorch：-自定义模块" class="headerlink" title="PyTorch： 自定义模块"></a>PyTorch： 自定义模块</h2><p>有时你想要指定比现有模块序列更复杂的模型；对于这种情况，你可以通过继承 <code>nn.Module</code> 来定义自己的模块，并实现 <code>forward</code> 函数，他接收一个输入变量，并使用其他模块或 autograd 操作符来生成输出变量。</p><p>在这个例子中，我们实现了一个两层网络来作为自定义模块：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoLayerNet</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, D_in, H, D_out)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the constructor we instantiate two nn.Linear modules and assign them as member variables.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(TwoLayerNet, self).__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(D_in, H)</span><br><span class="line">        self.linear2 = torch.nn.Linear(H, D_out)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the forward function we accept a Variable of input data and we must return</span></span><br><span class="line"><span class="string">        a Variable of output data. We can use Modules defined in the constructor as</span></span><br><span class="line"><span class="string">        well as arbitrary operators on Variables.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        h_relu = self.linear1(x).clamp(min=<span class="number">0</span>)</span><br><span class="line">        y_pred = self.linear2(h_relu)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables</span></span><br><span class="line">x = Variable(torch.randn(N, D_in))</span><br><span class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct our model by instantiating the class defined above</span></span><br><span class="line">model = TwoLayerNet(D_in, H, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct our loss function and an Optimizer. The call to model.parameters()</span></span><br><span class="line"><span class="comment"># in the SGD constructor will contain the learnable parameters of the two</span></span><br><span class="line"><span class="comment"># nn.Linear modules which are members of the model.</span></span><br><span class="line">criterion = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-4</span>)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: Compute predicted y by passing x to model</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = criterion(y_pred, y)</span><br><span class="line">    print(t, loss.data[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero gradients, perform a backward pass, and update the weights.</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure></p><h2 id="PyTorch-控制流-共享权重"><a href="#PyTorch-控制流-共享权重" class="headerlink" title="PyTorch: 控制流 + 共享权重"></a>PyTorch: 控制流 + 共享权重</h2><p>作为动态图和权值共享的例子，我们实现了一个不同的模型： 一个全连接的 ReLU 网络，每次前向传播时， 从1到4随机选择一个数来作为隐藏层的层数。=，多次重复使用相同的权重来计算最内层的隐藏层。</p><p>对于这个模型，我们可以使用普通的 Python 控制流来实现循环，在定义前向传播时，通过简单的重复使用同一个模块，我们可以实现最内层之间的权重共享。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DynamicNet</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, D_in, H, D_out)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the constructor we construct three nn.Linear instances that we will use</span></span><br><span class="line"><span class="string">        in the forward pass.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(DynamicNet, self).__init__()</span><br><span class="line">        self.input_linear = torch.nn.Linear(D_in, H)</span><br><span class="line">        self.middle_linear = torch.nn.Linear(H, H)</span><br><span class="line">        self.output_linear = torch.nn.Linear(H, D_out)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3</span></span><br><span class="line"><span class="string">        and reuse the middle_linear Module that many times to compute hidden layer</span></span><br><span class="line"><span class="string">        representations.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Since each forward pass builds a dynamic computation graph, we can use normal</span></span><br><span class="line"><span class="string">        Python control-flow operators like loops or conditional statements when</span></span><br><span class="line"><span class="string">        defining the forward pass of the model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Here we also see that it is perfectly safe to reuse the same Module many</span></span><br><span class="line"><span class="string">        times when defining a computational graph. This is a big improvement from Lua</span></span><br><span class="line"><span class="string">        Torch, where each Module could be used only once.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        h_relu = self.input_linear(x).clamp(min=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(random.randint(<span class="number">0</span>, <span class="number">3</span>)):</span><br><span class="line">            h_relu = self.middle_linear(h_relu).clamp(min=<span class="number">0</span>)</span><br><span class="line">        y_pred = self.output_linear(h_relu)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables</span></span><br><span class="line">x = Variable(torch.randn(N, D_in))</span><br><span class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct our model by instantiating the class defined above</span></span><br><span class="line">model = DynamicNet(D_in, H, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct our loss function and an Optimizer. Training this strange model with</span></span><br><span class="line"><span class="comment"># vanilla stochastic gradient descent is tough, so we use momentum</span></span><br><span class="line">criterion = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-4</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: Compute predicted y by passing x to the model</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = criterion(y_pred, y)</span><br><span class="line">    print(t, loss.data[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero gradients, perform a backward pass, and update the weights.</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure></p><h1 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h1><p>以上实例的代码地址如下：</p><p><strong>Tensors</strong></p><p><a href="http://pytorch.org/tutorials/beginner/examples_tensor/two_layer_net_numpy.html#sphx-glr-beginner-examples-tensor-two-layer-net-numpy-py" target="_blank" rel="noopener">Warm-up: numpy</a></p><p><a href="http://pytorch.org/tutorials/beginner/examples_tensor/two_layer_net_tensor.html#sphx-glr-beginner-examples-tensor-two-layer-net-tensor-py" target="_blank" rel="noopener">PyTorch: Tensors</a></p><p><strong>autograd</strong><br><a href="http://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_autograd.html#sphx-glr-beginner-examples-autograd-two-layer-net-autograd-py" target="_blank" rel="noopener">PyTorch: Variables and autograd</a></p><p><a href="http://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html#sphx-glr-beginner-examples-autograd-two-layer-net-custom-function-py" target="_blank" rel="noopener">PyTorch: Defining new autograd functions</a></p><p><a href="http://pytorch.org/tutorials/beginner/examples_autograd/tf_two_layer_net.html#sphx-glr-beginner-examples-autograd-tf-two-layer-net-py" target="_blank" rel="noopener">Tensorflow: Static Graphs</a></p><p><strong>nn Module</strong><br><a href="http://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_nn.html#sphx-glr-beginner-examples-nn-two-layer-net-nn-py" target="_blank" rel="noopener">PyTorch: nn</a></p><p><a href="http://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html#sphx-glr-beginner-examples-nn-two-layer-net-optim-py" target="_blank" rel="noopener">PyTorch: optim</a></p><p><a href="http://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html#sphx-glr-beginner-examples-nn-two-layer-net-module-py" target="_blank" rel="noopener">PyTorch: Custom nn Modules</a></p><p><a href="http://pytorch.org/tutorials/beginner/examples_nn/dynamic_net.html#sphx-glr-beginner-examples-nn-dynamic-net-py" target="_blank" rel="noopener">PyTorch: Control FLow + Weights Sharing</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2017/12/27/Learning-PyTorch-with-Examples/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github地址&lt;/a&gt;&lt;br&gt;&lt;a href=
      
    
    </summary>
    
      <category term="PyTorch" scheme="http://yoursite.com/categories/PyTorch/"/>
    
    
      <category term="PyTorch" scheme="http://yoursite.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Android 四大组件之 Service</title>
    <link href="http://yoursite.com/2017/12/16/Android-Service/"/>
    <id>http://yoursite.com/2017/12/16/Android-Service/</id>
    <published>2017-12-16T05:20:33.000Z</published>
    <updated>2018-03-20T14:02:04.000Z</updated>
    
    <content type="html"><![CDATA[<p><code>Service</code> 是 Android 应用的四大组件之一(其余的为<code>Activity</code>, <code>Broadcatst Receiver</code>, <code>Content Provider</code>),它不同于 <code>Activity</code>, 它有两个特点:</p><ol><li><strong>在后台执行长时间运行的操作</strong></li><li><strong>没有用户界面</strong></li></ol><p>Android 应用中的任何组件都可以启动和绑定 <code>Service</code>,有2种形式的 <code>Service</code>:</p><ol><li>启动式(Started)</li></ol><blockquote><p>当一个组件(比如 <code>Activity</code>)通过调用<code>startService()</code>时,就会启动一个 <code>Service</code>。一旦启动,这个 <code>service</code> 就会在后台无限执行下去,即使创建这个 <code>Service</code> 的组件已经销毁。通常启动式(started)的 <code>Service</code> 执行单一操作切不给调用者返回结果。比如,通过网络下载文件。<strong>注意</strong>,当操作执行完后,应该手动停止 <code>Service</code>。</p></blockquote><ol><li>绑定式(Bound)</li></ol><blockquote><p>当一个组件通过调用 <code>bindService()</code> 时,就会绑定到一个 <code>Service</code>。一个绑定的 <code>Service</code> 类似于”客户端-服务器”的结构,它允许组件与它进行交互,还可以进行跨进程交互(IPC)。它不同于启动式的 <code>Service</code> 会在后台无限执行知道手动被停止,绑定式的 <code>Service</code>只要有组件与之绑定,该 <code>Service</code> 就会运行,反之,则会被自动销毁。</p></blockquote><p>任何一个 <code>Service</code> 都可以同时以这两种方式运行，关键在于你是否实现了以下两个方法：</p><ol><li><code>onStartCommand()</code>: 允许组件启动该 ’Service‘；</li><li><code>onBind()</code>： 允许组件与之绑定；</li></ol><p><strong>注意</strong>：<code>Service</code> 和 <code>Activity</code> 一样，运行在主线程（UI线程）中， 它不会创建独立的线程，因此，当在 <code>Service</code> 中执行 CPU 密集操作或者阻塞操作时（比如网络或MP3播放），应该在 <code>service</code> 中创建一个新的线程，来避免出现 “Application Not Responding（ANR）”。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;code&gt;Service&lt;/code&gt; 是 Android 应用的四大组件之一(其余的为&lt;code&gt;Activity&lt;/code&gt;, &lt;code&gt;Broadcatst Receiver&lt;/code&gt;, &lt;code&gt;Content Provider&lt;/code&gt;),它不同于
      
    
    </summary>
    
      <category term="Android" scheme="http://yoursite.com/categories/Android/"/>
    
    
      <category term="Android" scheme="http://yoursite.com/tags/Android/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch:Torch 用户</title>
    <link href="http://yoursite.com/2017/12/03/PyTorch-for-former-Torch-users/"/>
    <id>http://yoursite.com/2017/12/03/PyTorch-for-former-Torch-users/</id>
    <published>2017-12-03T03:12:02.000Z</published>
    <updated>2018-03-20T14:03:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>此教程翻译自 <code>PyTorch</code> <a href="http://pytorch.org/tutorials/beginner/former_torchies_tutorial.html" target="_blank" rel="noopener">官方教程</a></p><p><strong>作者:</strong><a href="http://soumith.ch/" target="_blank" rel="noopener">Soumith Chintala</a></p><p>在这个教程中,你将学习如下知识:</p><ol><li>使学习用 <code>torch</code> 张量, 和与 <code>(Lua) Torch</code> 的不同;</li><li>使用自动求导包;</li><li><p>构建神经网络</p><ul><li>构建卷积网络(‘ConvNet`)</li><li>构建 循环神经网络(`Recurrent Net)</li></ul></li><li>使用多GPU</li></ol><h1 id="一-张量-Tensors"><a href="#一-张量-Tensors" class="headerlink" title="一. 张量(Tensors)"></a>一. 张量(<code>Tensors</code>)</h1><p><code>PyTorch</code> 中的张量和 <code>Torch</code> 中的张量完全一样的.</p><p>创建一个未初始化的大小为 5*7 的张量:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.FloatTensor(<span class="number">5</span>, <span class="number">7</span>)</span><br></pre></td></tr></table></figure></p><p>用均值为 0, 方差为 1 的正态分布随机初始化一个张量:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">5</span>, <span class="number">7</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(a.size())</span><br></pre></td></tr></table></figure></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> 0.6608  0.0584 -1.9318  1.6068 -1.0049  1.3115 -0.3893</span><br><span class="line"> 0.5118  1.7707 -0.3681  0.2345 -0.0295  2.0229  0.2815</span><br><span class="line"> 1.2753 -1.2878  1.3112 -0.0965  1.4672  0.5731  2.7306</span><br><span class="line"> 1.2824  0.5972  1.4461  1.1568  0.1307  0.0381 -1.1927</span><br><span class="line"> 0.7658  0.1555  0.2385  1.0136 -0.6666 -0.3318 -0.8383</span><br><span class="line">[torch.FloatTensor of size 5x7]</span><br><span class="line">torch.Size([5, 7])</span><br></pre></td></tr></table></figure></p><p><strong>注意</strong><br><code>torch.Size</code> 本质上是一个元组,所以它支持和元组一样的操作.</p><h2 id="Inplace-Out-of-place"><a href="#Inplace-Out-of-place" class="headerlink" title="Inplace/Out-of-place"></a>Inplace/Out-of-place</h2><p>第一个不同就是张量上的所有就地(Inplace)操作都有一个’_’后缀,比如,<code>add</code>不是一个原地(Out-of-place)操作版本,<code>add_</code>是一个原地操作版本.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a.fill_(<span class="number">3.5</span>)</span><br><span class="line"><span class="comment"># a has now been filled with the value 3.5</span></span><br><span class="line"></span><br><span class="line">b = a.add(<span class="number">4.0</span>)</span><br><span class="line"><span class="comment"># is still filled with 3.5</span></span><br><span class="line"><span class="comment"># new tensor b is returned with 3.5 + 4.0 = 7.0</span></span><br><span class="line">print(a, b)</span><br></pre></td></tr></table></figure></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">3.5000  3.5000  3.5000  3.5000  3.5000  3.5000  3.5000</span><br><span class="line"> 3.5000  3.5000  3.5000  3.5000  3.5000  3.5000  3.5000</span><br><span class="line"> 3.5000  3.5000  3.5000  3.5000  3.5000  3.5000  3.5000</span><br><span class="line"> 3.5000  3.5000  3.5000  3.5000  3.5000  3.5000  3.5000</span><br><span class="line"> 3.5000  3.5000  3.5000  3.5000  3.5000  3.5000  3.5000</span><br><span class="line">[torch.FloatTensor of size 5x7]</span><br><span class="line"></span><br><span class="line"> 7.5000  7.5000  7.5000  7.5000  7.5000  7.5000  7.5000</span><br><span class="line"> 7.5000  7.5000  7.5000  7.5000  7.5000  7.5000  7.5000</span><br><span class="line"> 7.5000  7.5000  7.5000  7.5000  7.5000  7.5000  7.5000</span><br><span class="line"> 7.5000  7.5000  7.5000  7.5000  7.5000  7.5000  7.5000</span><br><span class="line"> 7.5000  7.5000  7.5000  7.5000  7.5000  7.5000  7.5000</span><br><span class="line">[torch.FloatTensor of size 5x7]</span><br></pre></td></tr></table></figure></p><p>某些操作,比如 <code>narrow</code> 没有原地操作版本, 因此 <code>narrow_</code>不存在.<br>同样地,某些操作, 比如 <code>fill_</code> 没有 <code>out-of-place</code> 版本,所以 <code>.fill</code> 不存在.</p><h2 id="零索引-Zero-Indexing"><a href="#零索引-Zero-Indexing" class="headerlink" title="零索引(Zero Indexing)"></a>零索引(Zero Indexing)</h2><p>另一个不同是 PyTorch 中的张量是从 0 开始索引,而在 lua 中, 它是从1 开始索引.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b = a[<span class="number">0</span>, <span class="number">3</span>] <span class="comment"># 选择 a 中的第一行,第四列的元素</span></span><br></pre></td></tr></table></figure></p><p>张量也可以用 Python 的切片来索引<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b = a[:, <span class="number">3</span>:<span class="number">5</span>]</span><br></pre></td></tr></table></figure></p><h2 id="无驼峰写法"><a href="#无驼峰写法" class="headerlink" title="无驼峰写法"></a>无驼峰写法</h2><p>下一个微小的不同是所有的函数现在不再是驼峰命名.比如 <code>indexAdd</code> 现在写为 <code>index_add_</code>.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1  1  1  1  1</span><br><span class="line"> 1  1  1  1  1</span><br><span class="line"> 1  1  1  1  1</span><br><span class="line"> 1  1  1  1  1</span><br><span class="line"> 1  1  1  1  1</span><br><span class="line">[torch.FloatTensor of size 5x5]</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">z = torch.Tensor(<span class="number">5</span>, <span class="number">2</span>)</span><br><span class="line">z[:, <span class="number">0</span>] = <span class="number">10</span></span><br><span class="line">z[:, <span class="number">1</span>] = <span class="number">100</span></span><br><span class="line">print(z)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">10  100</span><br><span class="line">  10  100</span><br><span class="line">  10  100</span><br><span class="line">  10  100</span><br><span class="line">  10  100</span><br><span class="line">[torch.FloatTensor of size 5x2]</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x.index_add_(<span class="number">1</span>, torch.LongTensor([<span class="number">4</span>, <span class="number">0</span>]), z)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">101    1    1    1   11</span><br><span class="line"> 101    1    1    1   11</span><br><span class="line"> 101    1    1    1   11</span><br><span class="line"> 101    1    1    1   11</span><br><span class="line"> 101    1    1    1   11</span><br><span class="line">[torch.FloatTensor of size 5x5]</span><br></pre></td></tr></table></figure></p><h2 id="numpy-桥"><a href="#numpy-桥" class="headerlink" title="numpy 桥"></a>numpy 桥</h2><p>把一个torch张量转换为numpy数组或者反过来都是很简单的。Torch张量和numpy数组将共享潜在的内存，改变其中一个也将改变另一个。</p><h3 id="把-Torch-张量转换为-numpy-数组"><a href="#把-Torch-张量转换为-numpy-数组" class="headerlink" title="把 Torch 张量转换为 numpy 数组"></a>把 Torch 张量转换为 numpy 数组</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">5</span>)</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line"> 1</span><br><span class="line"> 1</span><br><span class="line"> 1</span><br><span class="line"> 1</span><br><span class="line">[torch.FloatTensor of size 5]</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = a.numpy()</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ 1.  1.  1.  1.  1.]</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a.add_(1)</span><br><span class="line">print(a)</span><br><span class="line">print(b)    # 观察 numpy 数组的值如何在变化</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">2</span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line">[torch.FloatTensor of size 5]</span><br><span class="line"></span><br><span class="line">[ 2.  2.  2.  2.  2.]</span><br></pre></td></tr></table></figure></p><h3 id="把-numpy-数组转换为-torch-张量"><a href="#把-numpy-数组转换为-torch-张量" class="headerlink" title="把 numpy 数组转换为 torch 张量"></a>把 numpy 数组转换为 torch 张量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">np.add(a, <span class="number">1</span>, out=a)</span><br><span class="line">print(a)</span><br><span class="line">print(b)  <span class="comment"># 观察改变numpy数组的值如何在自动改变torch张量的值</span></span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[ 2.  2.  2.  2.  2.]</span><br><span class="line"></span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line">[torch.DoubleTensor of size 5]</span><br></pre></td></tr></table></figure></p><p>除了 <code>CharTensor</code>, 所有在 CPU 上的张量都支持在 numpy 之间来回转换.</p><h2 id="CUDA-张量"><a href="#CUDA-张量" class="headerlink" title="CUDA 张量"></a>CUDA 张量</h2><p>在 PyTorch 中, CUDA 张量是很容易实现的,并且从cpu 到 GPU 来转换一个 CUDA 张量将保存它的潜在类型.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 让我们在只有CUDA可用时才运行这个单元</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    <span class="comment"># 创建一个LongTensor,并把它转换为 GPU 版的 torch.cuda.LongTensor</span></span><br><span class="line">    a = torch.LongTensor(<span class="number">10</span>).fill_(<span class="number">3</span>).cuda()</span><br><span class="line">    print(type(a))</span><br><span class="line">    b = a.cpu()</span><br><span class="line">    <span class="comment"># 把它转换为 cpu 版本,还原为torch.LongTensor</span></span><br></pre></td></tr></table></figure></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;class &apos;torch.cuda.LongTensor&apos;&gt;</span><br></pre></td></tr></table></figure></p><p><strong>脚本总运行时间:</strong> 0.002 秒</p><p><a href="http://pytorch.org/tutorials/_downloads/tensor_tutorial1.py" target="_blank" rel="noopener">Python 源码</a></p><p><a href="http://pytorch.org/tutorials/_downloads/tensor_tutorial1.ipynb" target="_blank" rel="noopener">Jupyter 源码</a></p><h1 id="自动求导-Autograd"><a href="#自动求导-Autograd" class="headerlink" title="自动求导(Autograd)"></a>自动求导(Autograd)</h1><p>自动求导是 torch 中自动微分的核心包,它使用一个基于磁带的系统来进行自动微分.</p><p>在前向阶段,这个磁带会记住所有执行的操作,在反向阶段, 它重新执行这些操作.</p><h2 id="变量-Variable"><a href="#变量-Variable" class="headerlink" title="变量(Variable)"></a>变量(Variable)</h2><p>在 <code>autograd</code> 中,我们引入一个 <code>Variable</code> 类, 它是一个 <code>Tensor</code> 的包装器.你可以通过 <code>.data</code> 属性来访问原始的张量,在执行完反向过程后,关于这个变量的梯度被累积到 <code>.grad</code> 属性中.</p><p>![Variable][/images/Variable.png]</p><p>还有一个类对于自动求导的实验非常重要,即 <code>Function</code>类.<br><code>Variable</code> 和 <code>Function</code> 是相互连接的并构成一个无环图来编码整个计算历史.每个 <code>Variable</code> 有一个 <code>grad_fn</code> 属性,它指向创建该变量的一个<code>Function</code>,用户自己创建的变量除外,它的<code>grad_fn</code>属性为None.</p><p>如果你想计算导数,你可以在一个 <code>Variable</code> 上调用 <code>.backward()</code>. 如果 <code>Variable</code> 是一个标量(他只有一个元素),你不必给 <code>backward()</code>指定任何的参数,然而如果他拥有多个元素, 你需要指定一个 <code>grad_output</code> 参数,他是一个和该 <code>Variable</code> 大小相匹配的张量.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span>  torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line">x = Variable(torch.ones(<span class="number">2</span>, <span class="number">2</span>), requires_grad=<span class="keyword">True</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Variable containing:</span><br><span class="line"> 1  1</span><br><span class="line"> 1  1</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  1</span><br><span class="line"> 1  1</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">None</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x.grad_fn)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">None</span><br></pre></td></tr></table></figure><p>在 x 上执行一个操作:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = x + 2</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Variable containing:</span><br><span class="line"> 3  3</span><br><span class="line"> 3  3</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br></pre></td></tr></table></figure></p><p>y 作为一个操作的结果, 因此它有<code>grad_fn</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.grad_fn)</span><br></pre></td></tr></table></figure></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;torch.autograd.function.AddConstantBackward object at 0x7faa6602d7c8&gt;</span><br></pre></td></tr></table></figure></p><p>在 y 上执行更多的操作:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line">print(z, out)</span><br></pre></td></tr></table></figure></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Variable containing:</span><br><span class="line"> 27  27</span><br><span class="line"> 27  27</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br><span class="line"> Variable containing:</span><br><span class="line"> 27</span><br><span class="line">[torch.FloatTensor of size 1]</span><br></pre></td></tr></table></figure></p><h2 id="梯度-Gradients"><a href="#梯度-Gradients" class="headerlink" title="梯度(Gradients)"></a>梯度(Gradients)</h2><p>现在我们进行反向机选,并打印出梯度 $d(out)/dx$<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">out.backward()</span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Variable containing:</span><br><span class="line"> 4.5000  4.5000</span><br><span class="line"> 4.5000  4.5000</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br></pre></td></tr></table></figure></p><p><strong>未完待续…</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;此教程翻译自 &lt;code&gt;PyTorch&lt;/code&gt; &lt;a href=&quot;http://pytorch.org/tutorials/beginner/former_torchies_tutorial.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
      
    
    </summary>
    
      <category term="PyTorch" scheme="http://yoursite.com/categories/PyTorch/"/>
    
    
      <category term="PyTorch" scheme="http://yoursite.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>如何在 hexo 中支持 Mathjax？</title>
    <link href="http://yoursite.com/2017/11/29/hexo-support-mathjax/"/>
    <id>http://yoursite.com/2017/11/29/hexo-support-mathjax/</id>
    <published>2017-11-29T12:52:28.000Z</published>
    <updated>2018-03-20T14:02:25.178Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2017/11/29/hexo-support-mathjax/" target="_blank" rel="noopener">Github 地址</a><br><a href="http://www.jianshu.com/p/e8d433a2c5b7" target="_blank" rel="noopener">简书地址</a><br><a href="http://blog.csdn.net/u014630987/article/details/78670258" target="_blank" rel="noopener">CSDN地址</a></p><p>在 hexo 中，你会发现我们不能用 <code>Latex</code> 语法来书写数学公式，这对于书写学术博客来说是很大的不便，因为我们会经常碰到很多的数学公式推导，但是我们可以通过安装第三方库来解决这一问题。</p><h2 id="第一步：-使用Kramed代替-Marked"><a href="#第一步：-使用Kramed代替-Marked" class="headerlink" title="第一步： 使用Kramed代替 Marked"></a>第一步： 使用Kramed代替 Marked</h2><p><code>hexo</code> 默认的渲染引擎是 <code>marked</code>，但是 <code>marked</code> 不支持 <code>mathjax</code>。 <code>kramed</code> 是在 <code>marked</code> 的基础上进行修改。我们在工程目录下执行以下命令来安装 <code>kramed</code>.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure></p><p>然后，更改<your-project-dir>/node_modules/hexo-renderer-kramed/lib/renderer.js，更改：<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Change inline math rule</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">formatText</span>(<span class="params">text</span>) </span>&#123;</span><br><span class="line">    <span class="comment">// Fit kramed's rule: $$ + \1 + $$</span></span><br><span class="line">    <span class="keyword">return</span> text.replace(<span class="regexp">/`\$(.*?)\$`/g</span>, <span class="string">'$$$$$1$$$$'</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></your-project-dir></p><p>为：<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Change inline math rule</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">formatText</span>(<span class="params">text</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> text;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="第二步-停止使用-hexo-math"><a href="#第二步-停止使用-hexo-math" class="headerlink" title="第二步: 停止使用 hexo-math"></a>第二步: 停止使用 hexo-math</h2><p>首先，如果你已经安装 <code>hexo-math</code>, 请卸载它：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-math --save</span><br></pre></td></tr></table></figure></p><p>然后安装 <a href="https://github.com/phoenixcw/hexo-renderer-mathjax" target="_blank" rel="noopener">hexo-renderer-mathjax</a> 包：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-renderer-mathjax --save</span><br></pre></td></tr></table></figure></p><h2 id="第三步-更新-Mathjax-的-CDN-链接"><a href="#第三步-更新-Mathjax-的-CDN-链接" class="headerlink" title="第三步: 更新 Mathjax 的 CDN 链接"></a>第三步: 更新 Mathjax 的 CDN 链接</h2><p>首先，打开<path-to-your-project>/node_modules/hexo-renderer-mathjax/mathjax.html</path-to-your-project></p><p>然后，把<code>&lt;script&gt;</code>更改为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure></p><h2 id="第四步-更改默认转义规则"><a href="#第四步-更改默认转义规则" class="headerlink" title="第四步: 更改默认转义规则"></a>第四步: 更改默认转义规则</h2><p>因为 <code>hexo</code> 默认的转义规则会将一些字符进行转义，比如 <code>_</code> 转为 <code>&lt;em&gt;</code>, 所以我们需要对默认的规则进行修改.<br>首先， 打开&lt;path-to-your-project/node_modules/kramed/lib/rules、inline.js,</p><p>然后，把:<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">escape</span>: <span class="regexp">/^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/</span>,</span><br></pre></td></tr></table></figure></p><p>更改为:<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">escape</span>: <span class="regexp">/^\\([`*\[\]()# +\-.!_&gt;])/</span>,</span><br></pre></td></tr></table></figure></p><p>把<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">em: <span class="regexp">/^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br></pre></td></tr></table></figure></p><p>更改为:<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">em: <span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br></pre></td></tr></table></figure></p><h2 id="第五步-开启mathjax"><a href="#第五步-开启mathjax" class="headerlink" title="第五步: 开启mathjax"></a>第五步: 开启mathjax</h2><p>在主题 <code>_config.yml</code> 中开启 Mathjax， 找到 <code>mathjax</code> 字段添加如下代码：<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">mathjax:</span></span><br><span class="line"><span class="attr">    enable:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></p><p>这一步可选，在博客中开启 <code>Mathjax</code>，， 添加以下内容：<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: Testing Mathjax with Hexo</span><br><span class="line">category: Uncategorized</span><br><span class="line">date: 2017/05/03</span><br><span class="line">mathjax: true</span><br><span class="line">---</span><br></pre></td></tr></table></figure></p><p>通过以上步骤，我们就可以在 <code>hexo</code> 中使用 <code>Mathjax</code> 来书写数学公式。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2017/11/29/hexo-support-mathjax/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github 地址&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://w
      
    
    </summary>
    
      <category term="Hexo" scheme="http://yoursite.com/categories/Hexo/"/>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 深度学习:60分钟快速入门</title>
    <link href="http://yoursite.com/2017/11/27/PyTorch-60-min-Tutorial/"/>
    <id>http://yoursite.com/2017/11/27/PyTorch-60-min-Tutorial/</id>
    <published>2017-11-26T16:20:22.000Z</published>
    <updated>2018-03-20T14:03:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>此教程为翻译<a href="http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="noopener">官方地址</a></p><p><a href="https://ranmaosong.github.io/2017/11/27/PyTorch-60-min-Tutorial/" target="_blank" rel="noopener">Github 地址</a><br><a href="http://www.jianshu.com/p/889dbc684622" target="_blank" rel="noopener">简书地址</a><br><a href="http://blog.csdn.net/u014630987/article/details/78669051" target="_blank" rel="noopener">CSDN地址</a></p><p><strong>作者:</strong><a href="http://soumith.ch/" target="_blank" rel="noopener">Soumith Chintala</a></p><p>本教程的目标:</p><ul><li>深入理解PyTorch张量库和神经网络</li><li>训练一个小的神经网络来分类图片</li></ul><p>这个教程假设你熟悉numpy的基本操作。</p><p><strong>注意</strong></p><p>请确保<code>torch</code>和<code>torchvision</code>包已经安装。</p><h2 id="一、PyTorch-是什么"><a href="#一、PyTorch-是什么" class="headerlink" title="一、PyTorch 是什么"></a>一、PyTorch 是什么</h2><p>他是一个基于Python的科学计算包，目标用户有两类</p><ul><li>为了使用GPU来替代numpy</li><li>一个深度学习援救平台：提供最大的灵活性和速度</li></ul><h2 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h2><h4 id="张量（Tensors"><a href="#张量（Tensors" class="headerlink" title="张量（Tensors)"></a>张量（Tensors)</h4><p>张量类似于numpy的ndarrays，不同之处在于张量可以使用GPU来加快计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure><p>构建一个未初始化的5*3的矩阵：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.Tensor(5, 3)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><p>输出 ：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1.00000e-10 *</span><br><span class="line"> -1.1314  0.0000 -1.1314</span><br><span class="line">  0.0000  0.0000  0.0000</span><br><span class="line">  0.0000  0.0000  0.0000</span><br><span class="line">  0.0000  0.0000  0.0000</span><br><span class="line">  0.0000  0.0000  0.0000</span><br><span class="line">[torch.FloatTensor of size 5x3]</span><br></pre></td></tr></table></figure><p>构建一个随机初始化的矩阵</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(5, 3)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0.2836  0.6710  0.5146</span><br><span class="line"> 0.8842  0.2821  0.7768</span><br><span class="line"> 0.3409  0.0428  0.6726</span><br><span class="line"> 0.1982  0.6950  0.6040</span><br><span class="line"> 0.0272  0.6586  0.3555</span><br><span class="line">[torch.FloatTensor of size 5x3]</span><br></pre></td></tr></table></figure><p>获取矩阵的大小：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x.size())</span><br></pre></td></tr></table></figure><p>输出:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([5, 3])</span><br></pre></td></tr></table></figure><p><strong>注意</strong></p><p><code>torch.Size</code>实际上是一个元组，所以它支持元组相同的操作。</p><h4 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h4><p>张量上的操作有多重语法形式，下面我们以加法为例进行讲解。</p><p><strong>语法1</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = torch.rand(5, 3)</span><br><span class="line">print(x + y)</span><br></pre></td></tr></table></figure><p>输出:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0.9842  1.5171  0.8148</span><br><span class="line"> 1.1334  1.6540  1.5739</span><br><span class="line"> 0.9804  1.1647  0.4759</span><br><span class="line"> 0.6232  0.2689  1.0596</span><br><span class="line"> 1.0777  1.1705  0.3206</span><br><span class="line">[torch.FloatTensor of size 5x3]</span><br></pre></td></tr></table></figure><p><strong>语法二</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(torch.add(x, y))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0.9842  1.5171  0.8148</span><br><span class="line"> 1.1334  1.6540  1.5739</span><br><span class="line"> 0.9804  1.1647  0.4759</span><br><span class="line"> 0.6232  0.2689  1.0596</span><br><span class="line"> 1.0777  1.1705  0.3206</span><br><span class="line">[torch.FloatTensor of size 5x3]</span><br></pre></td></tr></table></figure><p><strong>语法三</strong>：给出一个输出向量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">result = torch.Tensor(5, 3)</span><br><span class="line">torch.add(x, y, out=result)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0.9842  1.5171  0.8148</span><br><span class="line"> 1.1334  1.6540  1.5739</span><br><span class="line"> 0.9804  1.1647  0.4759</span><br><span class="line"> 0.6232  0.2689  1.0596</span><br><span class="line"> 1.0777  1.1705  0.3206</span><br><span class="line">[torch.FloatTensor of size 5x3]</span><br></pre></td></tr></table></figure><p><strong>语法四：</strong>原地操作（in-place）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 把x加到y上</span><br><span class="line">y.add_(x)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0.9842  1.5171  0.8148</span><br><span class="line"> 1.1334  1.6540  1.5739</span><br><span class="line"> 0.9804  1.1647  0.4759</span><br><span class="line"> 0.6232  0.2689  1.0596</span><br><span class="line"> 1.0777  1.1705  0.3206</span><br><span class="line">[torch.FloatTensor of size 5x3]</span><br></pre></td></tr></table></figure><p><strong>注意</strong></p><p>任何在原地(in-place)改变张量的操作都有一个’_‘后缀。例如<code>x.copy_(y), x.t_()</code>操作将改变<code>x</code>.</p><p>你可以使用所有的numpy索引操作。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x[:, 1])</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1.5171</span><br><span class="line">1.6540</span><br><span class="line">1.1647</span><br><span class="line">0.2689</span><br><span class="line">1.1705</span><br><span class="line">[torch.FloatTensor of size 5]</span><br></pre></td></tr></table></figure><p><strong>稍后阅读</strong></p><p><a href="http://pytorch.org/docs/torch" target="_blank" rel="noopener">这里</a>描述了一百多种张量操作，包括转置，索引，数学运算，线性代数，随机数等。</p><h3 id="numpy桥"><a href="#numpy桥" class="headerlink" title="numpy桥"></a>numpy桥</h3><p>把一个torch张量转换为numpy数组或者反过来都是很简单的。</p><p>Torch张量和numpy数组将共享潜在的内存，改变其中一个也将改变另一个。</p><h4 id="把Torch张量转换为numpy数组"><a href="#把Torch张量转换为numpy数组" class="headerlink" title="把Torch张量转换为numpy数组"></a>把Torch张量转换为numpy数组</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(5)</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line"> 1</span><br><span class="line"> 1</span><br><span class="line"> 1</span><br><span class="line"> 1</span><br><span class="line">[torch.FloatTensor of size 5]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b = a.numpy()</span><br><span class="line">print(b)</span><br><span class="line">print(type(b))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ 1.  1.  1.  1.  1.]</span><br><span class="line">&lt;class &apos;numpy.ndarray&apos;&gt;</span><br></pre></td></tr></table></figure><p>通过如下操作，我们看一下numpy数组的值如何在改变。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a.add_(<span class="number">1</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line">[torch.FloatTensor of size 5]</span><br><span class="line"></span><br><span class="line">[ 2.  2.  2.  2.  2.]</span><br></pre></td></tr></table></figure><h4 id="把numpy数组转换为torch张量"><a href="#把numpy数组转换为torch张量" class="headerlink" title="把numpy数组转换为torch张量"></a>把numpy数组转换为torch张量</h4><p>看看改变numpy数组如何自动改变torch张量。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">np.add(a, <span class="number">1</span>, out=a)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[ 2.  2.  2.  2.  2.]</span><br><span class="line"></span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line">[torch.DoubleTensor of size 5]</span><br></pre></td></tr></table></figure><p>所有在CPU上的张量，除了字符张量，都支持在numpy之间转换。</p><h3 id="CUDA张量"><a href="#CUDA张量" class="headerlink" title="CUDA张量"></a>CUDA张量</h3><p>使用<code>.cuda</code>函数可以将张量移动到GPU上。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># let us run this cell only if CUDA is available</span><br><span class="line">if torch.cuda.is_available():</span><br><span class="line">x = x.cuda()</span><br><span class="line">y = y.cuda()</span><br><span class="line">x + y</span><br></pre></td></tr></table></figure><p><strong>脚本总运行时间:</strong>0.003秒</p><p><a href="http://pytorch.org/tutorials/_downloads/tensor_tutorial.py" target="_blank" rel="noopener">Python源码</a></p><p><a href="http://pytorch.org/tutorials/_downloads/tensor_tutorial.ipynb" target="_blank" rel="noopener">Jupyter源码</a></p><h2 id="二、Autograd-自动求导-automatic-differentiation"><a href="#二、Autograd-自动求导-automatic-differentiation" class="headerlink" title="二、Autograd: 自动求导(automatic differentiation)"></a>二、Autograd: 自动求导(automatic differentiation)</h2><p>PyTorch 中所有神经网络的核心是<code>autograd</code>包.我们首先简单介绍一下这个包,然后训练我们的第一个神经网络.</p><p><code>autograd</code>包为张量上的所有操作提供了自动求导.它是一个运行时定义的框架,这意味着反向传播是根据你的代码如何运行来定义,并且每次迭代可以不同.</p><p>接下来我们用一些简单的示例来看这个包</p><h3 id="变量-Variable"><a href="#变量-Variable" class="headerlink" title="变量(Variable)"></a>变量(Variable)</h3><p><em><code>autograd.Variable</code></em>是<code>autograd</code>包的核心类.它包装了<em>张量</em>(<code>Tensor</code>),支持几乎所有的张量上的操作.一旦你完成你的前向计算,可以通过<code>.backward()</code>方法来自动计算所有的梯度.</p><p>你可以通过<code>.data</code>属性来访问变量中的原始张量,关于这个变量的梯度被计算放入<code>.grad</code>属性中</p><p><img src="/images/Variable.png" alt="Variable"></p><p>对自动求导的实现还有一个非常重要的类,即<em>函数</em>(<code>Function</code>).</p><p><em>变量</em>(<code>Variable</code>)和<em>函数</em>(<code>Function</code>)是相互联系的,并形成一个非循环图来构建一个完整的计算过程.每个变量有一个<code>.grad_fn</code>属性,它指向创建该变量的一个<code>Function</code>,用户自己创建的变量除外,它的<code>grad_fn</code>属性为None.</p><p>如果你想计算导数,可以在一个变量上调用<code>.backward()</code>.如果一个<code>Variable</code>是一个标量(它只有一个元素值),你不必给该方法指定任何的参数,但是该<code>Variable</code>有多个值,你需要指定一个和该变量相同形状的的<code>grad_output</code>参数(查看API发现实际为<code>gradients</code>参数).</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.autograd import Variable</span><br></pre></td></tr></table></figure><p>创建一个变量:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.ones(2, 2), requires_grad=True)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><p>输出:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Variable containing:</span><br><span class="line"> 1  1</span><br><span class="line"> 1  1</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br></pre></td></tr></table></figure><p>在变量上执行操作:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = x + 2</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><p>输出:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Variable containing:</span><br><span class="line"> 3  3</span><br><span class="line"> 3  3</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br></pre></td></tr></table></figure><p>因为<code>y</code>是通过一个操作创建的,所以它有<code>grad_fn</code>,而<code>x</code>是由用户创建,所以它的<code>grad_fn</code>为None.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(y.grad_fn)</span><br><span class="line">print(x.grad_fn)</span><br></pre></td></tr></table></figure><p>输出:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;torch.autograd.function.AddConstantBackward object at 0x7faa6f3bdd68&gt;</span><br><span class="line">None</span><br></pre></td></tr></table></figure><p>在y上执行操作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">z = y * y * 3</span><br><span class="line">out = z.mean()</span><br><span class="line"></span><br><span class="line">print(z, out)</span><br></pre></td></tr></table></figure><p>输出:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Variable containing:</span><br><span class="line"> 27  27</span><br><span class="line"> 27  27</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br><span class="line"> Variable containing:</span><br><span class="line"> 27</span><br><span class="line">[torch.FloatTensor of size 1]</span><br></pre></td></tr></table></figure><h3 id="梯度-Gradients"><a href="#梯度-Gradients" class="headerlink" title="梯度(Gradients)"></a>梯度(Gradients)</h3><p>现在我们来执行反向传播,<code>out.backward()</code>相当于执行<code>out.backward(torch.Tensor([1.0]))</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out.backward()</span><br></pre></td></tr></table></figure><p>输出<code>out</code>对<code>x</code>的梯度d(out)/dx:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><p>输出:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Variable containing:</span><br><span class="line"> 4.5000  4.5000</span><br><span class="line"> 4.5000  4.5000</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br></pre></td></tr></table></figure><p>你应该得到一个值全为4.5的矩阵,我们把变量<code>out</code>称为$o$,则$o=\frac{1}{4}\sum_iz_i,z_i=3(x_i + 2)^2,z_i\vert_{x_i=1}=27$,因此$\frac{\partial o}{\partial x_i}=\frac{3}{2}(x_i+2),\frac{\partial o}{\partial x_i}=\frac{9}{2}=4.5$</p><p>我们还可以用自动求导做更多有趣的事!</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(3)</span><br><span class="line">x = Variable(x, requires_grad=True)</span><br><span class="line"></span><br><span class="line">y = x * 2</span><br><span class="line">while y.data.norm() &lt; 1000:</span><br><span class="line">    y = y * 2</span><br><span class="line"></span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><p>输出:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Variable containing:</span><br><span class="line"> 682.4722</span><br><span class="line">-598.8342</span><br><span class="line"> 692.9528</span><br><span class="line">[torch.FloatTensor of size 3]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">gradients = torch.FloatTensor([0.1, 1.0, 0.0001])</span><br><span class="line">y.backward(gradients)</span><br><span class="line"></span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><p>输出:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Variable containing:</span><br><span class="line">  102.4000</span><br><span class="line"> 1024.0000</span><br><span class="line">    0.1024</span><br><span class="line">[torch.FloatTensor of size 3]</span><br></pre></td></tr></table></figure><p>稍后阅读:</p><p>关于<code>Variable</code>和<code>Function</code>的文档在<a href="http://pytorch.org/docs/autograd" target="_blank" rel="noopener">http://pytorch.org/docs/autograd</a>.</p><p>以上脚本的总的运行时间为0.003秒.</p><p><a href="http://pytorch.org/tutorials/_downloads/autograd_tutorial.py" target="_blank" rel="noopener">Python源码</a></p><p><a href="http://pytorch.org/tutorials/_downloads/autograd_tutorial.ipynb" target="_blank" rel="noopener">Jupyter源码</a></p><h2 id="三、神经网络"><a href="#三、神经网络" class="headerlink" title="三、神经网络"></a>三、神经网络</h2><p>可以使用<code>torch.nn</code>包来构建神经网络.</p><p>你已知道<code>autograd</code>包,<code>nn</code>包依赖<code>autograd</code>包来定义模型并求导.一个<code>nn.Module</code>包含各个层和一个<code>forward(input)</code>方法,该方法返回<code>output</code>.</p><p>例如,我们来看一下下面这个分类数字图像的网络.</p><p><img src="/images/mnist.png" alt="mnist"></p><p align="center">convnet</p><p>他是一个简单的前馈神经网络,它接受一个输入,然后一层接着一层的输入,直到最后得到结果.</p><p>神经网络的典型训练过程如下:</p><ol><li>定义神经网络模型,它有一些可学习的参数(或者权重);</li><li>在数据集上迭代;</li><li>通过神经网络处理输入;</li><li>计算损失(输出结果和正确值的差距大小)</li><li>将梯度反向传播会网络的参数;</li><li>更新网络的参数,主要使用如下简单的更新原则:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weight = weight - learning_rate * gradient</span><br></pre></td></tr></table></figure></li></ol><h3 id="定义网络"><a href="#定义网络" class="headerlink" title="定义网络"></a>定义网络</h3><p>我们先定义一个网络</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line">import torch.nn </span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line">class Net(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        # 1 input image channel, 6 output channels, 5*5 square convolution</span><br><span class="line">        # kernel</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(1, 6, 5)</span><br><span class="line">        self.conv2 = nn.Conv2d(6, 16, 5)</span><br><span class="line">        # an affine operation: y = Wx + b</span><br><span class="line">        self.fc1 = nn.Linear(16 * 5 * 5, 120)</span><br><span class="line">        self.fc2 = nn.Linear(120, 84)</span><br><span class="line">        self.fc3 = nn.Linear(84, 10)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        # max pooling over a (2, 2) window</span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))</span><br><span class="line">        # If size is a square you can only specify a single number</span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), 2)</span><br><span class="line">        x = x.view(-1, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        return x</span><br><span class="line">    </span><br><span class="line">    def num_flat_features(self, x):</span><br><span class="line">        size = x.size()[1:] # all dimensions except the batch dimension</span><br><span class="line">        num_features = 1</span><br><span class="line">        for s in size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        return num_features</span><br><span class="line">net = Net()</span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure><p>输出:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Net (</span><br><span class="line">  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="line">  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="line">  (fc1): Linear (400 -&gt; 120)</span><br><span class="line">  (fc2): Linear (120 -&gt; 84)</span><br><span class="line">  (fc3): Linear (84 -&gt; 10)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>你只需定义<code>forward</code>函数,<code>backward</code>函数(计算梯度)在使用<code>autograd</code>时自动为你创建.你可以在<code>forward</code>函数中使用<code>Tensor</code>的任何操作.</p><p><code>net.parameters()</code>返回模型需要学习的参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">params = list(net.parameters())</span><br><span class="line">print(len(params))</span><br><span class="line">for param in params:</span><br><span class="line">print(param.size())</span><br></pre></td></tr></table></figure><p>输出:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">10</span><br><span class="line">torch.Size([6, 1, 5, 5])</span><br><span class="line">torch.Size([6])</span><br><span class="line">torch.Size([16, 6, 5, 5])</span><br><span class="line">torch.Size([16])</span><br><span class="line">torch.Size([120, 400])</span><br><span class="line">torch.Size([120])</span><br><span class="line">torch.Size([84, 120])</span><br><span class="line">torch.Size([84])</span><br><span class="line">torch.Size([10, 84])</span><br><span class="line">torch.Size([10])</span><br></pre></td></tr></table></figure><p><code>forward</code>的输入和输出都是<code>autograd.Variable</code>.<em>注意</em>:这个网络(LeNet)期望的输入大小是32*32.如果使用MNIST数据集来训练这个网络,请把图片大小重新调整到32*32.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input = Variable(torch.randn(1, 1, 32, 32))</span><br><span class="line">out = net(input)</span><br><span class="line">print(out)</span><br></pre></td></tr></table></figure><p>输出:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Variable containing:</span><br><span class="line">-0.0536 -0.0548 -0.1079  0.0030  0.0521 -0.1061 -0.1456 -0.0095  0.0704  0.0259</span><br><span class="line">[torch.FloatTensor of size 1x10]</span><br></pre></td></tr></table></figure><p>将所有参数的梯度缓存清零,然后进行随机梯度的的反向传播.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(1, 10))</span><br></pre></td></tr></table></figure><p><strong>注意</strong></p><ol><li><code>torch.nn</code> 只支持小批量输入,整个<code>torch.nn</code>包都只支持小批量样本,而不支持单个样本</li><li>例如,<code>nn.Conv2d</code>将接受一个4维的张量,每一维分别是sSamples <em> nChannels </em> Height <em> Width(样本数\</em>通道数*高*宽).</li><li>如果你有单个样本,只需使用<code>input.unsqueeze(0)</code>来添加其它的维数.</li></ol><p>在继续之前,我们回顾一下到目前为止见过的所有类.</p><p><strong>回顾</strong></p><ul><li><code>torch.Tensor</code>-一个多维数组</li><li><code>autograd.Variable</code>-包装一个<code>Tensor</code>,记录在其上执行过的操作.除了拥有<code>Tensor</code>拥有的API,还有类似<code>backward()</code>的API.也保存关于这个向量的梯度.</li><li><code>nn.Module</code>-神经网络模块.封装参数,移动到GPU上运行,导出,加载等</li><li><code>nn.Parameter</code>-一种变量,当把它赋值给一个<code>Module</code>时,被自动的注册为一个参数.</li><li><code>autograd.Function</code>-实现一个自动求导操作的前向和反向定义,每个变量操作至少创建一个函数节点,(Every <code>Variable</code> operation, creates at least a single <code>Function</code> node, that connects to functions that created a <code>Variable</code> and <em>encodes its history</em>.)</li></ul><p>现在,我们包含了如下内容:</p><ul><li>定义一个神经网络</li><li>处理输入和调用<code>backward</code></li></ul><p>剩下的内容:</p><ul><li>计算损失值</li><li>更新神经网络的权值</li></ul><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>一个损失函数接受一对<code>(output, target)</code>作为输入(output为网络的输出,target为实际值),计算一个值来估计网络的输出和目标值相差多少.</p><p>在<code>nn</code>包中有几种不同的损失函数.一个简单的损失函数是:<code>nn.MSELoss</code>,他计算输入(个人认为是网络的输出)和目标值之间的均方误差.</p><p>例如:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">out = net(input)</span><br><span class="line">target = Variable(torch.arange(1, 11))  # a dummy target, for example</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(out, target)</span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure><p>输出:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Variable containing:</span><br><span class="line"> 38.1365</span><br><span class="line">[torch.FloatTensor of size 1]</span><br></pre></td></tr></table></figure><p>现在,你反向跟踪<code>loss</code>,使用它的<code>.grad_fn</code>属性,你会看到向下面这样的一个计算图:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d</span><br><span class="line">      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear</span><br><span class="line">      -&gt; MSELoss</span><br><span class="line">      -&gt; loss</span><br></pre></td></tr></table></figure><p>所以, 当你调用<code>loss.backward()</code>,整个图关于损失被求导,图中所有变量将拥有<code>.grad</code>变量来累计他们的梯度.</p><p>为了说明,我们反向跟踪几步:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(loss.grad_fn)  # MSELoss</span><br><span class="line">print(loss.grad_fn.next_functions[0][0])  # Linear</span><br><span class="line">print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU</span><br></pre></td></tr></table></figure><p>输出:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;torch.autograd.function.MSELossBackward object at 0x7fb3c0dcf4f8&gt;</span><br><span class="line">&lt;torch.autograd.function.AddmmBackward object at 0x7fb3c0dcf408&gt;</span><br><span class="line">&lt;AccumulateGrad object at 0x7fb3c0db79e8&gt;</span><br></pre></td></tr></table></figure><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>为了反向传播误差,我们所需做的是调用<code>loss.backward()</code>.你需要清除已存在的梯度,否则梯度将被累加到已存在的梯度.</p><p>现在,我们将调用<code>loss.backward()</code>,并查看conv1层的偏置项在反向传播前后的梯度.</p><p>输出(官网的例子)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">conv1.bias.grad before backward</span><br><span class="line">Variable containing:</span><br><span class="line"> 0numpy</span><br><span class="line"> 0</span><br><span class="line"> 0</span><br><span class="line"> 0</span><br><span class="line"> 0</span><br><span class="line"> 0</span><br><span class="line">[torch.FloatTensor of size 6]</span><br><span class="line"></span><br><span class="line">conv1.bias.grad after backward</span><br><span class="line">Variable containing:</span><br><span class="line">-0.0317</span><br><span class="line">-0.1682</span><br><span class="line">-0.0158</span><br><span class="line"> 0.2276</span><br><span class="line">-0.0148</span><br><span class="line">-0.0254</span><br><span class="line">[torch.FloatTensor of size 6]</span><br></pre></td></tr></table></figure><p>本人运行输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">conv1.bias.grad before backward</span><br><span class="line">None</span><br><span class="line">conv1.bias.grad after backward</span><br><span class="line">Variable containing:</span><br><span class="line"> 0.0011</span><br><span class="line"> 0.1170</span><br><span class="line">-0.0012</span><br><span class="line">-0.0204</span><br><span class="line">-0.0325</span><br><span class="line">-0.0648</span><br><span class="line">[torch.FloatTensor of size 6]</span><br></pre></td></tr></table></figure><p>不同之处在于backward之前不同,官网示例的梯度为0,而实际运行出来却是None.</p><p>现在我们已知道如何使用损失函数.</p><p><strong>稍后阅读</strong></p><p>神经网络包包含了各种用来构成深度神经网络构建块的模块和损失函数,一份完整的文档查看<a href="http://pytorch.org/docs/nn" target="_blank" rel="noopener">这里</a></p><p><strong>唯一剩下的内容:</strong></p><ul><li>更新网络的权重</li></ul><h3 id="更新权重"><a href="#更新权重" class="headerlink" title="更新权重"></a>更新权重</h3><p>实践中最简单的更新规则是随机梯度下降(SGD)．</p><script type="math/tex; mode=display">weight = weight - learning\_rate * gradient</script><p>我们可以使用简单的Python代码实现这个规则.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = 0.01</span><br><span class="line">for f in net.parameters():</span><br><span class="line">f.data.sub_(f.grad.data * learning_rate)</span><br></pre></td></tr></table></figure><p>然而,当你使用神经网络是,你想要使用各种不同的更新规则,比如SGD,Nesterov-SGD,Adam, RMSPROP等.为了能做到这一点,我们构建了一个包<code>torch.optim</code>实现了所有的这些规则.使用他们非常简单:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import torch.optim as optim</span><br><span class="line"></span><br><span class="line"># create your optimizer</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=0.01)</span><br><span class="line"></span><br><span class="line">#in your trainning loop:</span><br><span class="line">optimizer.zero_grad()  # zero the gradient buffers</span><br><span class="line">output = net(input)</span><br><span class="line">loss = criter(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step() # does the update</span><br></pre></td></tr></table></figure><p><strong>脚本总运行时间: 0.367秒</strong></p><p><a href="http://pytorch.org/tutorials/_downloads/neural_networks_tutorial.py" target="_blank" rel="noopener">Python源码</a></p><p><a href="http://pytorch.org/tutorials/_downloads/neural_networks_tutorial.ipynb" target="_blank" rel="noopener">Jupyter源码</a></p><h2 id="四、训练一个分类器"><a href="#四、训练一个分类器" class="headerlink" title="四、训练一个分类器"></a>四、训练一个分类器</h2><p>你已经看到如何去定义一个神经网络,计算损失值和更新网络的权重.</p><p>你现在可能在思考.</p><h3 id="关于数据"><a href="#关于数据" class="headerlink" title="关于数据"></a>关于数据</h3><p>通常，当你处理图像，文本，音频和视频数据时，你可以使用标准的Python包来加载数据到一个numpy数组中.然后把这个数组转换成<code>torch.*Tensor</code>.</p><ul><li>对于图像,有诸如Pillow,OpenCV包.</li><li>对于音频,有诸如scipy和librosa包</li><li>对于文本,原始Python和Cython来加载,或者NLTK和SpaCy是有用的.</li></ul><p>对于视觉,我们创建了一个<code>torchvision</code>包,包含常见数据集的数据加载,比如Imagenet,CIFAR10,MNIST等,和图像转换器,也就是<code>torchvision.datasets</code>和<code>torch.utils.data.DataLoader</code>.</p><p>这提供了巨大的便利,也避免了代码的重复.</p><p>在这个教程中,我们使用CIFAR10数据集,它有如下10个类别:’airplane’,’automobile’,’bird’,’cat’,’deer’,’dog’,’frog’,’horse’,’ship’,’truck’.这个数据集中的图像大小为3*32*32,即,3通道,32*32像素.</p><p><img src="/images/cifar10.png" alt="cifar10"></p><h3 id="训练一个图像分类器"><a href="#训练一个图像分类器" class="headerlink" title="训练一个图像分类器"></a>训练一个图像分类器</h3><p>我们将一次按照下列顺序进行:</p><ol><li>使用<code>torchvision</code>加载和归一化CIFAR10训练集和测试集.</li><li>定义一个卷积神经网络</li><li>定义损失函数</li><li>在训练集上训练网络</li><li>在测试机上测试网络</li></ol><h4 id="1-加载和归一化CIFAR0"><a href="#1-加载和归一化CIFAR0" class="headerlink" title="1. 加载和归一化CIFAR0"></a>1. 加载和归一化CIFAR0</h4><p>使用<code>torchvision</code>加载CIFAR10是非常容易的.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torchvision</span><br><span class="line">import torchvision.transforms as transforms</span><br></pre></td></tr></table></figure><p>torchvision的输出是[0,1]的PILImage图像,我们把它转换为归一化范围为[-1, 1]的张量.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])</span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=&apos;./data&apos;, train=True,</span><br><span class="line">                                        download=True, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,</span><br><span class="line">                                          shuffle=True, num_workers=2)</span><br><span class="line"></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=&apos;./data&apos;, train=False,</span><br><span class="line">                                       download=True, transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=4,</span><br><span class="line">                                         shuffle=False, num_workers=2)</span><br><span class="line"></span><br><span class="line">classes = (&apos;plane&apos;, &apos;car&apos;, &apos;bird&apos;, &apos;cat&apos;,</span><br><span class="line">           &apos;deer&apos;, &apos;dog&apos;, &apos;frog&apos;, &apos;horse&apos;, &apos;ship&apos;, &apos;truck&apos;)</span><br></pre></td></tr></table></figure></p><p>输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Files already downloaded and verified</span><br><span class="line">Files already downloaded and verified</span><br></pre></td></tr></table></figure><p>为了好玩,我们展示一些训练图像.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># functions to show an image</span><br><span class="line"></span><br><span class="line">def imshow(img):</span><br><span class="line">    img = img / 2 + 0.5     # unnormalize</span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (1, 2, 0)))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># get some random training images</span><br><span class="line">dataiter = iter(trainloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"># show images</span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"># print labels</span><br><span class="line">print(&apos; &apos;.join(&apos;%5s&apos; % classes[labels[j]] for j in range(4)))</span><br></pre></td></tr></table></figure><p>输出:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">truck   cat   car plane</span><br></pre></td></tr></table></figure><p><img src="/images/sphx_glr_cifar10_tutorial_001.png" alt="sphx_glr_cifar10_tutorial_001"></p><h4 id="2-定义一个卷积神经网络"><a href="#2-定义一个卷积神经网络" class="headerlink" title="2. 定义一个卷积神经网络"></a>2. 定义一个卷积神经网络</h4><p>从之前的神经网络一节复制神经网络代码,并修改为接受3通道图像取代之前的接受单通道图像.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">from torch.autograd import Variable</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Net(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(3, 6, 5)</span><br><span class="line">        self.pool = nn.MaxPool2d(2, 2)</span><br><span class="line">        self.conv2 = nn.Conv2d(6, 16, 5)</span><br><span class="line">        self.fc1 = nn.Linear(16 * 5 * 5, 120)</span><br><span class="line">        self.fc2 = nn.Linear(120, 84)</span><br><span class="line">        self.fc3 = nn.Linear(84, 10)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(-1, 16 * 5 * 5)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br></pre></td></tr></table></figure><h4 id="3-定义损失函数和优化器"><a href="#3-定义损失函数和优化器" class="headerlink" title="3. 定义损失函数和优化器"></a>3. 定义损失函数和优化器</h4><p>我们使用交叉熵作为损失函数,使用带动量的随机梯度下降.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import torch.optim as optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)</span><br></pre></td></tr></table></figure><h4 id="4-训练网络"><a href="#4-训练网络" class="headerlink" title="4. 训练网络"></a>4. 训练网络</h4><p>这是开始有趣的时刻.我们只需在数据迭代器上循环,听歌数据输入给网络,并优化.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">for epoch in range(2):  # loop over the dataset multiple times</span><br><span class="line"></span><br><span class="line">    running_loss = 0.0</span><br><span class="line">    for i, data in enumerate(trainloader, 0):</span><br><span class="line">        # get the inputs</span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        # wrap them in Variable</span><br><span class="line">        inputs, labels = Variable(inputs), Variable(labels)</span><br><span class="line"></span><br><span class="line">        # zero the parameter gradients</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        # forward + backward + optimize</span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        # print statistics</span><br><span class="line">        running_loss += loss.data[0]</span><br><span class="line">        if i % 2000 == 1999:    # print every 2000 mini-batches</span><br><span class="line">            print(&apos;[%d, %5d] loss: %.3f&apos; %</span><br><span class="line">                  (epoch + 1, i + 1, running_loss / 2000))</span><br><span class="line">            running_loss = 0.0</span><br><span class="line"></span><br><span class="line">print(&apos;Finished Training&apos;)</span><br></pre></td></tr></table></figure><p>输出:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[1,  2000] loss: 2.191</span><br><span class="line">[1,  4000] loss: 1.866</span><br><span class="line">[1,  6000] loss: 1.696</span><br><span class="line">[1,  8000] loss: 1.596</span><br><span class="line">[1, 10000] loss: 1.502</span><br><span class="line">[1, 12000] loss: 1.496</span><br><span class="line">[2,  2000] loss: 1.422</span><br><span class="line">[2,  4000] loss: 1.370</span><br><span class="line">[2,  6000] loss: 1.359</span><br><span class="line">[2,  8000] loss: 1.321</span><br><span class="line">[2, 10000] loss: 1.311</span><br><span class="line">[2, 12000] loss: 1.275</span><br><span class="line">Finished Training</span><br></pre></td></tr></table></figure><h4 id="5-在测试集上测试网络"><a href="#5-在测试集上测试网络" class="headerlink" title="5. 在测试集上测试网络"></a>5. 在测试集上测试网络</h4><p>我们在整个训练集上训练了两次网络,但是我们需要检查网络是否从数据集中学习到东西.</p><p>我们通过预测神经网络输出的类别标签并根据实际情况进行检测.如果预测正确,我们把该样本添加到正确预测列表.</p><p>第一步,显示测试集中的图片一遍熟悉图片内容.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dataiter = iter(testloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"># print images</span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line">print(&apos;GroundTruth: &apos;, &apos; &apos;.join(&apos;%5s&apos; % classes[labels[j]] for j in range(4)))</span><br></pre></td></tr></table></figure><p><img src="/images/sphx_glr_cifar10_tutorial_002.png" alt="sphx_glr_cifar10_tutorial_002"></p><p>输出:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GroundTruth:    cat  ship  ship plane</span><br></pre></td></tr></table></figure><p>现在我们来看看神经网络认为以上图片是什么?</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">outputs = net(Variable(images))</span><br></pre></td></tr></table></figure><p>输出是10个标签的能量.一个类别的能量越大,神经网络越认为他是这个类别.所以让我们得到最高能量的标签.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">_, predicted = torch.max(outputs.data, 1)</span><br><span class="line"></span><br><span class="line">print(&apos;Predicted: &apos;, &apos; &apos;.join(&apos;%5s&apos; % classes[predicted[j]]</span><br><span class="line">                              for j in range(4)))</span><br></pre></td></tr></table></figure><p>输出:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Predicted:    cat  ship   car plane</span><br></pre></td></tr></table></figure><p>这结果看起来非常的好.</p><p>接下来让我们看看网络在整个测试集上的结果如何.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">correct = 0</span><br><span class="line">total = 0</span><br><span class="line">for data in testloader:</span><br><span class="line">    images, labels = data</span><br><span class="line">    outputs = net(Variable(images))</span><br><span class="line">    _, predicted = torch.max(outputs.data, 1)</span><br><span class="line">    total += labels.size(0)</span><br><span class="line">    correct += (predicted == labels).sum()</span><br><span class="line"></span><br><span class="line">print(&apos;Accuracy of the network on the 10000 test images: %d %%&apos; % (</span><br><span class="line">    100 * correct / total))</span><br></pre></td></tr></table></figure><p>输出:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy of the network on the 10000 test images: 55 %</span><br></pre></td></tr></table></figure><p>结果看起来好于偶然,偶然的正确率为10%,似乎网络学习到了一些东西.</p><p>那在什么类上预测较好,什么类预测结果不好呢.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">class_correct = list(0. for i in range(10))</span><br><span class="line">class_total = list(0. for i in range(10))</span><br><span class="line">for data in testloader:</span><br><span class="line">    images, labels = data</span><br><span class="line">    outputs = net(Variable(images))</span><br><span class="line">    _, predicted = torch.max(outputs.data, 1)</span><br><span class="line">    c = (predicted == labels).squeeze()</span><br><span class="line">    for i in range(4):</span><br><span class="line">        label = labels[i]</span><br><span class="line">        class_correct[label] += c[i]</span><br><span class="line">        class_total[label] += 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for i in range(10):</span><br><span class="line">    print(&apos;Accuracy of %5s : %2d %%&apos; % (</span><br><span class="line">        classes[i], 100 * class_correct[i] / class_total[i]))</span><br></pre></td></tr></table></figure><p>输出:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Accuracy of plane : 60 %</span><br><span class="line">Accuracy of   car : 46 %</span><br><span class="line">Accuracy of  bird : 44 %</span><br><span class="line">Accuracy of   cat : 35 %</span><br><span class="line">Accuracy of  deer : 38 %</span><br><span class="line">Accuracy of   dog : 43 %</span><br><span class="line">Accuracy of  frog : 57 %</span><br><span class="line">Accuracy of horse : 76 %</span><br><span class="line">Accuracy of  ship : 71 %</span><br><span class="line">Accuracy of truck : 74 %</span><br></pre></td></tr></table></figure><p>接下来干什么?</p><p>我们如何在GPU上运行神经网络呢?</p><h3 id="在GPU上训练"><a href="#在GPU上训练" class="headerlink" title="在GPU上训练"></a>在GPU上训练</h3><p>你是如何把一个<code>Tensor</code>转换GPU上,你就如何把一个神经网络移动到GPU上训练.这个操作会递归遍历有所模块,并将其参数和缓冲区转换为CUDA张量.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.cuda()</span><br></pre></td></tr></table></figure><p>请记住,你也必须在每一步中把你的输入和目标值转换到GPU上:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputs, labels = Variable(inputs.cuda()), Variable(target.cuda())</span><br></pre></td></tr></table></figure><p>为什么我们没注意到GPU的速度提升很多?那是因为网络非常的小.</p><p><strong>实践</strong>:尝试增加你的网络的宽度(第一个<code>nn.Conv2d</code>的第2个参数, 第二个<code>nn.Conv2d</code>的第一个参数,他们需要是相同的数字),看看你得到了什么样的加速.</p><p><strong>实现的目标:</strong></p><ul><li>深入了解了PyTorch的张量库和神经网络.</li><li>训练了一个小网络来分类图片.</li></ul><h3 id="在多GPU上训练"><a href="#在多GPU上训练" class="headerlink" title="在多GPU上训练"></a>在多GPU上训练</h3><p>如果你希望使用所有GPU来更大的加快速度,请查看<a href="http://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html" target="_blank" rel="noopener">选读:数据并行</a></p><h3 id="接下来做什么"><a href="#接下来做什么" class="headerlink" title="接下来做什么?"></a>接下来做什么?</h3><ul><li><a href="http://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html" target="_blank" rel="noopener">训练神经网络玩电子游戏</a></li><li><a href="https://github.com/pytorch/examples/tree/master/imagenet" target="_blank" rel="noopener">在ImageNet上训练最好的ResNet</a></li><li><a href="https://github.com/pytorch/examples/tree/master/dcgan" target="_blank" rel="noopener">使用对抗生成网络来训练一个人脸生成器</a></li><li><a href="https://github.com/pytorch/examples/tree/master/word_language_model" target="_blank" rel="noopener">使用LSTM网络训练一个字符级的语言模型</a></li><li><a href="https://github.com/pytorch/examples" target="_blank" rel="noopener">更多示例</a></li><li><a href="https://github.com/pytorch/tutorials" target="_blank" rel="noopener">更多教程</a></li><li><a href="https://discuss.pytorch.org/" target="_blank" rel="noopener">在论坛上讨论PyTorch</a></li><li><a href="[Chat with other users on Slack](http://pytorch.slack.com/messages/beginner/">在Slack上与其他用户聊天</a>)</li></ul><p><strong>脚本总运行时间:</strong>3:24.484</p><p><a href="http://pytorch.org/tutorials/_downloads/cifar10_tutorial.py" target="_blank" rel="noopener">Python源码</a></p><p><a href="http://pytorch.org/tutorials/_downloads/cifar10_tutorial.ipynb" target="_blank" rel="noopener">Jupyter源码</a></p><h2 id="五、数据并行-选读"><a href="#五、数据并行-选读" class="headerlink" title="五、数据并行(选读)"></a>五、数据并行(选读)</h2><p><strong>作者</strong>:<a href="https://github.com/hunkim" target="_blank" rel="noopener">Sung Kim</a>和<a href="https://github.com/jennykang" target="_blank" rel="noopener">Jenny Kang</a></p><p>在这个教程里,我们将学习如何使用<code>DataParallel</code>来使用多GPU.</p><p>PyTorch非常容易的就可以使用GPU,你可以用如下方式把一个模型防盗GPU上:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.gpu()</span><br></pre></td></tr></table></figure><p>然后你可以复制所有的张量到GPU上:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mytensor = mytensor.gpu()</span><br></pre></td></tr></table></figure><p>请注意,只调用<code>mytensor.gpu()</code>并没有复制张量到GPU上。你需要把它赋值给一个新的张量并在GPU上使用这个张量。</p><p>在多GPU上执行前向和反向传播是自然而然的事。然而，PyTorch默认将只是用一个GPU。你可以使用<code>DataParallel</code>让模型并行运行来轻易的让你的操作在多个GPU上运行。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = nn.DataParallel(model)</span><br></pre></td></tr></table></figure><p>这是这篇教程背后的核心，我们接下来将更详细的介绍它。</p><h3 id="导入和参数"><a href="#导入和参数" class="headerlink" title="导入和参数"></a>导入和参数</h3><p>导入PyTorch模块和定义参数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line">from torch.utils.data import Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"># Parameters and DataLoaders</span><br><span class="line">input_size = 5</span><br><span class="line">output_size = 2</span><br><span class="line"></span><br><span class="line">batch_size = 30</span><br><span class="line">data_size = 100</span><br></pre></td></tr></table></figure><h3 id="虚拟数据集"><a href="#虚拟数据集" class="headerlink" title="虚拟数据集"></a>虚拟数据集</h3><p>制作一个虚拟（随机）数据集，你只需实现<code>__getitem__</code>.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class RandomDataset(Dataset):</span><br><span class="line"></span><br><span class="line">    def __init__(self, size, length):</span><br><span class="line">        self.len = length</span><br><span class="line">        self.data = torch.randn(length, size)</span><br><span class="line"></span><br><span class="line">    def __getitem__(self, index):</span><br><span class="line">        return self.data[index]</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        return self.len</span><br><span class="line"></span><br><span class="line">rand_loader = DataLoader(dataset=RandomDataset(input_size, 100),</span><br><span class="line">                         batch_size=batch_size, shuffle=True)</span><br></pre></td></tr></table></figure><h3 id="简单模型"><a href="#简单模型" class="headerlink" title="简单模型"></a>简单模型</h3><p>作为演示，我们的模型只接受一个输入，执行一个线性操作，然后得到结果。然而，你能在任何模型（CNN，RNN，Capsule Net等）上使用<code>DataParallel</code>。</p><p>我们在模型内部放置了一条打印语句来检测输入和输出向量的大小。请注意批等级为0时打印的内容。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class Model(nn.Module):</span><br><span class="line">    # Our model</span><br><span class="line"></span><br><span class="line">    def __init__(self, input_size, output_size):</span><br><span class="line">        super(Model, self).__init__()</span><br><span class="line">        self.fc = nn.Linear(input_size, output_size)</span><br><span class="line"></span><br><span class="line">    def forward(self, input):</span><br><span class="line">        output = self.fc(input)</span><br><span class="line">        print(&quot;  In Model: input size&quot;, input.size(),</span><br><span class="line">              &quot;output size&quot;, output.size())</span><br><span class="line"></span><br><span class="line">        return output</span><br></pre></td></tr></table></figure><h3 id="创建一个模型和数据并行"><a href="#创建一个模型和数据并行" class="headerlink" title="创建一个模型和数据并行"></a>创建一个模型和数据并行</h3><p>这是本教程的核心部分。首先，我们需要创建一个模型实例和检测我们是否有多个GPU。如果我们有多个GPU，我们使用<code>nn.DataParallel</code>来包装我们的模型。然后通过<code>model.gpu()</code>（看代码实际是<code>model.cuda()</code>)把模型放到GPU上。</p><h3 id="运行模型"><a href="#运行模型" class="headerlink" title="运行模型"></a>运行模型</h3><p>现在我们可以看输入和输出张量的大小。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">for data in rand_loader:</span><br><span class="line">    if torch.cuda.is_available():</span><br><span class="line">        input_var = Variable(data.cuda())</span><br><span class="line">    else:</span><br><span class="line">        input_var = Variable(data)</span><br><span class="line"></span><br><span class="line">    output = model(input_var)</span><br><span class="line">    print(&quot;Outside: input size&quot;, input_var.size(),</span><br><span class="line">          &quot;output_size&quot;, output.size())</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">  In Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">  In Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">  In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</span><br></pre></td></tr></table></figure><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>当我们对30个输入和输出进行批处理时，我们和期望的一样得到30个输入和输出，但是如果你有多个GPU，你得到如下的结果。</p><p><strong>2个GPU</strong></p><p>如果你有2个GPU，你将看到：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># on 2 GPUs</span><br><span class="line">Let&apos;s use 2 GPUs!</span><br><span class="line">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])</span><br><span class="line">    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])</span><br><span class="line">Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</span><br></pre></td></tr></table></figure><p><strong>3个GPU</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Let&apos;s use 3 GPUs!</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</span><br></pre></td></tr></table></figure><p><strong>8个GPU</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">Let&apos;s use 8 GPUs!</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>DataParallel自动的划分数据，并将作业发送到多个GPU上的多个模型。在每个模型完成作业后，DataParallel收集并合并结果返回给你。</p><p>更多信息请看这里：</p><p><a href="http://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html" target="_blank" rel="noopener">http://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html</a></p><p><strong>脚本总运行时间：</strong>0.0003秒</p><p><a href="http://pytorch.org/tutorials/_downloads/data_parallel_tutorial.py" target="_blank" rel="noopener">Python源码</a></p><p><a href="http://pytorch.org/tutorials/_downloads/data_parallel_tutorial.ipynb" target="_blank" rel="noopener">Jupyter源码</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;此教程为翻译&lt;a href=&quot;http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官方地址&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a hr
      
    
    </summary>
    
      <category term="PyTorch" scheme="http://yoursite.com/categories/PyTorch/"/>
    
    
      <category term="PyTorch" scheme="http://yoursite.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Hexo 搭建 github 个人博客</title>
    <link href="http://yoursite.com/2017/11/27/hexo-configuration/"/>
    <id>http://yoursite.com/2017/11/27/hexo-configuration/</id>
    <published>2017-11-26T16:00:00.000Z</published>
    <updated>2018-03-20T14:02:16.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/hexojs/hexo" target="_blank" rel="noopener">Hexo</a>是一款基于 Node.js 的静态博客框架,因此可以看出用Hexo搭建个人博客需要使用Node.js、Github申请，使用过github的人应该都知道Git工具。因此我们需要如下几个工具：</p><ul><li>Node.js</li><li>Git</li><li>申请Github</li></ul><p>今天我们都假设这些基本工作都已准备，直接在 Ubuntu 系统上搭建 Hexo 环境，并使用 Material 主题。</p><h2 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h2><p>1.我们首选需要全局安装Hexo，执行如下命令会在全局环境安装Hexo包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-cli -g</span><br></pre></td></tr></table></figure><p>执行以上命令后，我们可以通过如下命令检测是否安装成功：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo -v</span><br></pre></td></tr></table></figure><p>如果安装成功则会输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hexo-cli: 1.0.4</span><br><span class="line">os: Linux 4.4.0-101-generic linux x64</span><br><span class="line">http_parser: 2.7.0</span><br><span class="line">node: 9.2.0</span><br><span class="line">v8: 6.2.414.44-node.11</span><br><span class="line">uv: 1.16.1</span><br><span class="line">zlib: 1.2.11</span><br><span class="line">ares: 1.13.0</span><br><span class="line">modules: 59</span><br><span class="line">nghttp2: 1.25.0</span><br><span class="line">openssl: 1.0.2m</span><br><span class="line">icu: 60.1</span><br><span class="line">unicode: 10.0</span><br><span class="line">cldr: 32.0</span><br><span class="line">tz: 2017c</span><br></pre></td></tr></table></figure><p>2.接着我们初始化我们的博客环境，这里有两种方法</p><ul><li><p>先建立一个 blogname 文件夹，然后进入文件夹执行如下语句：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo init</span><br></pre></td></tr></table></figure><p>然后hexo会为我们生成一些必要的文件夹和文件。</p></li><li><p>第二种方法是，不建立文件夹，直接在命令中添加文件夹名</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo init blogname</span><br></pre></td></tr></table></figure><p>这中方式和第一种方式的效果是相同的，生成的内容如下</p></li></ul><p><img src="/images/17-11-26-1.png" alt="17-11-26-1"></p><p>至此，我们的博客环境已经搭建好，那么我们该怎么查看博客内容呢，首先我们需要把我们的内容生成静态页面，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo generate</span><br></pre></td></tr></table></figure><p>或者简写<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo g</span><br></pre></td></tr></table></figure></p><p>接着我们可以启动本地服务器来查看网页内容，启动本地服务器的命令为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo server</span><br></pre></td></tr></table></figure></p><p>简写为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure></p><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">INFO  Start processing</span><br><span class="line">INFO  Hexo is running at http://localhost:4000/. Press Ctrl+C to stop.</span><br></pre></td></tr></table></figure></p><p>此时在浏览器中输入<a href="http://localhost:4000/" target="_blank" rel="noopener">http://localhost:4000/</a> ,即可看到我们的界面<br><img src="/images/20171129hexohome.png" alt="hexo home"><br>除了使用以上命令，我们还可以使用如下命令指定端口<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo s -p 端口号</span><br></pre></td></tr></table></figure></p><p>接下来我们就是把hexo生成的内容不熟到github上，首先在主题_config.yml文件家中天机如下内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repository: 这里填写你的github博客的那个仓库的ssh地址</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure></p><p>注意，以上的冒号后面有一个空格。<br>然后执行一下安装命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure></p><p>然后执行部署命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo deploy</span><br></pre></td></tr></table></figure></p><p>或者<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo d</span><br></pre></td></tr></table></figure></p><p>然后在浏览器中输入<a href="http://githubname.github.io" target="_blank" rel="noopener">http://githubname.github.io</a> 就可以了，githubname为你的github账户名。</p><p>当我们新建博客时，执行一下命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new 博客名字</span><br></pre></td></tr></table></figure></p><p>这时会在./source/_posts 文件夹下生成一个.md文件夹，同时在我们编写md文件夹时的图片资源放在source文件夹下，假设我们在source文件下新建一个images（这个名字任意取），并有一张叫 temp.png 图片，我们这 .md 文件家中这样引用它：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![temp](/images/temp.png)</span><br></pre></td></tr></table></figure></p><p>此时我们基础的 hexo 模板已经搭建完毕， 接下来我们为 hexo 配置 <a href="https://github.com/viosey/hexo-theme-material" target="_blank" rel="noopener">Material主题</a>，首先我们从Github上获取Material，然后将其放置在hexo工程项目文件夹下的themes目录下，同时将其更名为 <code>material</code>，然后将主题配置文件（即刚下载的material文件下的_config.yml)中的<code>theme</code> 字段更改为material。其他的配置美化，可参考<a href="https://material.viosey.com/docs/#/start" target="_blank" rel="noopener">官方文档</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://github.com/hexojs/hexo&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;是一款基于 Node.js 的静态博客框架,因此可以看出用Hexo搭建个人博客需要使用Node.js、Github
      
    
    </summary>
    
      <category term="Hexo" scheme="http://yoursite.com/categories/Hexo/"/>
    
    
      <category term="Hexo" scheme="http://yoursite.com/tags/Hexo/"/>
    
  </entry>
  
</feed>
