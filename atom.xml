<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>技术闲谈</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-08-10T06:25:03.480Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>茂松</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>目标检测 —— Selective Search 算法</title>
    <link href="http://yoursite.com/2019/08/10/cv-selective-search/"/>
    <id>http://yoursite.com/2019/08/10/cv-selective-search/</id>
    <published>2019-08-10T05:38:58.000Z</published>
    <updated>2019-08-10T06:25:03.480Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://ranmaosong.github.io/2019/08/10/cv-selective-search/" target="_blank" rel="noopener">GitHub</a><br><a href="https://www.jianshu.com/p/351a1afc0d52" target="_blank" rel="noopener">简书</a><br><a href="https://blog.csdn.net/u014630987/article/details/99072284" target="_blank" rel="noopener">CSDN</a></p><p>论文题目: <a href="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf" target="_blank" rel="noopener">Selective Search for Object Recognition</a></p><h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h1><p>由于目标检测和图像分类的不同，一张图可能存在多个目标，因此为了定位和识别出图片中的目标，一个简单的做法是通过 <strong>滑动窗口</strong> 的方法来将图像分割成许多的子区域，然后进行识别和坐标修正。但是这种方法有一个缺点就是： 他是在图像上通过穷举的方法找出所有的子区域，这样就会导致生成的小区域数量居多。而 Selective Search 是另一种基于区域的方法，该方法能大大减少子区域的数量。并且在 R-CNN 和 Fast R-CNN 中均利用该算法来生成 Region Proposals。</p><h1 id="2-Selective-Search"><a href="#2-Selective-Search" class="headerlink" title="2. Selective Search"></a>2. Selective Search</h1><p>该算法有三个优势：</p><ol><li>捕捉不同尺度(Capture All Scales)</li><li>多样化(Diversification)</li><li>计算速度快(Fast to Compute)</li></ol><p>该方法主要包括两个内容：</p><ol><li>Hierarchical Grouping Algorithm</li><li>Diversification Strategies</li></ol><p>选择性算法使用的是按层次合并算法（Hierarchical Grouping），基本思路如下：首先使用论文“Efficient Graph-Based Image Segmentation”中的方法生成一些起始的小区域，之后使用贪心算法将区域归并到一起：先计算所有临近区域间的相似度（通过颜色，纹理，吻合度，大小等相似度），将最相似的两个区域归并，然后重新计算临近区域间的相似度，归并相似区域直至整幅图像成为一个区域</p><h2 id="2-1-Hierarchical-Grouping-Algorithm-层次合并算法"><a href="#2-1-Hierarchical-Grouping-Algorithm-层次合并算法" class="headerlink" title="2.1  Hierarchical Grouping Algorithm(层次合并算法)"></a>2.1  Hierarchical Grouping Algorithm(层次合并算法)</h2><p><img src="\images\ss-01.png" alt=""></p><p>使用 Efficient Graph-Based Image Segmentation的方法获取原始分割区域R={r1,r2,…,rn}；<br>初始化相似度集合S=∅；<br>计算两两相邻区域之间的相似度，将其添加到相似度集合S中；</p><p>从相似度集合S中找出相似度最大的两个区域 ri 和rj，<br>将其合并成为一个区域 rt，<br>从相似度集合S中除去原先与ri和rj相邻区域之间计算的相似度，<br>计算rt与其相邻区域（原先与ri或rj相邻的区域）的相似度，将其结果添加到相似度集合S中，<br>同时将新区域rt添加到区域集合R中；</p><p>在集合R中所有区域获取每个区域的Bounding Boxes，这个结果就是物体位置的可能结果L。</p><h2 id="2-2-Diversification-Strategies（多样性策略）"><a href="#2-2-Diversification-Strategies（多样性策略）" class="headerlink" title="2.2 Diversification Strategies（多样性策略）"></a>2.2 Diversification Strategies（多样性策略）</h2><p>作者在这里主要考虑了三种不同的多样性策略：</p><ol><li>颜色空间多样性</li><li>相似度多样性</li><li>初始化区域多样性</li></ol><h3 id="2-2-1-颜色空间多样性"><a href="#2-2-1-颜色空间多样性" class="headerlink" title="2.2.1 颜色空间多样性"></a>2.2.1 颜色空间多样性</h3><p>考虑场景以及光照条件等，作者采用了8种不同的颜色空间，这个策略主要应用于图像分割算法中原始区域的生成。主要使用的颜色空间有：（1）RGB，（2）灰度I，（3）Lab，（4）rgI（归一化的rg通道加上灰度），（5）HSV，（6）rgb（归一化的RGB），（7）C（8）H（HSV的H通道）。具体如下：</p><p><img src="\images\ss-02.png" alt=""></p><h3 id="2-2-2-相似度多样性"><a href="#2-2-2-相似度多样性" class="headerlink" title="2.2.2 相似度多样性"></a>2.2.2 相似度多样性</h3><p><strong>1. 颜色度量</strong></p><p>$C_i=\{c_i^1, \cdots c_i^n\}$ 表示第 i 个区域的在三个通道上的研制直方图，每个颜色通道有 25 个 bins 的直方图，这样三个通道共有 n=75 个 bins, 即 $C_i$ 为一个 75 维的向量，切该直方图使用 <code>L1-norm</code>标准化了，且用如下公式计算颜色相似度：</p><script type="math/tex; mode=display">s_{color}(r_i, r_j)=\sum_{k=1}^n \min(r_i^k, r_j^k)\tag{1}</script><p>合并后的颜色直方图计算如下：</p><script type="math/tex; mode=display">C_t = \frac{size(r_i) * C_i + size(r_j) * C_j}{size(r_i)+size(r_j)}\tag{2}</script><p><strong>2. 纹理相似度</strong></p><p>纹理采用SIFT-Like特征，具体做法是对每个颜色通道的8个不同方向计算方差σ=1的高斯微分（Gaussian Derivative），每个通道每个颜色获取10 bins的直方图（L1-norm归一化），这样就可以获取到一个240维的向量 $T_i=\{t_i^1, t_i^2, \cdots t_i^n\}$，区域之间纹理相似度计算方式和颜色相似度计算方式类似，合并之后新区域的纹理特征计算方式和颜色特征计算相同</p><script type="math/tex; mode=display">s_{texture}(r_i, r_j)=\sum_{k=1}^n min(t_i^k, t_j^k)\tag{3}</script><p><strong>3. 大小相似度</strong></p><p>大小是指区域中包含像素点的个数，使用如下公式计算ri和rj区域大小占的比例共同决定大小相似度，计算方式是总体减去两个像素和占全图像像素比例，这样可以让小区域优先级高，尽量让小的区域先合并，避免对大区域关注度过高，或者说是避免某个大区域对周围小区域进行吞并。</p><script type="math/tex; mode=display">s_{size}(r_i, r_j)=1-\frac{size(r_i) + size(r_j)}{size(im)}\tag{4}</script><p><strong>4. 形状相似度</strong></p><p>形状相似度主要是为了衡量两个区域是否更加“吻合”，其指标是合并后的区域的Bounding Box（能够框住区域的最小矩形（没有旋转））和原始两图像大小和的差越小，其吻合度越高，计算公式如下</p><script type="math/tex; mode=display">fill(r_i, r_j)=1-\frac{size(BB_{ij}-size(r_i)-size(r_j))}{size(im)}\tag{5}</script><p><strong>5. 总的相似度计算</strong></p><script type="math/tex; mode=display">s(r_i, r_j) = a_1s_{color}(r_i, r_j)+a_2s_{texture}(r_i, r_j)+a3s_{size}(r_i, r_j)+a_4fill(r_i, r_j)\tag{6}</script><h3 id="2-2-3-初始化区域多样性"><a href="#2-2-3-初始化区域多样性" class="headerlink" title="2.2.3  初始化区域多样性"></a>2.2.3  初始化区域多样性</h3><p>基于图的图像分割得到初始区域，而这个初始区域对于最终的影响是很大的，因此通过多种参数初始化图像分割，也算是扩充了多样性</p><h2 id="2-3-区域打分"><a href="#2-3-区域打分" class="headerlink" title="2.3 区域打分"></a>2.3 区域打分</h2><p>通过上述的步骤我们能够得到很多很多的区域，但是显然不是每个区域作为目标的可能性都是相同的，因此我们需要衡量这个可能性，这样就可以根据我们的需要筛选区域建议个数啦。</p><p>这篇文章做法是，给予最先合并的图片块较大的权重，比如最后一块完整图像权重为1，倒数第二次合并的区域权重为2以此类推。但是当我们策略很多，多样性很多的时候呢，这个权重就会有太多的重合了，排序不好搞啊。文章做法是给每个权重乘以一个 $[0, 1]$ 的随机数，然后对于相同的区域多次出现的也叠加下权重，毕竟多个方法都说你是目标，也是有理由的嘛。这样我就得到了所有区域的目标分数，也就可以根据自己的需要选择需要多少个区域了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;http://ranmaosong.github.io/2019/08/10/cv-selective-search/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://www.j
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>目标检测算法——R-CNN</title>
    <link href="http://yoursite.com/2019/07/23/cv-R-CNN/"/>
    <id>http://yoursite.com/2019/07/23/cv-R-CNN/</id>
    <published>2019-07-23T13:50:37.000Z</published>
    <updated>2019-07-23T14:31:56.908Z</updated>
    
    <content type="html"><![CDATA[<p>论文题目: <a href="https://arxiv.org/pdf/1311.2524.pdf" target="_blank" rel="noopener">Rich feature hierarchies for accurate object detection and semantic segmentation</a></p><p>R-CNN(Regions with CNN featurtes) 是一种 two-stage 的目标检测，是将深度学习应用到目标检测开山之作之一，并在PASCAL VOC 2010实现了 53.7% 的mAP。该方法主要有两个贡献:</p><ol><li>将卷积神经网络应用到region proposals；</li><li>利用了预训练模型来提升目标检测性能，</li></ol><h1 id="1-概览"><a href="#1-概览" class="headerlink" title="1. 概览"></a>1. 概览</h1><p>该模型整体包含五部分，其中一个时输入，另外三部分是具体的模型，如下图所示（其中位置回归没有给出）：</p><ol><li>输入：输入大小为 $227 \times 227$图像</li><li>生成候选区域: 提取 2000 个自下而上的 region proposals</li><li>提取特征: 利用 CNN 对每个 region proposal计算特征</li><li>类别判断: 在计算出的特征上，利用特定类别的 SVM 对其进行分类</li><li>位置修正: 使用回归器对候选框的位置进行修正</li></ol><p><img src="\images\r-cnn_01.png" alt=""></p><h1 id="2-Region-proposals"><a href="#2-Region-proposals" class="headerlink" title="2. Region proposals"></a>2. Region proposals</h1><p>传统的目标检测算法一般在图片上使用穷举法或华东窗口选出所有物体可能出现的区域，然后对这些区域提取特征进行式别，而这样做的缺点是复杂度很高，产生很多的冗余候选框。R-CNN 首先使用某些算法在输入的图像上生成 2000 个类别独立的 region proposas。 这些 proposals 定义了检测器的可用的候选区域。生成 region proposals 的方法有很多， 如：objectness, selective search, category-independent object proposals, constrained parametric min-cuts (CPMC), multi-scale combinatorial grouping 和 Ciresan 等。该算法使用了 selective search 算法来生成 proposals。</p><p><strong>Selective Search 主要思想:</strong></p><ol><li>使用一种过分割手段，将图像分割成小区域 (1k~2k 个)</li><li>查看现有小区域，按照合并规则合并可能性最高的相邻两个区域。重复直到整张图像合并成一个区域位置</li><li>输出所有曾经存在过的区域，所谓候选区域</li></ol><p><strong>合并规则</strong>： 由县合并以下四种区域</p><ol><li>颜色（颜色直方图）相似的</li><li>纹理(梯度直方图)相似的</li><li>合并后总面积小的： 保证合并操作的尺度较为均匀，避免一个大区域陆续“吃掉”其他小区域 （例：设有区a-b-c-d-e-f-g-h。较好的合并方式是：ab-cd-ef-gh -&gt; abcd-efgh -&gt;abcdefgh。 不好的合并方法是：ab-c-d-e-f-g-h -&gt;abcd-e-f-g-h -&gt;abcdef-gh -&gt; abcdefgh）</li><li>合并后，总面积在其BBOX中所占比例大的： 保证合并后形状规则</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文题目: &lt;a href=&quot;https://arxiv.org/pdf/1311.2524.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Rich feature hierarchies for accurate object detection
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>目标检测中的样本不平衡处理方法——OHEM, Focal Loss, GHM, PISA</title>
    <link href="http://yoursite.com/2019/07/20/cv-imbalance-between-easy-and-hard-examples/"/>
    <id>http://yoursite.com/2019/07/20/cv-imbalance-between-easy-and-hard-examples/</id>
    <published>2019-07-20T06:42:37.000Z</published>
    <updated>2019-07-24T08:52:09.090Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2019/07/20/imbalance-between-easy-and-hard-examples/" target="_blank" rel="noopener">GitHub</a><br><a href="https://www.jianshu.com/p/f305b573df8f" target="_blank" rel="noopener">简书</a><br><a href="https://blog.csdn.net/u014630987/article/details/96646662" target="_blank" rel="noopener">CSDN</a></p><h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h1><p>目前。计算机视觉中的性能最好的目标检测方法主要分为两种: one-stage 和two-stage 方法。two-stage方法分为两步，第一步为候选区域生成阶段(Proposal stage),通过如Selective Search、EdgeBoxes等方法可以生成数量相对较小候选目标检测框；第二步为分类与回归阶段，对第一阶段生成的 Candiate Proposal 进行分类和位置回归。one-stage 代表性的方法是R-CNN系列，如 R-CNN， Fast R-CNN, Faster R-CNN。 而one-stage 方法直接对图像的大量 Candiate Proposals 进行分类与回归。</p><p>这两类方法中均存在类别不平衡问题，two-stage 方法通过第一步已经将当量的候选区域降低到一个数量较小的范围，同时又在第二步通过一些启发式原则，将正负样本的比例降低到一定程度。而 one-stage 优于没有这降低候选框的步骤，因此，候选区域的数量大大超过 two-stage 方法，因此，在精度上，two-stage 仍然优于 one-stage 方法，但是在速度和模型复杂度上， one-stage 占优势。</p><p>类别不平衡会使检测器评估 $10^4-10^6$ 的候选位置，但是通常只有少量的位置存在目标，这回导致两个问题:</p><ol><li>由于大量的 easy negative 对模型的训练没有提供很大的作用，因此训练很低效；</li><li>由于在训练中 easy negatives 在数据中占主导，因此这些样本会覆盖少量的正样本的损失（即负样本的损失梯度占主导），从而导致模型退化。</li></ol><p>因此，解决样本不平衡问题是提高目标检测精度的一个关键技术之一。</p><h1 id="2-OHEM"><a href="#2-OHEM" class="headerlink" title="2. OHEM"></a>2. OHEM</h1><p>论文题目：<a href="https://arxiv.org/pdf/1604.03540.pdf" target="_blank" rel="noopener">Training Region-based Object Detectors with Online Hard Example Mining</a></p><p>OHEM 是通过改进 Hard Example Mining 方法，使其适应online learning算法特别是基于SGD的神经网络方法。Hard Example Mining 通过交替地用当前样本集训练模型，然后将模型固定，选择 False Positive 样本来重新组建用于下一次模型训练的样本集。但是因为训练神经网络本事就是一个耗时的操作，这样在训练中固定模型，这样会急剧降低模型的训练进程。</p><p>Hard Examples Mining通常有两种方法：</p><ol><li><p><strong>用于优化 SVM 的方法</strong>： 该方法首先需要一个工作的样本集，用来训练 SVM， 使其在该样本集上收敛，然后通过某些原则来添加或删除某些样本以更新该样本集。该原则是：删除那些简单样本(Easy Examples), 即分类正确的概率高与某个阈值(个人理解，原话The rule removes examples that are “easy” in the sense that they are correctly classified beyond the current model’s margin. Conversely)， 添加那些困难那样本，即分类正确的概率低于某个阈值(the rule adds new examples that are hard in the sense that they violate the current model’s margin)</p></li><li><p><strong>用于非SVM的方法</strong>：该方法首先用完整的数据集中的正样本和一个随机的负样本及来构成开始的训练样本集，然后训练模型使其在该训练集上收敛，然后完整的数据集中选择 False Positive 添加到训练样本集中。</p></li></ol><p>OHEM算法的大致流程是: 首先计算出每个ROI的loss， 然后按loss从高到低来排列每个 ROI， 然后为每张图片选择 $B/N$ 个损失最高的 ROI 作为Hard Examples，其中 B 表示总的 ROI 数量， $N$ 表示batch-size 的大小，在 Fast R-CNN 中， N=2， B=128时，效果很好。</p><p>但是如果直接按照 loss 对所有的 ROI 进行选择，会有一个缺点，由于 ROI 很多，这样 很多 ROI 的位置就会相关并重叠，如果和某个高 Loss 的 ROI 重合度很高的其它 ROI很多， 这样， 这些 ROI 的 Loss 通常也会很多，这样这些样本都会被选择，但是它们可以近似认为时同一个，这样就会给其它较低 Loss 的 ROI 更少的选择余地，这样就会存在冗余。为了消除这种冗余，作者提出先使用 NMS (non-maximum suppression) 删除部分重合度很高的 ROI， 在使用上述方法进行 选择 Hard Example。</p><p><strong>实现技巧:</strong></p><p>论文，作者将该方法是现在 Fsat R-CNN 目标检测方法中。最简单做法是更改损失函数层，损失函数层首先计算所有 ROI 的 loss， 然后根据 loss 对 ROI 进行排序，并选择 hard RoIs， 让 那些 non-RoIs的损失变为0. 这种方法虽然很简单，但是非常不高效，因为还需要为所有的 RoIs 分配进行反向传播时需要的内存空间。</p><p>为了克服这个缺点，作者对下面的 Figure 1 进行改进， 如下面的 Figure 2.该改进时使用两份同样的 RoI network。 其中一个是只读的(readonly), 即只进行前向计算，不进行反向传播优化，所以只需要为前向传播分配内存，它的参数实时保持和另一个 RoI network（regular RoI network)保持一样。在每次迭代时，首先使用 readonly RoI network 对每个 ROI 计算起 loss，然后用上面描述的选择 hard RoIs 的方法选择 hard RoIs. 然后利用 regular RoI network来对选择的 hard RoIs 进行前向和后向计算来优化网络。</p><p><img src="\images\imbalance_01.png" alt=""><br><img src="\images\imbalance_02.png" alt=""></p><h1 id="3-Focal-Loss"><a href="#3-Focal-Loss" class="headerlink" title="3. Focal Loss"></a>3. Focal Loss</h1><p>论文题目 <a href="https://arxiv.org/pdf/1708.02002.pdf" target="_blank" rel="noopener">Focal Loss for Dense Object Detection</a></p><h2 id="3-1-Cross-Entropy"><a href="#3-1-Cross-Entropy" class="headerlink" title="3.1 Cross Entropy"></a>3.1 Cross Entropy</h2><p>在改论文中，作者认为样本类别的不平衡可以归结为难易样本的不平衡，从而更改交叉熵损失函数，重新对样本赋予不同的权值，之前的模型这些样本都是同等重要的，从而使模型训练更加关注 hard examples。</p><p>首先引入交叉熵的公式:</p><script type="math/tex; mode=display">CE(p, y)=\begin{cases}    -\log (p), \quad if  \quad y == 1 \\    -\log(1-p), \quad otherwise\end{cases}\tag{公式1}</script><p>其中，$y\in\{-1, 1\}$,表示真实类别， $p\in[0,1]$表示我们预测的概率，为了方便，我们定义:</p><script type="math/tex; mode=display">p_t=\begin{cases}    p, \quad if  \quad y == 1 \\    1-p, \quad otherwise\end{cases}\tag{公式2}</script><p>因此， $CE(p,y)=CE(p_t)=-\log(p_t)$,该方法在 $p_t$ 较大时，该loss是一个较小的量级， 如下图的连线所示所示，因为存在大量的易分类样本，相加后会淹没正样本的loss。</p><p><img src="\images\imbalance_03.png" alt=""></p><h2 id="3-2-Balanced-Cross-Entropy"><a href="#3-2-Balanced-Cross-Entropy" class="headerlink" title="3.2 Balanced Cross Entropy"></a>3.2 Balanced Cross Entropy</h2><p>一个常见的解决类别不平衡的方式是引入一个加权因子 $\alpha \in [0,1]$ 来表示正样本的权重， $1-\alpha$表示负样本的权重。我们按照定义 $p_t$ 的方法重新定义$\alpha$为 $\alpha_t$,$\alpha-balance\quad CE \quad loss$ 定义如下:</p><script type="math/tex; mode=display">CE(p,y)=CE(p_t)=-\alpha_t\log(p_t)\tag{公式3}</script><h2 id="3-3-Focal-Loss"><a href="#3-3-Focal-Loss" class="headerlink" title="3.3 Focal Loss"></a>3.3 Focal Loss</h2><p>$\alpha-balance\quad CE \quad loss$ 虽然可以平衡 positive和negative的重要性，但是对 easy/hard 样本还是无法区分， Focal loss 通过更 Cross loss来达到区分easy/hard的目的:</p><script type="math/tex; mode=display">FL(p_t)=-(1-p_t)^\gamma\log(p_t)\tag{公式4}</script><p>上图展示了不同 $\gamma$ 取值对应的 loss，通过分析上述公式，我们发现，当 $p_t$ 非常小时，即样本被分类错误，此时 $(1-p_t)^\gamma$ 接近1， loss几乎不受影响，当 $p_t$ 接近于1时，即样本被分类正确，此时 $(1-p_t)^\gamma$ 接近0，此时降低了该样本的权重，比如，取 $\gamma=2$, 当时 $p_t==0.9$时，该样本的 loss 会降低100倍，</p><p>在实际应用中，作者使用了该 Focal loss的变体，即加入了 $\alpha$ 平衡因子:</p><script type="math/tex; mode=display">FL(p_t)=-\alpha_t(1-p_t)^\gamma\log(p_t)\tag{公式5}</script><h2 id="3-4-模型初始化"><a href="#3-4-模型初始化" class="headerlink" title="3.4 模型初始化"></a>3.4 模型初始化</h2><p>作者提出，对于二分类，一般模型的模型初始化会同概率的对待正负样本，由于类别不平衡，负样本的 loss 会占主导，作者提出在训练初始阶段对正样本的概率估计“prior”的概念，用 $\pi$ 表示， 通过设置它来达到正样本的输出概率低的效果，比如为0.01，从而使模型更加关注正样本。实际操作中，出了最后一个用于分类的卷积层，其余卷积层的参数初始化为bias $b=0$ ，而最后一层 $b=-\log((1-\pi)/\pi)$, 实验中设置为 0.01.</p><p><strong>两个实现细节</strong></p><ol><li><p>计算 total loss 时，是对一张图片上所有的 ~100k 的anchors 的 focal loss求和，而并不像使用启发式的 RPN 和 OHEM 使用部分小的anchors来计算 loss。 然后通过拥有 gt box 的anchors的数量来进行归一化，不适用所有 anchors 的数量进行归一化的原因是: 由于anchors 存在大量的 easy examples， 因此得到的 Focal loss非常小，如果使用所有 anchors 的数量进行归一化，回导致归一化后的 loss 非常小。</p></li><li><p>当只使用$\alpha$ 时， 将 $\alpha$ 偏向样本少的类别，当同时使用 $\alpha 和 \gamma$ 时，他们需要向相反方向变化，论文中设置为 $\alpha=0.25，\gamma=2$.</p></li></ol><h1 id="4-GHM-Gradient-Harmonizing-Mechanism"><a href="#4-GHM-Gradient-Harmonizing-Mechanism" class="headerlink" title="4.GHM(Gradient Harmonizing Mechanism)"></a>4.GHM(Gradient Harmonizing Mechanism)</h1><p>论文题目: <a href="https://arxiv.org/pdf/1811.05181.pdf" target="_blank" rel="noopener">Gradient Harmonized Single-stage Detector</a></p><p>改论文提出 Focal Loss 存在两个缺点:</p><ol><li>loss 存在两个超参数，调节该参数是一个费力的操作；</li><li>该 loss 是一个不会随着数据分布变化而变化的静态 loss。</li></ol><p>该论文有一句概括该篇论文的核心思想的一句话: 类别的不平衡可以归结为难易样本的不平衡，难易样本的不平衡可以归结为梯度的不平衡原话如下:</p><blockquote><p>In this work, we first point out that the class imbalance can be summarized to the imbalance in difficulty and the imbalance in difficulty can be summarized to the imbalance in gradient norm distribution.</p></blockquote><p>如下图所示:</p><p><img src="\images\imbalance_04.png" alt=""></p><p>左边是样本数量关于梯度的分布，中间表示各个修正后的函数的梯度（使用了log scale）对原始梯度的，右边表示所有样本集的梯度贡献的分布。</p><h2 id="4-1-Cross-Entropy"><a href="#4-1-Cross-Entropy" class="headerlink" title="4.1 Cross Entropy"></a>4.1 Cross Entropy</h2><script type="math/tex; mode=display">L_{CE}(p, p^*)=\begin{cases}    -\log (p), \quad if  \quad p^* == 1 \\    -\log(1-p), \quad if  \quad p^* == 0\end{cases}\tag{公式6}</script><p>定义 $x$ 是模型未经过 sigmoid 之前的输出， 则 $p = sigmoid(x)$,得出该损失函数对 $x$ 的梯度为:</p><script type="math/tex; mode=display">\begin{aligned}    \frac{\partial L_{CE}(p, p^*)}{\partial x} &=\begin{cases}    p - 1, \quad if  \quad p^* == 1 \\    p, \quad if  \quad p^* == 0    \end{cases} \\    &= p - p^*\end{aligned}\tag{公式7}</script><p>定义梯度的模长(norm) $g$ 为：</p><script type="math/tex; mode=display">g=|p-p^*|=\begin{cases}    1-p, \quad if  \quad p^* == 1 \\    p, \quad if  \quad p^* == 0\end{cases}\tag{公式6}</script><h2 id="4-2-梯度密度-Gradient-Density"><a href="#4-2-梯度密度-Gradient-Density" class="headerlink" title="4.2 梯度密度(Gradient Density)"></a>4.2 梯度密度(Gradient Density)</h2><p>训练样本的梯度密度(Gradient Density)定义如下:</p><script type="math/tex; mode=display">GD(g)=\frac{1}{l_\epsilon}\sum_{k=1}^{N}\delta_{\epsilon}(g_k, g)\tag{公式7}</script><p>其中， $g_k$ 是第 k 个样本的gradient norm， </p><script type="math/tex; mode=display">\delta_{\epsilon}(x, y)=\begin{cases}    1, \quad if  \quad y-\frac{\epsilon}{2} \le x < y + \frac{\epsilon}{2} \\    0, \quad if  \quad otherwise\end{cases}\tag{公式8}</script><script type="math/tex; mode=display">l_{\epsilon}(g) = min(g+\frac{\epsilon}{2}, 1) - max(g-\frac{\epsilon}{2}, 0)\tag{公式9}</script><p>这个公式可以理解为，以梯度 $g$ 为中心，宽度为$\epsilon$ 的区域内的样本密度。</p><p>梯度密度协调参数:</p><script type="math/tex; mode=display">\beta_i=\frac{N}{GD(g_i)} = \frac{1}{GD(g_1)/N}\tag{公式10}</script><p> 分母是对梯度位于 $g_i$ 范围的部分样本进行归一化，如果所有样本的梯度时均分分布，那么对于任意 $g_i$ 都有 $GD(g_i)=N$.(这里不是很理解为什么N，可以理解它们相等)</p><p>通过将梯度密度协调参数将 GHM 嵌入到损失函数中，则 <strong>GHM-C Loss</strong>  为:</p><script type="math/tex; mode=display">\begin{aligned}    L_{GHM-C}&=\frac{1}{N}\sum_{i=1}^{N}\beta_iL_{CE}(p_i, p_i^*) \\            &=\sum_{i=1}^{N}\frac{L_{CE}(p_i, p_i^*)}{GD(g_i)}\end{aligned}\tag{公式11}</script><h2 id="4-3-Unit-Region-Approximation"><a href="#4-3-Unit-Region-Approximation" class="headerlink" title="4.3 Unit Region Approximation"></a>4.3 Unit Region Approximation</h2><p>计算公式11时，求和有一个N，再求 $GD(g_i)$ 时会遍历所有的样本，因此该公式的时间复杂度为 $O(N^2)$.如果并行的化，每个计算单元也有N的计算量。对gradient norm进行排序的最好的算法复杂度为 $O(N\log N)$，然后用一个队列去扫描样本得到梯度密度的时间复杂度为 n 。基于排序的方法即使并行也不能较快的计算，因为N往往是 $10^5$ 甚至 $10^6$ ，仍然是非常耗时的.</p><p>作者提出的近似求解的方法如下:</p><ol><li>将 g 划分为 $M=\frac{1}{\epsilon}$ 个单元</li><li>$r_j=[(j-1)\epsilon, j\epsilon]$ 表示第 j 个单元， $R_j$ 表示 $r_j$ 中的样本数量， $ind(g)=t$ 表示 $g$ 属于第几个 unit。</li></ol><p>根据上述定义，得出近似梯度密度函数为:</p><script type="math/tex; mode=display">\hat {GD}(g)=\frac{R_{ind(g)}}{\epsilon}=ind(g)m\tag{公式12}</script><script type="math/tex; mode=display">\hat {\beta_i}=\frac{N}{\hat {GD}(g_i)}\tag{公式13}</script><script type="math/tex; mode=display">\begin{aligned}    \hat L_{GHM-C}&=\frac{1}{N}\sum_{i=1}^{N}\hat \beta_iL_{CE}(p_i, p_i^*) \\            &=\sum_{i=1}^{N}\frac{L_{CE}(p_i, p_i^*)}{\hat {GD}(g_i)}\end{aligned}\tag{公式14}</script><p>利用上面的公式，由于我们可以事先求好 $\hat \beta$, 在求和时只需查找$\hat \beta_i$即可，因此时间复杂度为 $O(MN)$.</p><p>因为loss的计算是基于梯度密度函数，而梯度密度函数根据一个batch中的数据得到，一个batch的统计结果是有噪声的。与batch normalization相同，作者用Exponential moving average来解决这个问题，也就是</p><script type="math/tex; mode=display">\begin{aligned}    S_{j}^{(t)} &= \alpha S_{j}^{(t-1)} + (1-\alpha) R_J^{(t)} \\    \hat GD(g) &= \frac{S_{ind(g)}}{\epsilon} = S_{ind(g)}M\end{aligned}\tag{公式15}</script><h2 id="4-4-GHM-R-Loss"><a href="#4-4-GHM-R-Loss" class="headerlink" title="4.4 GHM-R Loss"></a>4.4 GHM-R Loss</h2><p>将模型鱼的的偏移量定义为 $t=(t_x, t_y, t_w, t_h)$, 将真实的偏移量定义为 <script type="math/tex">t^*=(t_x^*, t_y^*, t_w^*, t_h^*)</script>,回归loss采用 Smooth L1 loss:</p><script type="math/tex; mode=display">L_{reg}=\sum_{i\in{x, y, w, h}}SL_1(t_i-t_i^*)\tag{公式16}</script><p>其中 </p><script type="math/tex; mode=display">SL_1(d)=\begin{cases}    \frac{d_2}{2\delta} \quad if |d|\le\delta \\    |d|-\frac{\delta}{2} \quad otherwise\end{cases}\tag{公式17}</script><p>则 $L_{reg}$ 关于 $d=t_i-t_i^*$ 的梯度为:</p><script type="math/tex; mode=display">\frac{\partial SL_1}{\partial t_i}=\frac{\partial SL_1}{\partial d}\begin{cases}    \frac{d}{\delta} \quad if |d| \le \delta \\    sgn(d) \quad otherwise\end{cases}\tag{18}</script><p>从公式可以看出，当样本操作 $|d|$ 时， 所有样本都有相同的梯度 1， 这就使依赖梯度范数来区分不同样本是不可能的， 一种简单的替代方法时直接使用 $|d|$ 作为衡量标准，但是该值理论上无限大，导致 Unit Region Approximation 无法实现，</p><p>为了将 GHM 应用到回归损失上，作者修改了原始的 $SL_1$ 损失函数:</p><script type="math/tex; mode=display">ASL_1(d) = \sqrt{d_2+u^2}-u\tag{19}</script><p>该函数和 $SL_1$ 具有类似的属性，当d的绝对值很小时，近似 L2 loss， 当d的绝对值比较大时， 近似 L1 loss， $ASL_1$ 关于d的梯度为:</p><script type="math/tex; mode=display">\frac{\partial ASL_1}{\partial d} = \frac{d}{\sqrt{d^2+u^2}}\tag{20}</script><p>这样就将梯度值限制在 $[0,1)$</p><p>定义 $dr=|\frac{d}{\sqrt{d^2+u^2}}|$, 则 GHM-R Loss 为:</p><script type="math/tex; mode=display">\begin{aligned}    L_{GHM-R}&=\frac{1}{N}\sum_{i=1}^N\beta_i ASL_1(d_i) \\            &=\sum_{i=1}^N\frac{ASL_1(d_i)}{GD(gr_i)}\end{aligned}\tag{21}</script><h1 id="5-PISA"><a href="#5-PISA" class="headerlink" title="5.PISA"></a>5.PISA</h1><p>论文题目: <a href="https://arxiv.org/pdf/1904.04821.pdf" target="_blank" rel="noopener">Prime Sample Attention in Object Detection</a></p><p>PISA 方法和 Focal loss 和 GHM 有着不同， Focal loss 和 GHM 是利用 loss 来度量样本的难以程度，而本篇论文作者从 mAP 出法来度量样本的难易程度。</p><p>作者提出提出改论文的方法考虑了两个方面:</p><ol><li><p><strong>样本之间不应该是相互独立的或同等对待</strong>。基于区域的目标检测是从大量候选框中选取一小部分边界框，以覆盖图像中的所有目标。因此，不同样本的选择是相互竞争的，而不是独立的。一般来说，检测器更可取的做法是在确保所有感兴趣的目标都被充分覆盖时，在每个目标周围的边界框产生高分，而不是对所有正样本产生高分。作者研究表明关注那些与gt目标有最高IOU的样本是实现这一目标的有效方法。</p></li><li><p><strong>目标的分类和定位是有联系的</strong>。准确定位目标周围的样本非常重要，这一观察具有深刻的意义，即目标的分类和定位密切相关。具体地，定位好的样本需要具有高置信度好的分类。</p></li></ol><h2 id="5-1-Prime-Samples"><a href="#5-1-Prime-Samples" class="headerlink" title="5.1 Prime Samples"></a>5.1 Prime Samples</h2><p>Prime Samples 是指那些对检测性能有着巨大影响的样本。作者研究表明样本的重要程度依赖于它和ground truth 的 IoU值，因此作者提出了一种 IOU-HLR 排序。</p><p>在目标检测中时如何定义正样本（True Positive)的呢？</p><ol><li>Bouding box 和他最近的 GT 的IoU 大于一个阈值 $\theta$；</li><li>如果该 GT 没有大于阈值的 Bouding Box，则选择最大的 IoU。</li></ol><p>剩余的标注为负样本。</p><p>mAP 的原理揭露了对目标检测器更重要的<strong>两个准则</strong>:</p><ol><li><p>在所有和gt目标重合的边界框中，IoU最高的边界框时最重要的，因为它的IoU值直接影响召回率。</p></li><li><p>所有不同目标的最高IoU边界框中,具有更高的IoU的边界框更加重要,因为它是随着 $\theta$ 增加最后一个低于阈值 $\theta$ 的边界框,从而对整体精度有很大的影响。</p></li></ol><h2 id="5-2-IoU-Hierarchical-Local-Rank-IoU-HLR"><a href="#5-2-IoU-Hierarchical-Local-Rank-IoU-HLR" class="headerlink" title="5.2 IoU Hierarchical Local Rank (IoU-HLR)"></a>5.2 IoU Hierarchical Local Rank (IoU-HLR)</h2><p>基于上述分析，作者提出了一种称为 IoU-HLR 的排序方法，它既反映了局部的IoU关系(每个ground truth目标周围)，也反映了全局的IoU关系(覆盖整个图像或小批图像)。值得注意的是，不同于回归前的边界框坐标，IoU-HLR是根据样本的最终定位位置来计算的，因为mAP是根据回归后的样本位置来计算的。</p><p>该排序方法的大致流程如下图所示，其原理如下:</p><ol><li>首先将所有的样本根据理他最近的 GT ，将他们分到不同的组；</li><li>然后在每个组内，按照 IoU对它们进行降序排列，我们得到 IoU Local Rank（IoU-LR);</li><li>我们对 IoU-LR,进行采样，即将每个组内的TOP1的样本采样出来放在一个组，以此类推对 TOP2, TOP3，一次进行；</li><li>最后对上面采样的样本，在每个组内进行降序排序，就得到最终的 IoU-HLR.</li></ol><p>IoU-HLR遵循上述两个准则。首先，它通过局部排序（即上面的步骤2）将每个单独的 GT 的 对应的样本中 IoU 较高的样本放在前面，其次通过重采样和排序(步骤3， 4)将不同 GT 的 对应的样本中， 将 IoU 较高的放在了前面。<br><img src="\images\imbalance_05.png" alt=""></p><h2 id="5-3-Learn-Detectors-via-Prime-Sample-Attention"><a href="#5-3-Learn-Detectors-via-Prime-Sample-Attention" class="headerlink" title="5.3 Learn Detectors via Prime Sample Attention"></a>5.3 Learn Detectors via Prime Sample Attention</h2><p>作者提出Prime Sample Attention，一种简单且有效的采样策略，该采样策略将更多的注意力集中到 Prime examples 上， PISA 由两部分组成: Importance- based Sample Reweighting(ISR)和Classification Aware Regression Loss(为CARL).</p><p>PISA 的训练过程是基于 prime samples 而不是同等对待所有样本。</p><ol><li>首先 prime samples 的权重要大于其他样本的权重， 所以分类往往在这些样本上预测更高的分数；</li><li>用一个联合目标函数来对分类器和回归其进行学习，因此prime sample的分数相对于不重要样本的分数将会得到提高。</li></ol><h3 id="5-3-1-Importance-based-Sample-Reweighting"><a href="#5-3-1-Importance-based-Sample-Reweighting" class="headerlink" title="5.3.1 Importance-based Sample Reweighting"></a>5.3.1 Importance-based Sample Reweighting</h3><p>作者提出一种基于 soft sampling 的方法: Importance-based Sample Reweighting (ISR), 他给不同样本根据重要性赋予不同的权重。首先它将Iou-HLR排序转化为线性映射的真实值。 IoU-HLR在每个类中分别进行计算。对于类 $j$, 假设总共有 $n_j$ 个样本， 通过 IoU-HLR 表示为 $\{r_1,r_2,\cdots,r_{n_j} \}$. 其中 $0 \le r_i \le n_j -1$,使用一个线性转换函数将 $r_i$ 转换为 $u_i$, 表示第 $j$ 类中的第 $i$ 个样本的重要程度:</p><script type="math/tex; mode=display">u_i = \frac{n_j-r_i}{n_j}\tag{22}</script><p>采用指数函数的形式来京一部将样本重要性 $u_i$ 转换为 loss 的权值 $w_i$, $\gamma$ 表示对重要样本给予多大的优先权的程度因子， $\beta$ 决定最小样本权值的偏差(感觉就是一个决定最小的权值大小的一个变量)。</p><script type="math/tex; mode=display">w_i=((1-\beta)u_i + \beta)^\gamma\tag{23}</script><p>根据上面得到的权重值，重写交叉熵:</p><script type="math/tex; mode=display">\begin{aligned}    L_{cls} &= \sum_{i=1}^{n}w_i^{'}CE(s_i, s_i^*) + \sum_{i=n+1}^{m}CE(s_i, s_i^*) \\    w_i^{'} &= w_i \frac{\sum_{i=1}^{n}CE(s_i, s_i^*)}{\sum_{i=1}^{n}w_iCE(s_i, s_i^*)}\end{aligned}\tag{24}</script><p>其中 n 和 m 分别表示真样本和负样本的数量，  $s_i$ 和 $s_i^*$ 分别表示预测分数和分类目标，需要注意的是，如果只是简单的添加 loss 权值将会改变 loss 的值，并改变正负样本的比例，因此为了保持正样本的总的 loss 值不变， 作者将 $w_i$ 归一化为 $w_i^{‘}$<br>(这里不是很理解，欢迎大家解惑)</p><h3 id="5-3-2-Classification-Aware-Regression-Loss"><a href="#5-3-2-Classification-Aware-Regression-Loss" class="headerlink" title="5.3.2 Classification-Aware Regression Loss"></a>5.3.2 Classification-Aware Regression Loss</h3><p>5.3.1 已经介绍如何染个分类器知道 prime samples， 那么如何让回归其也知道 prime sample，作者提出了 <strong>Classification-Aware Regression Loss（CARL)</strong> 来联合优化分类器和回归其两个分支。CARL可以提升主要样本的分数，同时抑制其他样本的分数。回归质量决定了样本的重要性，我们期望分类器对重要样本输出更高的分数。两个分支的优化应该是相互关联的，而不是相互独立的。</p><p>作者的方法是让回归器知道分类器的分数，这样梯度就可以从回归器传播到分期其分支。公式如下:</p><script type="math/tex; mode=display">\begin{aligned}    L_{reg} &=\sum_{i=1}^n c_i^{'}\mathcal{L}(d_i, \hat d_i) \\    c_i^{'} &= c_i \frac{\sum_{i=1}^{n}\mathcal{L}(s_i, s_i^*)}{\sum_{i=1}^{n}c_i\mathcal{L}(s_i, s_i^*)} \\    c_i &= \frac{v_i}{\frac{1}{n}\sum_{i=1}^{n}v_i} \\    v_i &= ((1-b)p_i +b)^k\end{aligned}\tag{25}</script><p>$p_i$ 表示相应类别的预测分数， $d_i$ 表示输出的回归偏移量。利用一个指数函数将 $p_i$ 转化为 $v_i$ ，随后根据所有样本的平均值对它进行缩放。为了保持损失规模不变，对具有分类感知的 $c_i$ 进行归一化。 $\mathcal{L}$ 是常用的smooth L1 loss。</p><p>$L_{reg}$ 关于 $c_i^{‘}$ 的梯度与原回归损失 $\mathcal{L}(d_i, \hat d_i)$ 成正比。 $L_{reg}$ 关于 $p_i$ 的梯度与 $\mathcal{L}(d_i, \hat d_i)$ 正相关。即回归损失越大的样本分类得分的梯度越大，说明对分类得分的抑制作用越强。从另一个角度看， $\mathcal{L}(d_i, \hat d_i)$ 反映了样本i的定位质量，因此可以认为是一个IoU的估计，进一步可以看作是一个IoU-HLR的估计。可以近似认为，排序靠前的样本有较低的回归损失，于是分类得分的梯度较小。对于CARL来说，分类分支受到回归损失的监督。 不重要样本的得分被极大的抑制掉，而对重要样本的关注得到加强。</p><h1 id="6-AP-Loss"><a href="#6-AP-Loss" class="headerlink" title="6.AP-Loss"></a>6.AP-Loss</h1><p>待续。。。</p><h1 id="7-总结"><a href="#7-总结" class="headerlink" title="7.总结"></a>7.总结</h1><p>上面的方法大致可以分为两种：</p><ol><li>Hard Sampling: 从所有样本中选择自己来训练模型: hard negative mining, OHEM</li><li>Soft sampling： 为样本赋予不同的权值: Focal Loss, GHM, PISA</li></ol><p>Focal Loss认为正负样本的不平衡，本质上是因为难易样本的不平衡，于是通过修改交叉熵，使得训练过程更加关注那些困难样本，而GHM在Focal Loss的基础上继续研究，发现难易样本的不平衡本质上是因为梯度范数分布的不平衡，和Focal Loss的最大区别是GHM认为最困难的那些样本应当认为是异常样本，让检测器强行去拟合异常样本对训练过程是没有帮助的。PISA则是跳出了Focal Loss的思路，认为采样策略应当从mAP这个指标出发，通过IoU Hierarchical Local Rank (IoU-HLR)，对样本进行排序并权值重标定，从而使得recall和precision都能够提升。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2019/07/20/imbalance-between-easy-and-hard-examples/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;&lt;br&gt;
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>感知机(Perceptron)</title>
    <link href="http://yoursite.com/2019/06/04/ML-perceptron/"/>
    <id>http://yoursite.com/2019/06/04/ML-perceptron/</id>
    <published>2019-06-04T07:40:38.000Z</published>
    <updated>2019-06-04T09:11:30.064Z</updated>
    
    <content type="html"><![CDATA[<p>感知机(Perceptron)是一种二分类的线性分类模型，输入为实例的特征向量，输出为实例的类别，取+1和-1值。感知机对应于输入空间中将实例划分为正负两类的分离超平面，属于<strong>判别模型</strong>，感知机学习旨在求出将训练数据进行线性划分的分离超平面，基于五分类的损失函数，利用梯度下降法对损失函数进行极小化，求得模型参数。该模型有原始形式和对偶形式两种形式。</p><h1 id="1-感知机模型"><a href="#1-感知机模型" class="headerlink" title="1. 感知机模型"></a>1. 感知机模型</h1><p>假设输入空间（特征空间）是 $\chi \subseteq R^n$,输出空间时 $Y=\{+1, -1\}$,输入 $x \in \chi$ 表示实例的特征向量，对应于输入空间（特征空间）的点，输出 $y \in Y$ 表示实例的类别，由输入空间到输出空间的函数:</p><script type="math/tex; mode=display">f(x)=sign(w\cdot x+b)\tag{公式1}</script><p>称为感知机，其中w和b为感知机模型参数， $w\in R^n$ 叫做权值(weight)或权值向量(weight vector)， $b\in R$ 叫做偏置(bias), $w \cdot x$ 表示 $w$ 和 $x$ 的内积，sign是符号函数</p><script type="math/tex; mode=display">sign(x)=\begin{cases}    +1, x \geq 0 \\    -1, \quad x \le 0\end{cases}\tag{公式2}</script><p>假设训练数据集是线性可分的，感知机学习的目标是求得一个能够将训练数据集正实例点和负实例点完全正确分开的分离超平面 $S$。关于如何得到 $w$ 和 $b$ 的值，是通过损失函数来求得的。损失函数一般要求损失函数是参数 $w,b$ 的连续可导的函数，因此如果使用误分类点的数目作为损失函数显然不利于参数的优化。在感知机中，我们所选择的损失函数是误分类点到分离超平面的 $S$ 的距离。关于空间中一点 $x_0$ 到分离超平面的距离为:</p><script type="math/tex; mode=display">\frac{1}{||w||}|w\cdot x_0 +b|\tag{公式3}</script><p>若$x_0$是正实例点时，被误分类负实例点，则此时 $w\cdot x_0 +b <0, y_0="1$," 若$x_0$是负实例点时，被误分类正实例点，则此时="" $w\cdot="" x_0="" +b="">0, y_0=-1$, 因此 $y_0(w\cdot x_0 +b)&lt;0$,</0,></p><p>因此，对于误分类点的数据$(x_i,y_i)$,该点到分离超平面$S$的距离可以表示为:</p><script type="math/tex; mode=display">-\frac{1}{||w||}y_i(w \cdot x_i + b)\tag{公式4}</script><p>因此，所有误分类点到分离超平面的距离和为:</p><script type="math/tex; mode=display">-\frac{1}{||w||}\sum_{x_i\in M}y_i(w \cdot x_i + b)\tag{公式5}</script><p>其中， $M$ 为误分类点的集合。</p><p>在线性函数中，我们同时扩大或缩小 $w,b$,函数不改变，因此我们可以使 $||w||=1$.因此，感知机的损失函数为(经验风险函数):</p><script type="math/tex; mode=display">-\sum_{x_i\in M}y_i(w \cdot x_i + b)\tag{公式6}</script><h1 id="2-感知机模型的优化"><a href="#2-感知机模型的优化" class="headerlink" title="2. 感知机模型的优化"></a>2. 感知机模型的优化</h1><h2 id="2-1-原始形式"><a href="#2-1-原始形式" class="headerlink" title="2.1 原始形式"></a>2.1 原始形式</h2><p>感知机学习算法是误分类驱动的，采用随机梯度下降法。对于固定的误分类集合 $M$,那么损失函数的梯度为</p><script type="math/tex; mode=display">\begin{aligned}    \nabla_w L(w, b) &=-\sum_{x_i \in M}y_ix_i \\    \nabla_b L(w, b) &=-\sum_{x_i \in M}y_i \end{aligned}\tag{公式7}</script><p>若随机选取一个误分类点$(x_i, y_i)$,对$w,b$ 进行更新:</p><script type="math/tex; mode=display">\begin{aligned}    w &\gets w+\eta y_ix_i \\    b &\gets b+\eta y_i\end{aligned}\tag{公式8}</script><p>其中$\eta(0&lt;\eta \leq 1)$是步长或学习率(learning rate)</p><p><strong>感知机学习算法的原始形式</strong></p><p>输入:训练数据集$T=\{(x_1,y_1), (x_2,y_2),\dots,(x_N,y_N) \}$,学习率为$\eta(0&lt;\eta \leq 1)$</p><p>输出:w,b;感知机模型$f(x)=sign(w\cdot x+b)$</p><p>(1) 选取初始值 $w_0, b_0$</p><p>(2) 在训练集中选取数据$(x_i, y_i)$</p><p>(3) 如果 $y_i(w\cdot x_i +b) \leq 0$</p><script type="math/tex; mode=display">\begin{aligned}    w &\gets w+\eta y_ix_i \\    b &\gets b+\eta y_i\end{aligned}</script><p>(4) 转至 (2),直至训练集中没有误分类点</p><h2 id="2-2-对偶形式"><a href="#2-2-对偶形式" class="headerlink" title="2.2 对偶形式"></a>2.2 对偶形式</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;感知机(Perceptron)是一种二分类的线性分类模型，输入为实例的特征向量，输出为实例的类别，取+1和-1值。感知机对应于输入空间中将实例划分为正负两类的分离超平面，属于&lt;strong&gt;判别模型&lt;/strong&gt;，感知机学习旨在求出将训练数据进行线性划分的分离超平面，基
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>GBDT算法原理以及实例理解</title>
    <link href="http://yoursite.com/2019/04/27/ML-GBDT/"/>
    <id>http://yoursite.com/2019/04/27/ML-GBDT/</id>
    <published>2019-04-27T05:54:40.000Z</published>
    <updated>2019-05-28T07:15:49.425Z</updated>
    
    <content type="html"><![CDATA[<p>该内容完全转载自<a href="http://blog.csdn.net/zpalyq110/article/details/79527653" target="_blank" rel="noopener">同名CSDN博客</a>(<a href="http://blog.csdn.net/zpalyq110/article/details/79527653" target="_blank" rel="noopener">http://blog.csdn.net/zpalyq110/article/details/79527653</a>)</p><p><a href="https://ranmaosong.github.io/2019/04/27/ML-GBDT/" target="_blank" rel="noopener">GitHub</a></p><p><a href="https://www.jianshu.com/p/dcc5124fa35a" target="_blank" rel="noopener">简书</a></p><p><a href="https://blog.csdn.net/u014630987/article/details/89602217" target="_blank" rel="noopener">CSDN</a></p><p><strong>写在前面：</strong> 去年学习GBDT之初，为了加强对算法的理解，整理了一篇笔记形式的文章，发出去之后发现阅读量越来越多，渐渐也有了评论，评论中大多指出来了笔者理解或者编辑的错误，故重新编辑一版文章，内容更加翔实，并且在GitHub上实现了和本文一致的GBDT简易版（包括回归、二分类、多分类以及可视化），供大家交流探讨。感谢各位的点赞和评论，希望继续指出错误</p><p><strong>Github：</strong><br><strong><a href="https://github.com/Freemanzxp/GBDT_Simple_Tutorial" target="_blank" rel="noopener">https://github.com/Freemanzxp/GBDT_Simple_Tutorial</a></strong></p><p><strong>简介：</strong><br>GBDT 的全称是 Gradient Boosting Decision Tree，梯度提升树，在传统机器学习算法中，GBDT算的上TOP3的算法。想要理解GBDT的真正意义，那就必须理解GBDT中的Gradient Boosting 和Decision Tree分别是什么？</p><h1 id="1-Decision-Tree：CART回归树"><a href="#1-Decision-Tree：CART回归树" class="headerlink" title="1. Decision Tree：CART回归树"></a>1. Decision Tree：CART回归树</h1><p>首先，GBDT使用的决策树是CART回归树，无论是处理回归问题还是二分类以及多分类，GBDT使用的决策树通通都是都是CART回归树。为什么不用CART分类树呢？因为GBDT每次迭代要拟合的是<strong>梯度值</strong>，是连续值所以要用回归树。</p><p>  对于回归树算法来说最重要的是寻找最佳的划分点，那么回归树中的可划分点包含了所有特征的所有可取的值。在分类树中最佳划分点的判别标准是熵或者基尼系数，都是用纯度来衡量的，但是在回归树中的样本标签是连续数值，所以再使用熵之类的指标不再合适，取而代之的是平方误差，它能很好的评判拟合程度。</p><p><strong>回归树生成算法:</strong><br>输入: 训练数据集 $D$<br>输出: 回归树 $f(x)$<br>在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树：</p><p>(1) 选择最优切分变量 $j$ 与切分点 $s$， 求解:</p><script type="math/tex; mode=display">\min_{j,s}[\min_{c_1}\sum_{x_i \in R_1(j, s)} (y_i-c_1)^2+\min_{c_2}\sum_{x_i \in R_2(j, s)} (y_i-c_2)^2]</script><p>遍历变量 $j$，对固定的切分变量j扫描切分点 $s$，选择使得上式达到最小值的对  $(j,s)$.<strong>简要解释一下上述公式</strong>：中括号里面的公式是求出每个特征变量在哪一个划分点时损失函数最小，最外面的 $\min$ 是在所有特征值，求得使损失函数全局最小的特征及其切分点$(j^<em>, s^</em>)$;</p><p>(2) 用选定的对 $(j,s)$ 划分区域并决定相应的输出值：</p><script type="math/tex; mode=display">R_1(j, s)=\{x|x^{(j)}\leq s\},R_2(j, s)=\{x|x^{(j)} > s\}</script><script type="math/tex; mode=display">\hat {c_m}=\frac{1}{N}\sum_{x_1 \in R_m(j, s)}y_i, x \in R_m, m=1,2</script><p>求划分区域的输出值就是将该区域的所有样本的输出值求平均。</p><p>(3)继续对两个子区域调用步骤（1）和（2），直至满足停止条件。</p><p>(4)将输入空间划分为M个区域$R_1, R_2…R_M$，得到决策树</p><script type="math/tex; mode=display">f(x)=\sum_{m=1}^M \hat{c_m}I(x \in R_m)</script><h1 id="2-Gradient-Boosting：拟合负梯度"><a href="#2-Gradient-Boosting：拟合负梯度" class="headerlink" title="2. Gradient Boosting：拟合负梯度"></a>2. Gradient Boosting：拟合负梯度</h1><p>梯度提升树（Grandient Boosting）是提升树（Boosting Tree）的一种改进算法，所以在讲梯度提升树之前先来说一下提升树。</p><p>先来个通俗理解：假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。最后将每次拟合的岁数加起来便是模型输出的结果。</p><p><strong>提升树算法:</strong></p><p>(1) 初始化 $f_0(x)=0$</p><p>(2) 对$m=1, 2…M$</p><p>(a)计算残差</p><script type="math/tex; mode=display">r_{mi}=y_i-f_{m-1}(x_i), i=1, 2,...,M</script><p>(b) 拟合残差 $r_{mi}$ 学习一个回归树，得到 $h_m(x)$</p><p>(c) 更新 $f_m(x)=f_{m-1}(x)+h_m(x)$</p><p>(3)得到回归树</p><script type="math/tex; mode=display">f_{M}(x)=\sum_{m=1}^Mh_m(x)</script><p>上面伪代码中的残差是什么？</p><p>在提升树算法中，假设我们前一轮迭代得到的强学习器是</p><script type="math/tex; mode=display">f_{t-1}(x)</script><p>损失函数是</p><script type="math/tex; mode=display">L(y, f_{t-1}(x))</script><p>我们本轮迭代的目标是找到一个弱学习器</p><script type="math/tex; mode=display">h_{t}(x)</script><p>当采用平方损失函数时</p><script type="math/tex; mode=display">\begin{aligned}    & L(y, f_{t-1}(x)+h_t(x)) \\    & = (y - f_{t-1}(x) - h_t(x))^2 \\    & =(r - h_t(x))^2 \\\end{aligned}</script><p>这里，</p><script type="math/tex; mode=display">r = y - f_{t-1}(x)</script><p>是当前模型拟合数据的残差（residual）。所以，对于提升树来说只需要简单地拟合当前模型的残差。</p><p>回到我们上面讲的那个通俗易懂的例子中，第一次迭代的残差是10岁，第二次残差4岁…</p><p>当损失函数是平方损失和指数损失函数时，梯度提升树每一步优化是很简单的，但是对于一般损失函数而言，往往每一步优化起来不那么容易，针对这一问题，Freidman提出了梯度提升树算法，这是利用最速下降的近似方法，<strong>其关键是利用损失函数的负梯度作为提升树算法中的残差的近似值。</strong></p><p>那么负梯度长什么样呢？</p><p>第t轮的第i个样本的损失函数的负梯度为：</p><script type="math/tex; mode=display">-[\frac{\partial {L(y, f(x_i))}}{\partial {f(x_i)}}]_{f(x)=f_{t-1}(x)}</script><p>此时不同的损失函数将会得到不同的负梯度，如果选择平方损失</p><script type="math/tex; mode=display">L(y, f(x_i))=\frac{1}{2}(y-f(x_i))^2</script><p>负梯度为</p><script type="math/tex; mode=display">-[\frac{\partial {L(y, f(x_i))}}{\partial {f(x_i)}}]_{f(x)=f_{t-1}(x)}=-[\frac{\partial \frac{1}{2}(y-f(x_i))^2}{\partial {f(x_i)}}]_{f(x)=f_{t-1}(x)}=y-f(x_i)</script><p>此时我们发现GBDT的<strong>负梯度就是残差</strong>，所以说对于回归问题，我们要拟合的就是残差。<br> <br>那么对于分类问题呢？二分类和多分类的损失函数都是log loss，<strong>本文以回归问题为例进行讲解。</strong></p><h1 id="3-GBDT算法原理"><a href="#3-GBDT算法原理" class="headerlink" title="3. GBDT算法原理"></a>3. GBDT算法原理</h1><p> <br>上面两节分别将Decision Tree和Gradient Boosting介绍完了，下面将这两部分组合在一起就是我们的GBDT了。</p><p>GBDT算法：<br>（1）初始化弱学习器</p><script type="math/tex; mode=display">f_0(x)=\arg \min_{c}\sum_{i=1}^{N}L(y_i, c)</script><p>后面有证明，党委平方损失时，$f_0(x)=\frac{\sum_{i=1}^N y_i}{N}$</p><p>(2) 对m=1,2,…,M有：</p><p>（a）对每个样本i=1,2,…,N，计算负梯度，即残差</p><script type="math/tex; mode=display">r_{im}=-[\frac{\partial{L(y_i, f(x_i))}}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}</script><p>（b）将上步得到的残差作为样本新的真实值，并将数据 $(x_i, x_im), i=1, 2,…,N$ 作为下棵树的训练数据，得到一颗新的回归树 $f_m(x)$，其对应的叶子节点区域为 $R_jm, j=1, 2,…,J$。其中J为回归树t的叶子节点的个数。</p><p>（c）对叶子区域 $j =1,2,..J$ 计算最佳拟合值</p><script type="math/tex; mode=display">\gamma_{jm}=\arg \min_{\gamma}\sum_{x_i \in R_{jm}}L(y_i, f_{m-1}(x_i)+\gamma) (对 \gamma求导并令导数为0即可求得)</script><p>（d）更新强学习器</p><script type="math/tex; mode=display">f_m{x}=f_{m-1}(x)+\sum_{j=1}^{J}\gamma_{jm}I(x \in R_{jm})</script><p>(3)得到最终学习器</p><script type="math/tex; mode=display">f(x)=f_M{x}=f_{0}(x)+\sum_{m=1}^{M}\sum_{j=1}^{J}\gamma_{jm}I(x \in R_{jm})</script><h1 id="4-实例详解"><a href="#4-实例详解" class="headerlink" title="4. 实例详解"></a>4. 实例详解</h1><p><strong>==本人用python以及pandas库实现GBDT的简易版本，在下面的例子中用到的数据都在github可以找到，大家可以结合代码和下面的例子进行理解，欢迎star~== </strong></p><p><strong>Github：<a href="https://github.com/Freemanzxp/GBDT_Simple_Tutorial" target="_blank" rel="noopener">https://github.com/Freemanzxp/GBDT_Simple_Tutorial</a></strong></p><p><strong>数据介绍：</strong></p><p>如下表所示：一组数据，特征为年龄、体重，身高为标签值。共有5条数据，前四条为训练样本，最后一条为要预测的样本。</p><p><img src="\images\ml_gbdt_01.webp" alt=""></p><p><strong>训练阶段：</strong></p><p><strong>参数设置：</strong></p><ul><li><p>学习率：learning_rate=0.1</p></li><li><p>迭代次数：n_trees=5</p></li><li><p>树的深度：max_depth=3</p></li></ul><p><strong>1.初始化弱学习器:</strong></p><script type="math/tex; mode=display">f_0(x)=arg min_c\sum_{i=1}^N L(y_i, c)</script><p>损失函数为平方损失，因为平方损失函数是一个凸函数，直接求导，倒数等于零，得到 $c$。</p><script type="math/tex; mode=display">\sum_{i=1}^N\frac{\partial {L(y_i, c)}}{\partial c}=\sum_{i=1}^N \frac{\partial {\frac{1}{2}(y_i-c)^2}}{\partial c}=\sum_{i=1}^N(c - y_i)</script><p>令导数等于0</p><script type="math/tex; mode=display">\sum_{i=1}^N(c - y_i)=c - \sum_{i=1}^N y_i = 0</script><script type="math/tex; mode=display">c = (\sum_{i=1}^N y_i)/N</script><p>所以初始化时，$c$ 取值为所有训练样本标签值的均值。$c=(1.1+1.3+1.7+1.8)/4=1.475$，此时得到初始学习器 $f_0(x)$</p><script type="math/tex; mode=display">f_0(x)=c=1.475</script><p><strong>2.对迭代轮数m=1，2,…,M:</strong><br> <br>由于我们设置了迭代次数：n_trees=5，这里的M=5。</p><p>计算负梯度，根据上文损失函数为平方损失时，负梯度就是残差残差，再直白一点就是 y与上一轮得到的学习器 $f_{m-1}$ 的差值</p><script type="math/tex; mode=display">r_{i1}=-[\frac{\partial{L(y_i, f(x_i))}}{\partial f(x_i)}]_{f(x)=f_{0}(x)}</script><p>残差在下表列出</p><p><img src="\images\ml_gbdt_02.webp" alt=""></p><p>此时将残差作为样本的真实值来训练弱学习器 $f_1(x)$，即下表数据：</p><p><img src="\images\ml_gbdt_03.webp" alt=""></p><p>接着，寻找回归树的最佳划分节点，遍历每个特征的每个可能取值。从年龄特征的5开始，到体重特征的70结束，分别计算分裂后两组数据的平方损失（Square Error），$SE_l$左节点平方损失，$SE_r$ 右节点平方损失，找到使平方损失和 $SE_{sum}=SE_l+SE_r$ 最小的那个划分节点，即为最佳划分节点。</p><p>例如：以年龄7为划分节点，将小于7的样本划分为到左节点，大于等于7的样本划分为右节点。</p><p>左节点包括 $x_0$，右节点包括样本 $x_1,x_2,x_3$，$SE_l=0, SE_r=0.047, SE_{sum}=0.047$，所有可能划分情况如下表所示：<br><img src="\images\ml_gbdt_04.webp" alt=""></p><p>以上划分点是的总平方损失最小为0.025有两个划分点：年龄21和体重60，所以随机选一个作为划分点，这里我们选 年龄21<br> 现在我们的第一棵树长这个样子：</p><p><img src="\images\ml_gbdt_05.webp" alt=""></p><p>我们设置的参数中树的深度max_depth=3，现在树的深度只有2，需要再进行一次划分，这次划分要对左右两个节点分别进行划分：</p><p>对于<strong>左节点</strong>，只含有0,1两个样本，根据下表我们选择<strong>年龄7</strong>划分</p><p><img src="\images\ml_gbdt_06.webp" alt=""></p><p>对于<strong>右节点</strong>，只含有2,3两个样本，根据下表我们选择<strong>年龄30</strong>划分（也可以选<strong>体重70</strong>）</p><p><img src="\images\ml_gbdt_07.webp" alt=""></p><p>现在我们的第一棵树长这个样子：</p><p><img src="\images\ml_gbdt_08.webp" alt=""></p><p>此时我们的树深度满足了设置，还需要做一件事情，给这每个叶子节点分别赋一个参数Υ，来拟合残差。</p><script type="math/tex; mode=display">\gamma_{j1}=\arg \min_{\gamma}\sum_{x_i \in R_{j1}}L(y_i, f_{0}(x_i)+\gamma)</script><p>这里其实和上面初始化学习器是一个道理，平方损失求导，令导数等于零，化简之后得到每个叶子节点的参数 $Υ$，其实就是标签值的均值。这个地方的标签值不是原始的 $y$，而是本轮要拟合的标残差 $y-f_0(x)$。</p><p>根据上述划分结果，为了方便表示，规定从左到右为第1,2,3,4个叶子结点</p><p><img src="\images\ml_gbdt_09.webp" alt=""></p><p>此时的树长这个样子：</p><p><img src="\images\ml_gbdt_10.webp" alt=""></p><p>此时可更新强学习器，需要用到参数学习率：learning_rate=0.1，用lr表示。</p><script type="math/tex; mode=display">f_1(x)=f_0(x)+ lr * \sum_{j=1}^4\gamma_{j1}I(x \in R_{j1})</script><p>为什么要用学习率呢？这是<strong>Shrinkage</strong>的思想，如果每次都全部加上（学习率为1）很容易一步学到位导致过拟合。</p><p><strong>重复此步骤，直到 m&gt;5 结束，最后生成5棵树。</strong></p><p>下面将展示每棵树最终的结构，这些图都是GitHub上的代码生成的，感兴趣的同学可以去一探究竟</p><p><strong>Github：<a href="https://github.com/Freemanzxp/GBDT_Simple_Tutorial" target="_blank" rel="noopener">https://github.com/Freemanzxp/GBDT_Simple_Tutorial</a></strong><br><strong>第一棵树：</strong></p><p><img src="\images\ml_gbdt_11.webp" alt=""></p><p><strong>第二棵树：</strong><br><img src="\images\ml_gbdt_12.webp" alt=""></p><p><strong>第三棵树：</strong><br><img src="\images\ml_gbdt_13.webp" alt=""></p><p><strong>第四棵树：</strong><br><img src="\images\ml_gbdt_14.webp" alt=""></p><p><strong>第五棵树：</strong><br><img src="\images\ml_gbdt_15.webp" alt=""></p><p><strong>3.得到最后的强学习器： </strong></p><script type="math/tex; mode=display">f(x)=f_5(x)=f_0(x)+lr*(\sum_{m=1}^5\sum_{j=1}^4\gamma_{jm}I(x \in R_{jm}))</script><p>5.预测样本5：</p><script type="math/tex; mode=display">f_0(x)=1.475</script><p>$f_1(x)$在中，样本4的年龄为25，大于划分节点21岁，又小于30岁，所以被预测为<strong>0.2250</strong>。</p><p>$f_2(x)$在中，样本4的…此处省略…所以被预测为<strong>0.2025</strong></p><p>==为什么是0.2025？这是根据第二颗树得到的，可以GitHub简单运行一下代码==</p><p>$f_3(x)$在中，样本4的…此处省略…所以被预测为<strong>0.1823</strong></p><p>$f_4(x)$在中，样本4的…此处省略…所以被预测为<strong>0.1640</strong></p><p>$f_5(x)$在中，样本4的…此处省略…所以被预测为<strong>0.1476</strong></p><p>最终预测结果：</p><script type="math/tex; mode=display">f(x)=1.475+0.1*(0.225+0.2025+0.1823+0.164+0.1476)=1.56714</script><p><strong>5. 总结</strong></p><p>本文章从GBDT算法的原理到实例详解进行了详细描述，但是目前只写了回归问题，GitHub上的代码也是实现了回归、二分类、多分类以及树的可视化，希望大家继续批评指正，感谢各位的关注，如果对你有用欢迎star。</p><p><strong>参考：</strong></p><ol><li><p>李航 《统计学习方法》</p></li><li><p>Friedman J H . Greedy Function Approximation: A Gradient Boosting Machine[J]. The Annals of Statistics, 2001, 29(5):1189-1232.</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;该内容完全转载自&lt;a href=&quot;http://blog.csdn.net/zpalyq110/article/details/79527653&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;同名CSDN博客&lt;/a&gt;(&lt;a href=&quot;http://blo
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习中的熵、条件熵、相对熵(KL散度)和交叉熵</title>
    <link href="http://yoursite.com/2019/04/26/ML-entropy/"/>
    <id>http://yoursite.com/2019/04/26/ML-entropy/</id>
    <published>2019-04-26T02:47:10.000Z</published>
    <updated>2019-04-27T05:54:28.287Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2019/04/26/entropy/" target="_blank" rel="noopener">GitHub</a><br><a href="https://www.jianshu.com/p/31a683cddb94" target="_blank" rel="noopener">简书</a><br><a href="https://blog.csdn.net/u014630987/article/details/89544279" target="_blank" rel="noopener">CSDN</a></p><p>该文章转载自<a href="https://www.cnblogs.com/kyrieng/p/8694705.html" target="_blank" rel="noopener">详解机器学习中的熵、条件熵、相对熵和交叉熵</a></p><h1 id="1、信息熵-information-entropy"><a href="#1、信息熵-information-entropy" class="headerlink" title="1、信息熵 (information entropy)"></a>1、信息熵 (information entropy)</h1><p>熵 (entropy) 这一词最初来源于热力学。1948年，克劳德·爱尔伍德·香农将热力学中的熵引入信息论，所以也被称为香农熵 (Shannon entropy)，信息熵 (information entropy)。本文只讨论信息熵。首先，我们先来理解一下信息这个概念。信息是一个很抽象的概念，百度百科将它定义为：指音讯、消息、通讯系统传输和处理的对象，泛指人类社会传播的一切内容。那信息可以被量化么？可以的！香农提出的“信息熵”概念解决了这一问题。</p><p>一条信息的信息量大小和它的不确定性有直接的关系。我们需要搞清楚一件非常非常不确定的事，或者是我们一无所知的事，就需要了解大量的信息。相反，如果我们对某件事已经有了较多的了解，我们就不需要太多的信息就能把它搞清楚。所以，从这个角度，我们可以认为，信息量的度量就等于不确定性的多少。比如，有人说广东下雪了。对于这句话，我们是十分不确定的。因为广东几十年来下雪的次数寥寥无几。为了搞清楚，我们就要去看天气预报，新闻，询问在广东的朋友，而这就需要大量的信息，信息熵很高。再比如，中国男足进军2022年卡塔尔世界杯决赛圈。对于这句话，因为确定性很高，几乎不需要引入信息，信息熵很低。</p><p>考虑一个离散的随机变量 $x$，由上面两个例子可知，信息的量度应该依赖于概率分布$p(x)$ ，因此我们想要寻找一个函数 $I(x)$，它是概率 $p(x)$ 的单调函数，表达了信息的内容。怎么寻找呢？如果我们有两个不相关的事件 $x$ 和 $y$，那么观察两个事件同时发生时获得的信息量应该等于观察到事件各自发生时获得的信息之和，即：</p><script type="math/tex; mode=display">I(x,y)=I(x)+I(y)</script><p>因为两个事件是独立不相关的，因此 $p(x,y)=p(x)p(y)$。根据这两个关系，很容易看出 $I(x)$一定与 $p(x)$ 的对数有关 (因为对数的运算法则是 $log_a(mn)=log_am+log_an$)。因此，我们有</p><script type="math/tex; mode=display">I(x)=−\log_p(x)</script><p>其中负号是用来保证信息量是正数或者零。而 $log$ 函数基的选择是任意的（信息论中基常常选择为2，因此信息的单位为比特bits；而机器学习中基常常选择为自然常数，因此单位常常被称为奈特nats）。$I(x)$ 也被称为随机变量 x 的自信息 (self-information)，描述的是随机变量的某个事件发生所带来的信息量。图像如图：<br><img src="\images\entropy_01.png" alt=""></p><p>最后，我们正式引出信息熵。 现在假设一个发送者想传送一个随机变量的值给接收者。那么在这个过程中，他们传输的平均信息量可以通过求 $I(x)=−logp(x)$ 关于概率分布 $p(x)$ 的期望得到，即：</p><script type="math/tex; mode=display">H(X)=−∑xp(x)logp(x)=−∑i=1np(xi)logp(xi)</script><p>$H(X)$ 就被称为随机变量 $x$ 的<strong>熵,它是表示随机变量不确定的度量，是对所有可能发生的事件产生的信息量的期望。</strong></p><p>从公式可得，随机变量的取值个数越多，状态数也就越多，信息熵就越大，混乱程度就越大。当随机分布为均匀分布时，熵最大，且 $0≤H(X)≤logn$。稍后证明。将一维随机变量分布推广到多维随机变量分布，则其联合熵 (Joint entropy) 为：</p><script type="math/tex; mode=display">H(X,Y)=−∑x,yp(x,y)logp(x,y)=−∑i=1n∑j=1mp(xi,yi)logp(xi,yi)</script><p>注意点：1、熵只依赖于随机变量的分布,与随机变量取值无关，所以也可以将 $X$ 的熵记作 $H(p)$。2、令$0log0=0$(因为某个取值概率可能为0)。</p><p>那么这些定义有着什么样的性质呢？考虑一个随机变量 $x$。这个随机变量有4种可能的状态，每个状态都是等可能的。为了把 x 的值传给接收者，我们需要传输2比特的消息。$H(X)=−4×14log214=2 bits$</p><p>现在考虑一个具有4种可能的状态 {a,b,c,d} 的随机变量，每个状态各自的概率为 (12,14,18,18)</p><p>这种情形下的熵为：</p><script type="math/tex; mode=display">H(X)=−12log212−14log214−18log218−18log218=1.75 bits</script><p>我们可以看到，<strong>非均匀分布比均匀分布的熵要小</strong>。现在让我们考虑如何把变量状态的类别传递给接收者。与之前一样，我们可以使用一个2比特的数字来完成这件事情。然而，我们可以利用非均匀分布这个特点，<strong>使用更短的编码来描述更可能的事件，使用更长的编码来描述不太可能的事件</strong>(这点和Huffman编码的原理一样，不知道Huffman树是不是根据熵的原理发明出来的）。我们希望这样做能够得到一个更短的平均编码长度。我们可以使用下面的编码串（哈夫曼编码）：0、10、110、111来表示状态 {a,b,c,d}。传输的编码的平均长度就是：</p><script type="math/tex; mode=display">average code length = 12×1+14×2+2×18×3=1.75 bits</script><p>这个值与上方的随机变量的熵相等。熵和最短编码长度的这种关系是一种普遍的情形。</p><p><a href="https://baike.baidu.com/item/Shannon%20%E7%BC%96%E7%A0%81%E5%AE%9A%E7%90%86/15585931?fr=aladdin" target="_blank" rel="noopener">Shannon 编码定理</a> 表明熵是传输一个随机变量状态值所需的比特位下界（最短平均编码长度）。因此，信息熵可以应用在数据压缩方面。这里<a href="http://www.ruanyifeng.com/blog/2014/09/information-entropy.html" target="_blank" rel="noopener">这篇文章</a>讲的很详细了，我就不赘述了。</p><p>证明<script type="math/tex">0≤H(X)≤logn</script></p><p>利用拉格朗日乘子法证明：</p><p>因为 <script type="math/tex">p(1)+p(2)+⋯+p(n)=1</script></p><p>所以有</p><p><strong>目标函数</strong>：$f(p(1),p(2),…,p(n))=−(p(1)logp(1)+p(2)logp(2)+⋯+p(n)logp(n))$</p><p><strong>约束条件</strong>：$g(p(1),p(2),…,p(n),λ)=p(1)+p(2)+⋯+p(n)−1=0$</p><p>1、定义拉格朗日函数：</p><script type="math/tex; mode=display">L(p(1),p(2),…,p(n),λ)=−(p(1)logp(1)+p(2)logp(2)+⋯+p(n)logp(n))+λ(p(1)+p(2)+⋯+p(n)−1)</script><p>　2、$L(p(1),p(2),…,p(n),λ)$分别对 $p(1),p(2),p(n),λ$ 求偏导数，令偏导数为 0：</p><script type="math/tex; mode=display">λ−log(e⋅p(1))=0</script><script type="math/tex; mode=display">λ−log(e⋅p(2))=0</script><script type="math/tex; mode=display">……</script><script type="math/tex; mode=display">λ−log(e⋅p(n))=0</script><script type="math/tex; mode=display">p(1)+p(2)+⋯+p(n)−1=0</script><p>　　3、求出 $p(1),p(2),…,p(n)$ 的值：</p><p>解方程得，$p(1)=p(2)=⋯=p(n)=1n$</p><p>代入 $f(p(1),p(2),…,p(n))$ 中得到目标函数的极值为 $f(1n,1n,…,1n)=−(1nlog1n+1nlog1n+⋯+1nlog1n)=−log(1n)=logn$</p><p><strong>由此可证 logn 为最大值。</strong></p><p>个人感觉上述证明不是很严谨的拉格朗日推导，见<a href="https://wanghuaishi.wordpress.com/2017/02/21/%E5%9B%BE%E8%A7%A3%E6%9C%80%E5%A4%A7%E7%86%B5%E5%8E%9F%E7%90%86%EF%BC%88the-maximum-entropy-principle%EF%BC%89/" target="_blank" rel="noopener">另一篇文章</a></p><h1 id="2、条件熵-Conditional-entropy"><a href="#2、条件熵-Conditional-entropy" class="headerlink" title="2、条件熵 (Conditional entropy)"></a>2、条件熵 (Conditional entropy)</h1><p>条件熵 $H(Y|X)$ 表示在已知随机变量 $X$ 的条件下随机变量 $Y$ 的不确定性。条件熵 $H(Y|X)$ 定义为 $X$ 给定条件下 $Y$ 的条件概率分布的熵对  $X$ 的数学期望：</p><p><img src="\images\entropy_03.png" alt=""></p><p>条件熵 $H(Y|X)$ 相当于联合熵 $H(X,Y)$ 减去单独的熵 $H(X)$，即</p><p>$H(Y|X)=H(X,Y)−H(X)$，证明如下：</p><p><img src="\images\entropy_03.png" alt=""></p><p>举个例子，比如环境温度是低还是高，和我穿短袖还是外套这两个事件可以组成联合概率分布 $H(X,Y)$，因为两个事件加起来的信息量肯定是大于单一事件的信息量的。假设 $H(X)$ 对应着今天环境温度的信息量，由于今天环境温度和今天我穿什么衣服这两个事件并不是独立分布的，所以在已知今天环境温度的情况下，我穿什么衣服的信息量或者说不确定性是被减少了。当已知 $H(X)$ 这个信息量的时候，$H(X,Y)$ 剩下的信息量就是条件熵：</p><script type="math/tex; mode=display">H(Y|X)=H(X,Y)−H(X)</script><p>因此，可以这样理解，描述 $X$ 和 $Y$ 所需的信息是描述 $X$ 自己所需的信息,加上给定  $X$ 的条件下具体化  $Y$ 所需的额外信息。关于条件熵的例子可以看这篇文章，讲得很详细。<a href="https://zhuanlan.zhihu.com/p/26551798" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26551798</a></p><h1 id="3、相对熵-Relative-entropy-，也称KL散度-Kullback–Leibler-divergence"><a href="#3、相对熵-Relative-entropy-，也称KL散度-Kullback–Leibler-divergence" class="headerlink" title="3、相对熵 (Relative entropy)，也称KL散度 (Kullback–Leibler divergence)"></a>3、相对熵 (Relative entropy)，也称KL散度 (Kullback–Leibler divergence)</h1><p>设 $p(x)、q(x)$ 是 离散随机变量 $X$ 中取值的两个概率分布，则 $p$ 对 $q$ 的相对熵是：</p><script type="math/tex; mode=display">DKL(p||q)=\sum_xp(x)log\frac{p(x)}{q(x)}=E_{p(x)}log\frac{p(x)}{q(x)}</script><p>性质：</p><p>1、如果 $p(x)$ 和 $q(x)$ 两个分布相同，那么相对熵等于0</p><p>2、$DKL(p||q)≠DKL(q||p)$ ，相对熵具有不对称性。大家可以举个简单例子算一下。</p><p>3、$DKL(p||q)≥0$ 证明如下（利用Jensen不等式<a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Jensen%27s_inequality</a>):</p><p><img src="\images\entropy_05.png" alt=""></p><p>因为：</p><script type="math/tex; mode=display">\sum_xp(x)=1</script><p>所以：</p><script type="math/tex; mode=display">DKL(p||q)≥0</script><p> <strong>总结：相对熵可以用来衡量两个概率分布之间的差异，上面公式的意义就是求 $p$ 与 $q$ 之间的对数差在 $p$ 上的期望值。</strong></p><h1 id="4、交叉熵-Cross-entropy"><a href="#4、交叉熵-Cross-entropy" class="headerlink" title="4、交叉熵 (Cross entropy)"></a>4、交叉熵 (Cross entropy)</h1><p> 现在有关于样本集的两个概率分布 $p(x)$ 和 $q(x)$，其中  $p(x)$ 为真实分布， $q(x)$ 非真实分布。如果用真实分布 $p(x)$ 来衡量识别别一个样本所需要编码长度的期望（平均编码长度）为:</p><script type="math/tex; mode=display">H(p)=\sum_xp(x)log\frac{1}{p(x)}=-\sum_xp(x)log{p(x)}</script><p>如果使用非真实分布 $q(x)$ 来表示来自真实分布 $p(x)$ 的平均编码长度，则是：</p><p>$H(p)=\sum_xp(x)log\frac{1}{q(x)}=-\sum_xp(x)log{q(x)}$。（因为用 $q(x)$ 来编码的样本来自于分布 $q(x)$ ，所以 $H(p,q)$ 中的概率是 $p(x)$）。此时就将 $H(p,q)$ 称之为交叉熵。举个例子。考虑一个随机变量 $x$，真实分布$p(x)=(12,14,18,18)$，非真实分布 $q(x)=(14,14,14,14)$， 则$H(p)=1.75 bits$（最短平均码长），交叉熵 $H(p,q)=12log24+14log24+18log24+18log24=2 bits$。由此可以看出根据非真实分布 $q(x)$ 得到的平均码长大于根据真实分布 $p(x)$ 得到的平均码长。</p><p>我们再化简一下相对熵的公式。</p><script type="math/tex; mode=display">DKL(p||q)=\sum_xp(x)log \frac{p(x)}{q(x)}=\sum_xp(x)logp(x)−\sum_x p(x)logq(x)</script><p>有没有发现什么？</p><p>熵的公式 $H(p)=−\sum_xp(x)logp(x)$</p><p>交叉熵的公式 $H(p,q)=\sum_xp(x)log\frac{1}{q(x)}=−\sum_xp(x)logq(x)$</p><p>所以有：</p><p>$DKL(p||q)=H(p,q)−H(p)$（当用非真实分布 $q(x)$ 得到的平均码长比真实分布 p(x) 得到的平均码长多出的比特数就是相对熵）</p><p>又因为 $DKL(p||q)≥0$</p><p>所以 $H(p,q)≥H(p)$（当 $p(x)=q(x)$ 时取等号，此时交叉熵等于信息熵）</p><p>并且当 $H(p)$ 为常量时（注：在机器学习中，训练数据分布是固定的），最小化相对熵 $DKL(p||q)$ 等价于最小化交叉熵 $H(p,q)$ 也等价于最大化似然估计（具体参考Deep Learning 5.5）。</p><p>在机器学习中，我们希望在训练数据上模型学到的分布 $P(model)$ 和真实数据的分布  $P(real)$ 越接近越好，所以我们可以使其相对熵最小。但是我们没有真实数据的分布，所以只能希望模型学到的分布 $P(model)$ 和训练数据的分布 $P(train)$ 尽量相同。假设训练数据是从总体中独立同分布采样的，那么我们可以通过最小化训练数据的经验误差来降低模型的泛化误差。即：</p><p>希望学到的模型的分布和真实分布一致，$P(model)≃P(real)$<br>但是真实分布不可知，假设训练数据是从真实数据中独立同分布采样的，$P(train)≃P(real)$<br>因此，我们希望学到的模型分布至少和训练数据的分布一致，$P(train)≃P(model)$<br>根据之前的描述，最小化训练数据上的分布  $P(train)$ 与最小化模型分布 $P(model)$ 的差异等价于最小化相对熵，即 $DKL(P(train)||P(model))$。此时，$ P(train) $就是$DKL(p||q)$ 中的 $p$，即真实分布，$P(model)$ 就是 $q$。又因为训练数据的分布 $p$ 是给定的，所以求  $DKL(p||q)$  等价于求 $H(p,q)$。得证，交叉熵可以用来计算学习模型分布与训练分布之间的差异。交叉熵广泛用于逻辑回归的Sigmoid和Softmax函数中作为损失函数使用。这篇文章先不说了。</p><h1 id="5、总结"><a href="#5、总结" class="headerlink" title="5、总结"></a>5、总结</h1><p>信息熵是衡量随机变量分布的混乱程度，是随机分布各事件发生的信息量的期望值，随机变量的取值个数越多，状态数也就越多，信息熵就越大，混乱程度就越大。当随机分布为均匀分布时，熵最大；信息熵推广到多维领域，则可得到联合信息熵；条件熵表示的是在 X 给定条件下，Y 的条件概率分布的熵对 X的期望。<br>相对熵可以用来衡量两个概率分布之间的差异。<br>交叉熵可以来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小。</p><p>或者：</p><p>信息熵是传输一个随机变量状态值所需的比特位下界（最短平均编码长度）。<br>相对熵是指用 q 来表示分布 p  额外需要的编码长度。<br>交叉熵是指用分布 q 来表示本来表示分布 p 的平均编码长度。</p><h1 id="6、参考"><a href="#6、参考" class="headerlink" title="6、参考"></a>6、参考</h1><p>1、吴军《数学之美》</p><p>2、李航《统计学习方法》</p><p>3、马春鹏《模式识别与机器学习》</p><p>3、<a href="https://www.zhihu.com/question/41252833" target="_blank" rel="noopener">如何通俗的解释交叉熵与相对熵</a></p><p>4、<a href="https://www.zhihu.com/question/65288314/answer/244557337" target="_blank" rel="noopener">为什么交叉熵（cross-entropy）可以用于计算代价？</a></p><p>5、<a href="https://baike.baidu.com/item/%E4%BA%A4%E5%8F%89%E7%86%B5/8983241?fr=aladdin" target="_blank" rel="noopener">交叉熵的百度百科解释</a></p><p>6、<a href="https://blog.csdn.net/saltriver/article/details/53056816" target="_blank" rel="noopener">信息熵到底是什么</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2019/04/26/entropy/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://www.jianshu.com/
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习面试题库——笔试题</title>
    <link href="http://yoursite.com/2019/04/16/machine-learning-question/"/>
    <id>http://yoursite.com/2019/04/16/machine-learning-question/</id>
    <published>2019-04-16T04:30:48.000Z</published>
    <updated>2019-04-16T05:18:30.133Z</updated>
    
    <content type="html"><![CDATA[<p>该资源来自<a href="http://www.julyedu.com/" target="_blank" rel="noopener">七月在线网站</a></p><h1 id="1、-如果线性回归模型中的随机误差存在异方差性，那么参数的OLS估计量是（-）"><a href="#1、-如果线性回归模型中的随机误差存在异方差性，那么参数的OLS估计量是（-）" class="headerlink" title="1、 如果线性回归模型中的随机误差存在异方差性，那么参数的OLS估计量是（ ）"></a>1、 如果线性回归模型中的随机误差存在异方差性，那么参数的OLS估计量是（ ）</h1><ol><li>无偏的，有效的</li><li><strong>无偏的，非有效的</strong></li><li>有偏的，有效的</li><li>有偏的，非有效的</li></ol><blockquote><p>OLS即普通最小二乘法。由高斯—马尔可夫定理，在给定经典线性回归的假定下，最小二乘估计量是具有最小方差的线性无偏估计量。根据证明过程可知，随机误差中存在异方差性不会影响其无偏性，而有效性证明中涉及同方差性，即异方差会影响参数OLS估计量的有效性。</p><p>经典线性回归模型的一个重要假定：总体回归函数中的随机误差项满足同方差性，即它们都有相同的方差。如果这一假定不满足，即：随机误差项具有不同的方差，则称线性回归模型存在异方差性。</p></blockquote><h1 id="2-信息增益"><a href="#2-信息增益" class="headerlink" title="2 信息增益"></a>2 信息增益</h1><p><img src="/images/ml-question-002.png" alt=""></p><h1 id="3-在二分类问题中，当测试集的正例和负例数量不均衡时，以下评价方案哪个是相对不合理的（-）（假设precision-TP-TP-FP-recall-TP-TP-FN-。）"><a href="#3-在二分类问题中，当测试集的正例和负例数量不均衡时，以下评价方案哪个是相对不合理的（-）（假设precision-TP-TP-FP-recall-TP-TP-FN-。）" class="headerlink" title="3  在二分类问题中，当测试集的正例和负例数量不均衡时，以下评价方案哪个是相对不合理的（ ）（假设precision=TP/(TP+FP),recall=TP/(TP+FN)。）"></a>3  在二分类问题中，当测试集的正例和负例数量不均衡时，以下评价方案哪个是相对不合理的（ ）（假设precision=TP/(TP+FP),recall=TP/(TP+FN)。）</h1><ol><li><strong>Accuracy:(TP+TN)/all</strong></li><li>F-value:2<em>recall</em>precision/(recall+precision)</li><li>G-mean:sqrt(precision*recall)</li><li>AUC:ROC曲线下面积</li></ol><blockquote><p>解析：对于分类器，主要的评价指标有precision，recall，F-score，以及ROC曲线等。<br>在二分类问题中，我们主要关注的是测试集的正样本能否正确分类。当样本不均衡时，比如样本中负样本数量远远多于正样本，此时如果负样本能够全部正确分类，而正样本只能部分正确分类，那么(TP+TN)可以得到很高的值，也就是Accuracy是个较大的值，但是正样本并没有取得良好的分类效果。因此A选项是不合理的。在样本不均衡时，可以采用BCD选项方法来评价。<br>ROC曲线以True Positive rate为纵轴，False Positive Rate 为横轴，曲线离（0，1）点越近越好。</p></blockquote><h1 id="4-HMM中维特比算法的复杂度"><a href="#4-HMM中维特比算法的复杂度" class="headerlink" title="4 HMM中维特比算法的复杂度"></a>4 HMM中维特比算法的复杂度</h1><p><img src="/images/ml-question-004.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;该资源来自&lt;a href=&quot;http://www.julyedu.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;七月在线网站&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;1、-如果线性回归模型中的随机误差存在异方差性，那么参数的OLS估计量是（-）&quot;&gt;&lt;a 
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>期望最大算法(EM算法)</title>
    <link href="http://yoursite.com/2019/03/30/ML-EM/"/>
    <id>http://yoursite.com/2019/03/30/ML-EM/</id>
    <published>2019-03-30T04:25:08.000Z</published>
    <updated>2019-04-01T07:39:24.601Z</updated>
    
    <content type="html"><![CDATA[<p>最大期望算法（Expectation-Maximization algorithm, EM）是一类通过迭代进行极大似然估计（Maximum Likelihood Estimation, MLE）的优化算法。用于含有隐变量（Latent Variable) 的概率模型参数的极大似然估计或极大后验概率估计。Em算法的每次迭代包含两个步骤：1. E步： 求期望；2. M步：求极大，因此该算法被称作EM算法。</p><h1 id="1-极大似然估计"><a href="#1-极大似然估计" class="headerlink" title="1 极大似然估计"></a>1 极大似然估计</h1><p>极大似然估计是一种利用极大似然函数来求解模型参数 $\theta$ 的一种估计方法。 在数理统计学中，<strong>似然函数</strong>是一种关于统计模型的参数的函数。在给定一个样本 $x$ 时，关于参数 $\theta$ 的似然函数 $L(\theta|x)$ (在数值上)等于给定参数 $\theta$ 后变量 $x$ 的概率(似然函数与概率的一种关系):</p><script type="math/tex; mode=display">L(\theta|x) = P(X=x|\theta)\tag{1}</script><p>在给定样本集 $\bar x=\{x_1,x_2,…,x_N\}$, 在这个样本集上的似然函数为:</p><script type="math/tex; mode=display">L(\theta|\bar x) = P(\bar x|\theta) = \prod_{i=1}^{N}P(x_i|\theta)\tag{2}</script><p>由于公式2中含有连乘运算，因此不方便计算，通常我们利用对数的特性，来对公式2取对数来得到<strong>对数似然函数</strong>，为了方便，我们还是将 $\log L(\theta|\bar x)$ 记作 $L(\theta|\bar x)$:</p><script type="math/tex; mode=display">L(\theta|\bar x) = \log P(\bar x|\theta) = \log \prod_{i=1}^{N}P(x_i|\theta) = \sum_{i=1}^{N} \log P(x_i|\theta)\tag{3}</script><p>通过 $L(\theta|\bar x)$ 对 $\theta$ 求导，并取倒数为0，此时得到最优的 $\theta$.</p><p>下面以高斯模型为例来简单说明极大似然估计，并引导出EM算法。</p><p><strong>示例一</strong></p><p>如下图，我们采集到一些数据(黄色的+)服从单个高斯分布，由于高斯分布中有两个参数，均值和方差,我们将其表示为 $\theta =\{\mu,\sigma\}$,通过极大似然估计方法我们来求得最优的$\theta^{*} =\{\mu^{*},\sigma^{*}\}$,如下图我们就是从$\theta_1,\theta_2,\theta_3$中求得 $\theta_1$</p><p><img src="/images/ML_single_Gaussia.png" alt="单一高斯极大似然估计"><br>图1 单一高斯模型</p><p>大致思路就是利用公式3求得对数似然函数，然后对其求最大值，即求得 $\theta^*$,该过程通过如两个公式来分别求出 $\mu$ 和 $\sigma$:</p><script type="math/tex; mode=display">L(\theta|\bar x) = \sum_{i=1}^{N}\log N(x_i|\mu, \sigma)\tag{4}</script><script type="math/tex; mode=display">\frac{\partial L(\theta|\bar x)}{\partial \mu} =0\tag{5}</script><script type="math/tex; mode=display">\frac{\partial L(\theta|\bar x)}{\partial \sigma} =0\tag{6}</script><p>我们通过上式可以求得</p><script type="math/tex; mode=display">\begin{aligned}    \mu_{MLE}=&\frac{1}{N}\sum_{i=1}^Nx_i \\    \sigma_{MLE}=&\frac{\sum_{i=1}^N(x_i-\mu_{MLE})^2}{N}\end{aligned}\tag{7}</script><p><strong>示例二</strong></p><p>如图二所示的样本，从图中我们可以明显看出，黄色和黑色样本分别位于两个类中，此时如果我们还是用单一的高斯模型来为该样本集建模，是行不通的，主要有两个原因：</p><ol><li>高斯分布的特点是在 $\mu$ 附近的概率很大，而从徒儿看出，在 $\mu$ 附近没有样本，样本基本位于两端，距离 $\mu$ 很远，这不符合高斯分布特点；</li><li>黄色的样本比黑色的样本更密集，也就是他的概率更大，因此我们不应该同等对待。</li></ol><p><img src="/images/ML_EM_pic2.png" alt="混合样本单一模型"></p><p>既然单一高斯模型已经无法适用于该样本集，那我们用多个高斯模型呢，再此示例中，我们可以看出样本集分布在两个分布中较为合理，其结果如图 3所示.通过比较图2和图3，可以得出，图3时更合理的分布，此时得到高斯混合模型，其混合模型如图4所示。</p><p><img src="/images/ML_EM_pic3.png" alt="图3 双高斯模型"><br><img src="/images/ML_EM_pic4.png" alt="图4 混合高斯模型"></p><p>根据上面的描述，我们对每个高斯叠加起来，即:</p><script type="math/tex; mode=display">    P(x_i|\theta) = \sum_{l=1}^kN(x_i|\mu_l,\sigma_l)    \tag{8}</script><p>但是，我们把公式8对变量 $x$ 进行积分，按照概率，该积分应该为1，事实上，该积分不为一，而为2.为了解决这个问题，我们可以平均每个高斯，即给一个权重 $\frac{1}{2}$, 但是这部理想，有时样本在不同分布中权重不一致，因此我们可以给每个分布一个权重比例，且比例之和为1，我们将其推广到一般形式，即：</p><script type="math/tex; mode=display">\begin{aligned}    & P(x|\bar \theta) = \sum_{i=1}^{k}\alpha_i N(x_i|\mu_l,\sigma_l) \\    & s.t. \sum_{i=1}^k \alpha_i = 1\end{aligned}\tag{9}</script><p>该公式的参数变为 $\theta=\{\mu_1,\mu_2,…\mu_k, \sigma_1,\sigma_2,…,\sigma_k, \alpha_1, \alpha_2,…,\alpha_k-1\}$<br>此时，我们将上式代入公式3得到该样本集的似然函数为:</p><script type="math/tex; mode=display">L(\theta|\bar x) = \sum_{i=1}^{N}\log P(x_i|\theta)=\sum_{i=1}^{N}\log\sum_{l=1}^{k}\alpha_l N(x_i|\mu_l,\sigma_l)\tag{10}</script><script type="math/tex; mode=display">\begin{aligned}    \bar \theta_{MLE}=&\arg \max_{\theta}L(\theta|\bar x) \\            =&\arg \max_{\theta}\sum_{i=1}^{N}\log P(x_i|\theta)\\            =&\arg \max_{\theta} \sum_{i=1}^{N}\log\sum_{l=1}^{k}\alpha_l N(x_i|\mu_l,\sigma_l)\end{aligned}\tag{11}</script><p>由于 $\log$ 函数里面存在一个求和公式，如果我们再用极大似然估计法来求解上式的解，将变得很复杂，甚至求不出来，因此极大似然估计发在这里已行不通。</p><p>以上就是期望最大算法被提出的一个简单原因。</p><h1 id="2-期望最大算法"><a href="#2-期望最大算法" class="headerlink" title="2 期望最大算法"></a>2 期望最大算法</h1><p>期望最大算法通过迭代求解来求得最后的一个近似最优的 $\theta$ 值，由“迭代”二字可以看出，当前步的 $\theta$肯定与上一步的 $\theta$ 有关，即存在一种关系:</p><script type="math/tex; mode=display">\theta ^{(g+1)}=f(\theta ^{(g)})\tag{12}</script><p>EM算法对函数 $f$ 的定义为:</p><script type="math/tex; mode=display">\theta ^{(g+1)}=\arg \max_{\theta}\int_{z}\log P(x, z|\theta)P(z|x,\theta^{(g)})dz\tag{13}</script><p>其中 $z$ 是为了简化计算，而引入的一个隐变量（Latent Variable), $x$ 为观测变量(Observable Variable).这两个变量对应到高斯混合模型中，分别为: 我们收集到的样本集和隐藏的模型，每个样本 $x_i$ 对应一个 $z_i$,来表示该样本来自哪个分布。因此 $x_i$ 的概率分可表示为:</p><script type="math/tex; mode=display">P(x_i)=\int_{z_l}P_{\bar \theta}(x|z_l)P(z_l)dz_l\tag{14}</script><p>将上式右边分开描述，$P_{\bar \theta}(x|z_i)$ 表示 $x_i$ 在 $l$ 个高斯分布下的分布，$P(z_l)$ 表示分布为第 $l$ 个高斯分布的概率，即 $\alpha_l$,因此上式为:</p><script type="math/tex; mode=display">P(x_i)=\sum_{z_l=1}^{k}\alpha_{z_l}N(x_i|\mu_{z_i}, \sigma_{z_i})\tag{15}</script><p>该式和公式9保持一致，因此隐变量的引入，并没有改变函数的意义。</p><p>一般地，我们将 $x$ 和 $z$ 连在一起成为完全数据(complete-data),观测数据 $x$ 称为不完全数据。</p><h1 id="3-EM算法的收敛性"><a href="#3-EM算法的收敛性" class="headerlink" title="3 EM算法的收敛性"></a>3 EM算法的收敛性</h1><p>前一节讲解了通过引入隐变量并没有改变 $P(x)$ 的表示，同事给出了 EM 算法的公式，但是EM算法的收敛性如何呢？下面我们就讲解EM算法的收敛性，要使EM算法收敛同时满足公式11的极大似然函数。在迭代过程中，要使 $\theta$ 逐渐收敛，同时满足公式11，则必存在:</p><script type="math/tex; mode=display">\log P(x|\theta^{(g+1)}) \geq \log P(x|\theta^{(g)})\tag{16}</script><p>根据对数函数和条件概率一个特征 $\log \frac{A}{B}=\log A - \log B$ 和 $P(x)=\frac{P(x, z)}{P(z|x)}$可得：</p><script type="math/tex; mode=display">\log P(x|\theta)= \log (x,z|\theta) - \log(z|x,\theta)\tag{17}</script><p>我们对公式两边分别对分布 $P(z|x, \theta^{(g)})$ 求期望，左边为:</p><script type="math/tex; mode=display">\begin{aligned}   \int _z \log P(x|\theta) P(z|x, \theta^{(g)}) dz=& \log P(x|\theta) \int _z P(z|x, \theta^{(g)}) dz \\   =& \log P(x|\theta)\end{aligned}\tag{18}</script><p>右边为:</p><script type="math/tex; mode=display">    \int _zlogP(x,z|\theta)P(z|x, \theta^{(g)}) dz-\int_z\log P(z|x,\theta)P(z|x, \theta^{(g)}) dz =Q(\theta, \theta^{(g)}) -H(\theta, \theta^{(g)})\tag{19}</script><p>我们通常称 $Q(\theta, \theta^{(g)})$ 为 <strong>Q函数</strong>：完全数据的对数似然函数$\log P(x, z|\theta)$关于给定观测数据和当前参数 $\theta^{(g)}$下对隐藏数据 $z$ 的条件概率的期望称为Q函数。</p><p>根据公式17、18和19我们可以得出:</p><script type="math/tex; mode=display">\begin{aligned}    \log P(x|\theta)=&\int _z \log P(x,z|\theta)P(z|x, \theta^{(g)}) dz-\int_z\log P(z|x,\theta)P(z|x, \theta^{(g)}) dz\\    =& Q(\theta, \theta^{(g)}) -H(\theta, \theta^{(g)})\end{aligned}\tag{20}</script><p>极大似然估计就是对 $\log P(x|\theta)$ 进行极大化，对上面的右边公式进行极大化，可不可以呢？不可以，如果这样做就又回到最大似然函数了，因此得不了解，EM算法只是对 $Q(\theta, \theta^{(g)})$ 进行极大化，因此很容易得出:</p><script type="math/tex; mode=display">Q(\theta^{(g+1)}, \theta^{(g)}) \geq Q(\theta^{(g)}, \theta^{(g)})\tag{21}</script><p>我们要想通过优化$Q(\theta, \theta^{(g)})$ 使 $\log P(x|\theta)$ 逐渐增大，则存在:</p><script type="math/tex; mode=display">Q(\theta^{(g+1)}, \theta^{(g)}) - Q(\theta^{(g)}, \theta^{(g)})\geq H(\theta^{(g+1)}, \theta^{(g)}) - H(\theta^{(g)}, \theta^{(g)})\tag{22}</script><p>如果，我们假设，$\forall\theta,H(\theta^{(g)}, \theta^{(g)}) - H(\theta, \theta^{(g)}) \geq 0 \Rightarrow H(\theta^{(g)}, \theta^{(g)}) \geq H(\theta^{(g+1)}, \theta^{(g)})$</p><p>下面，就对这个假设进行证明:</p><script type="math/tex; mode=display">\begin{aligned}    & H(\theta^{(g)}, \theta^{(g)}) - H(\theta, \theta^{(g)}) \\    & =\int_z\log P(z|x,\theta^{(g)})P(z|x, \theta^{(g)})dz - \int_z\log p(z|x,\theta)P(z|x, \theta^{(g)})dz \\    & = \int_z\log [\frac{P(z|x,\theta^{(g)})}{p(z|x,\theta)}]P(z|x, \theta^{(g)})dz \\    & = \int_z -\log [\frac{p(z|x,\theta)}{P(z|x,\theta^{(g)})]}P(z|x, \theta^{(g)})dz \\    & \geq -\log [\int_z\frac{p(z|x,\theta)}{P(z|x,\theta^{(g)})]}P(z|x, \theta^{(g)})dz] \\    & = -log \int_z p(z|x,\theta) dz \\    & =-\log1=0 \\    & \Rightarrow H(\theta^{(g)}, \theta^{(g)}) - H(\theta, \theta^{(g)}) \geq 0\end{aligned}\tag{23}</script><p>$\geq$ 那一步利用了 Jensen 不等式（Jensen Inequality),这其实是凸函数的一个特点，如图5所示。我们求两个点的函数值的期望，即为直线上的黑点，如果我们先对自变量 x 求期望，则 $-\log(Ex)$ 为log函数曲线上的黑点，根据曲线图，我们可以得出</p><script type="math/tex; mode=display">\begin{aligned}    pf(x_1)+(1-p)f(x_2) \geq & f(px_1+(1-p)x_2) \\    E[f(x)] \geq & f[E(x)]\end{aligned}\tag{24}</script><p>即，函数的均值大于等于均值的函数。</p><p><img src="/images/ML_EM_pic5.png" alt="Jensen 不等式"></p><p>通过以上证明可以得出，EM算法通过迭代最大化 $\int _z \log P(x,z|\theta)P(z|x, \theta^{(g)}) dz$，从而使 $P(x|\theta)$ 逐渐增大。</p><script type="math/tex; mode=display">\begin{aligned}    &\log P(x|\theta^{(t+1)}) - \log P(x|\theta^{(t+1)}) \\     &= Q(\theta^{(t+1)}, \theta^{(t)})-H(\theta^{(t+1)},\theta^{(t)}) - [Q(\theta^{(t)}, \theta^{(t)})-H(\theta^{(t)},\theta^{(t)})] \\    &= [Q(\theta^{(t+1)}, Q(\theta^{(t)})-Q(\theta^{(t)}, \theta^{(t)})]-[H(\theta^{(t+1)},\theta^{(t)})-H(\theta^{(t)},\theta^{(t)})] \\    & \geq 0\end{aligned}\tag{25}</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最大期望算法（Expectation-Maximization algorithm, EM）是一类通过迭代进行极大似然估计（Maximum Likelihood Estimation, MLE）的优化算法。用于含有隐变量（Latent Variable) 的概率模型参数的极
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>支持向量机——线性可分支持向量机</title>
    <link href="http://yoursite.com/2019/03/26/ML-hard-margin-svm/"/>
    <id>http://yoursite.com/2019/03/26/ML-hard-margin-svm/</id>
    <published>2019-03-26T14:21:04.000Z</published>
    <updated>2019-03-27T07:05:56.548Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2019/03/26/ML-hard-margin-svm/" target="_blank" rel="noopener">GitHub</a><br><a href="https://www.jianshu.com/p/3938e0f22692" target="_blank" rel="noopener">简书</a><br><a href="https://blog.csdn.net/u014630987/article/details/88845317" target="_blank" rel="noopener">CSDN</a></p><h1 id="1-线性可分支持向量机"><a href="#1-线性可分支持向量机" class="headerlink" title="1 线性可分支持向量机"></a>1 线性可分支持向量机</h1><p>支持向量机(Support vector machines, SVM)是一种<strong>二分类模型</strong>，它的基本模型是定义在特征空间上的间隔最大的线性分类器，他的学习策略就是间隔最大化，同时该方法可以形式化为一个求解图二次规划。</p><p>支持向量机可分为三类:</p><ol><li>线性可分支持向量机、硬间隔（hard-margin svm)</li><li>线性支持向量机、软间隔(soft-margin svm)</li><li>非线性支持向量机、Kernel SVM</li></ol><p>支持向量机模型中存在三宝:</p><ol><li>间隔</li><li>对偶</li><li>核技巧</li></ol><p>支持向量机和感知机在某些方面很相似，其相同点:</p><ol><li>都是二分类模型</li><li>都是通过一个分离超平面对特征进行分类</li></ol><p>不同点：</p><ol><li>SVM 是特殊的感知机</li><li>感知机是用误分类最小的策略，求得分离超平面，这时存在无穷个解，感知机利用间隔最大化求得最优分离超平面。如下图所示</li></ol><p><img src="/images/mlp_svm.png" alt="SVM与感知机区别"><br>图1 感知机与支持向量机区别</p><p>图中的蓝色和黄色圆点分别表示正负样本，对于这个二分类，从图中我们可知，在最上面的黄线和最下面的绿线之间的线条都是可以把训练样本集完全分开的，这就是感知机的原理，通过这些分离超平面把训练集分开，这样的分离超平面存在很多条，比如图中的虚线，从视觉上中间那条实线应该是众多线条中最优的一条，感知机对于学习的分离超平面由于优化算法、学习率等不同因素，会随机地学习到这众多分离超平面中的一条，当学习到的是靠近上下边缘的分离超平面是，对于一个未知样本，当这个样本稍微浮动一下，模型就很可能对他进行误分类了，因此鲁棒性很低，而支持向量机的目标是找到图中中间那条最优的分离超平面。</p><p><strong>定义(线性可分支持向量机)</strong>:给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到一个分离超平面:</p><script type="math/tex; mode=display"> w^* \cdot x + b^* =0\tag{1}</script><p>即相应的决策模型:</p><script type="math/tex; mode=display">    f(x)=sign(w^* \cdot x + b^*)    \tag{2}</script><p>此模型就为线性可分支持向量机。其中 $ w^<em>$ 表示分离超平面的法向量， $b^</em>$ 表示截距，位于分离超平面之上的样本为正样本，之下的为负样本。</p><h1 id="2-函数间隔和几何间隔"><a href="#2-函数间隔和几何间隔" class="headerlink" title="2 函数间隔和几何间隔"></a>2 函数间隔和几何间隔</h1><p>一般来说，一个点到分离超平面的远近可以表示分类预测的确信程度，在给定分离超平面$w \cdot x + b = 0$的情况下， $|w \cdot x + b|$能够相对地表示点 $x$ 到分离超平面的远近。同时 $w \cdot x + b$的符号与类别标记 $y$ 是否保持一致来表示分类是否正确，所以，可以用$y(w \cdot x + b)$ 来表示分类的正确性及确信度，这就是函数间隔（functional margin)的概念。</p><p><strong>定义(函数间隔)</strong>:对于给定训练数据集 $T$ 和超平面 $(w, b)$,定义超平面 $(w, b)$ 关于样本点 $(x_i,y_i)$ 的函数间隔为:</p><script type="math/tex; mode=display">\hat \gamma_i = y_i(w \cdot x_i + b)\tag{3}</script><p>分离超平面关于训练数据集 $T$ 的函数间隔为超平面关于 $T$ 中所有样本点 $(x_i,y_i)$ 的函数间隔最小值:</p><script type="math/tex; mode=display">\hat \gamma = \min_{i=1...N}y_i(w \cdot x_i + b)\tag{4}</script><p>上述定义是在给定超平面 $(w,b)$ 的时候计算，然而在实际支持向量机的学习过程中，只有函数间隔是不够的，因为当 $w$ 和 $b$ 按比例同时扩大 $n$ 倍，此时函数间隔也扩大 $n$ 倍，而超平面并没有改变。因此我们需要对分离超平面加以约束，如规范化，$||w||=1$,使得间隔不随 $w$ 和 $b$ 成比例扩大而改变。这时函数间隔就成为了几何间隔（geometric margin)</p><p><strong>定义(几何间隔)</strong>:对于给定训练数据集 $T$ 和超平面 $(w, b)$,定义超平面 $(w, b)$ 关于样本点 $(x_i,y_i)$ 的几何间隔为:</p><script type="math/tex; mode=display"> \gamma_i = y_i(\frac{w}{||w||} \cdot x_i + \frac{b}{||w||})\tag{5}</script><p>分离超平面关于训练数据集 $T$ 的函数间隔为超平面关于 $T$ 中所有样本点 $(x_i,y_i)$ 的函数间隔最小值:</p><script type="math/tex; mode=display">\gamma = \min_{i=1...N} \gamma_i\tag{6}</script><p>$||w||$ 为 $w$ 的 $L_2$ 范数。其实上述公式就是我们中学时候学习的点到直线的距离公式的推广，或者说点到直线的距离公式是该公式在二位平面下的表示。</p><p>通过公式4和公式6的比较，我们可以得出函数间隔和几何间隔有如下关系:</p><script type="math/tex; mode=display">\begin{aligned}    \gamma_i =& \frac{\hat \gamma_i}{||w||} \\    \gamma =& \frac{\hat \gamma}{||w||}\end{aligned}\tag{7}</script><h1 id="3-间隔最大化"><a href="#3-间隔最大化" class="headerlink" title="3 间隔最大化"></a>3 间隔最大化</h1><p>支持向量机学习的基本思想是求解能够<strong>正确划分训练数据集</strong>且<strong>几何间隔最大</strong>的分离超平面。间隔最大化的直观解释是：使分类决策模型以较大的确信度来对数据集分类，同时对离超平面较近的点也有很大的确信度。</p><p>因此，最大间隔支持向量机形式化为:</p><script type="math/tex; mode=display">\begin{aligned}    &\max_{w,b}\quad\gamma \\    &s.t. \quad y_i(\frac{w}{||w||} \cdot x_i + \frac{b}{||w||}) \geq\gamma, i=1,2,...,N\end{aligned}\tag{8}</script><p>也即:</p><script type="math/tex; mode=display">\begin{aligned}    &\max_{w,b}\quad \frac{\hat \gamma}{||w||} \\    &s.t. \quad y_i(\frac{w}{||w||} \cdot x_i + \frac{b}{||w||}) \geq \hat \gamma, i=1,2,...,N\end{aligned}\tag{9}</script><p>我们得知函数间隔$\hat \gamma$的取值并不影响模型的最优化问题，将 $w$ 和 $b$ 成比例的改变 $\lambda$ 倍，函数间隔也变成 $\lambda \hat \gamma$,这一改变对上面最优化的不等式约束并没有印象，因此，我们可以令 $\hat \gamma=1$,于是上述公式就等价于:</p><script type="math/tex; mode=display">\begin{aligned}    & \min_{w,b} \quad \frac{1}{2}||w||^2 \Rightarrow \frac{1}{2}w^T    w\\    & s.t. \quad y_i(w \cdot x_i + b)-1 \geq 0, i=1,2,...,N\end{aligned}\tag{10}</script><p>此时，SVM优化问题变为一个凸二次规划问题，利用拉格朗日乘子法即可求出最优的 $(w^<em>,b^</em>)$</p><h1 id="4-学习的对偶算法"><a href="#4-学习的对偶算法" class="headerlink" title="4 学习的对偶算法"></a>4 学习的对偶算法</h1><p>为求解支持向量机的最优化问题，我们将公式10作为原始问题，应用拉格朗日对偶性，通过求解对偶问题(dual problem)得到原始问题(primal problem)的最优解，这就是支持向量机的对偶算法。这样做的<strong>优点</strong>:</p><ol><li>对偶问题往往更容易求解；</li><li>自然引入核函数，进而推广到非线性可分分类问题；</li></ol><p>通过对公式10的约束条件引入拉格朗日乘子$\alpha_i\geq 0,i=1,2,…,N$,构建出拉格朗日函数：</p><script type="math/tex; mode=display">\begin{aligned}    L(w, b, \alpha)=&\frac{1}{2}w^Tw+\sum_{i=1}^{N}\alpha_i(1-y_i(w^T x_i + b)) \\    =& \frac{1}{2}w^Tw-\sum_{i=1}^{N}\alpha_i y_i(w^T x_i + b) +\sum_{i=1}^{N}\alpha_i\end{aligned}\tag{11}</script><p>我们称公式10为带约束的原始问题，根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题</p><script type="math/tex; mode=display">\max_{\alpha}\min_{w,b}L(w, b, \alpha)\tag{12}</script><p>公式12和原始公式存在一种弱对偶关系，当等号成立时为强对偶关系:</p><script type="math/tex; mode=display">\min_{w,b}\max_{\alpha} L(w, b, \alpha) \geq \max_{\alpha}\min_{w,b} L(w, b, \alpha)\tag{13}</script><p>此时我们就可以按照利用拉格朗日对偶性求解问题的标准模型，求解出$w, b, \alpha$.</p><ol><li><p><strong>求 $\min_{w,b}L(w, b, \alpha)$</strong></p><p>将拉格朗日函数 $L(w, b, \alpha)$ 分别对 $w,b$求偏导，并令其值为0.</p></li></ol><script type="math/tex; mode=display">\begin{aligned}    & \frac{\partial L}{\partial w} = w - \sum_{i=1}^{N}\alpha_i y_i x_i =0 \\    & \Rightarrow w = \sum_{i=1}^{N}\alpha_i y_i x_i\end{aligned}\tag{14}</script><script type="math/tex; mode=display">\begin{aligned}    & \frac{\partial L}{b}=-\sum_{i=1}^{N}\alpha_iy_i=0 \\    & \Rightarrow \sum_{i=1}^{N}\alpha_iy_i=0\end{aligned}\tag{15}</script><p>将公式14和15带入公式11得:</p><script type="math/tex; mode=display">\begin{aligned}    \min_{w,b} L(w, b,\alpha)=&\frac{1}{2} (\sum_{i=1}^{N}\alpha_i y_i x_i)^T \sum_{i=1}^{N}\alpha_i y_i x_i-\sum_{i=1}^{N}\alpha_i y_i((\sum_{i=1}^{N}\alpha_i y_i x_i)^T x_i + b) +\sum_{i=1}^{N}\alpha_i \\    =&\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i y_i\alpha_j y_jx_i^Tx_j-\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i y_i\alpha_j y_jx_j^Tx_i+\sum_{i=1}^{N}\alpha_i \\    =& -\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i y_i\alpha_j y_jx_i^Tx_j+\sum_{i=1}^{N}\alpha_i\end{aligned}\tag{16}</script><ol><li><strong>求 $\min_{w,b}L(w, b,\alpha)$</strong>对 $\alpha$ 的极大值，即对偶问题:</li></ol><script type="math/tex; mode=display">\begin{aligned}    & \max_{\alpha} -\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i y_i\alpha_j y_jx_i^Tx_j+\sum_{i=1}^{N}\alpha_i \\    & s.t. \quad \sum_{i=1}^{N}\alpha_iy_i=0 \\    & \quad \quad \alpha_i \geq 0,i=1, 2...N\end{aligned}\tag{17}</script><p>将上式转化为求极小值</p><script type="math/tex; mode=display">\begin{aligned}    & \min_{\alpha} \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i y_i\alpha_j y_jx_i^Tx_j-\sum_{i=1}^{N}\alpha_i \\    & s.t. \quad \sum_{i=1}^{N}\alpha_iy_i=0 \\    & \quad \quad \alpha_i \geq 0,i=1, 2...N\end{aligned}\tag{18}</script><p>上式可以继续利用凸二次规划来求解 $\alpha^<em>=(\alpha_1^</em>,\alpha_2^<em>,…,\alpha_N^</em>)$,然后可由$\alpha^<em>$求得原始问题对 $(w,b)$ 的解 $w^</em>,b^*$。</p><p><strong>定理</strong> 设 $\alpha^<em>=(\alpha_1^</em>,\alpha_2^<em>,…,\alpha_N^</em>)$ 是对偶问题（即公式18）的解，则存在下标 $j$,使得 $\alpha_j &gt; 0$,并按如下公式求得$w^<em>,b^</em>$</p><script type="math/tex; mode=display">    w^*=\sum_{i=1}^{N} \alpha_i^*y_ix_i    \tag{19}</script><script type="math/tex; mode=display">    b^*=y_j-\sum_{i=1}^{N} \alpha_i^*y_i(x_i^Tx_j)    \tag{19}</script><p><strong>证明</strong><br>根据拉格朗日某定理，KKT条件成立，即:</p><script type="math/tex; mode=display">\begin{aligned}    & \frac{\partial L(w^*,b^*,\alpha ^*)}{\partial w} = w^*-\sum_{i=1}^{N}\alpha_i^* y_i x_i=0 \\    & \frac{\partial L(w^*,b^*,\alpha ^*)}{\partial b} = -\sum_{i=1}^{N}\alpha_i^*y_i=0 \\    & \frac{\partial L(w^*,b^*,\alpha ^*)}{\partial \alpha} = 0 \\    & \alpha_i(1-y_i(w^Tx_i+b)) =0  \\    & \alpha_i \geq 0 , i=1,2,...,N \\    & 1-y_i(w^Tx_i+b) \leq 0 , i=1,2,...,N\end{aligned}\tag{20}</script><p>此时，公式13具有强对偶关系，即等号成立。根据支持向量机的特点，至少存在一个 $\alpha_j &gt;0$,即对于支持向量（后面讲解），对此j有</p><script type="math/tex; mode=display">1-y_j(w^Tx_j+b) = 0 \Rightarrow y_j(w^{*^T}x_j+b) = 1\tag{21}</script><p>由于 $y_j$ 为1或-1，上式两边同乘以一个 $y_i$ 得：</p><script type="math/tex; mode=display">\begin{aligned}    b^* =& y_j - w^{*^T}x_j \\        =& y_j - \sum_{i=1}^{N} \alpha_i^*y_i(x_i^Tx_j)\end{aligned}\tag{22}</script><p>从上面的推导我们可以看出，$w^<em>$ 和 $b^</em>$只依赖于训练数据中对应于 $\alpha_i^<em> &gt; 0$的样本点 $(x_i, y_i)$,而其他样本点对 $w^</em>$ 和 $b^<em>$ 没有印象，我们把这些$\alpha_i^</em> &gt; 0$ 的样本点称为支持向量。这些样本点一定位于间隔边界上。</p><p><a href="https://github.com/RanMaosong/Machine-Learning-LiHang/blob/master/code/%E7%AC%AC%E4%B8%83%E7%AB%A0%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/Plot.ipynb" target="_blank" rel="noopener">文中绘图源码</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2019/03/26/ML-hard-margin-svm/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://www.j
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>最大熵模型</title>
    <link href="http://yoursite.com/2019/03/23/ML-MaxEntropy/"/>
    <id>http://yoursite.com/2019/03/23/ML-MaxEntropy/</id>
    <published>2019-03-23T13:44:38.000Z</published>
    <updated>2019-03-26T14:07:16.377Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2019/03/23/ML-MaxEntropy/" target="_blank" rel="noopener">GitHub</a><br><a href="https://www.jianshu.com/p/b5c57b80ee5f" target="_blank" rel="noopener">简书</a><br><a href="https://blog.csdn.net/u014630987/article/details/88831663" target="_blank" rel="noopener">CSDN</a></p><h1 id="1-最大熵原理"><a href="#1-最大熵原理" class="headerlink" title="1 最大熵原理"></a>1 最大熵原理</h1><p>最大熵模型（Maximum Entropy Model)是通过最大熵原理推导实现,那什么是最大熵原理？</p><p>熵是随机变量不确定性大的度量，不确定性越大，熵值越大；若随机变量变为定值，即某个值发生的概率为1，而其它事件都为0， 此时熵值为0，均匀分布是熵值最大的分布，也即“最不确定的分布”。</p><p>假设离散随机变量 $X$ 的概率分布是 $P(X)$,则其熵为:</p><script type="math/tex; mode=display"> H(P)=-\sum_{x}p(x)logp(x) \tag{1}</script><p>熵满足如下条件:</p><script type="math/tex; mode=display">0 \leq H(P) \leq log|X|</script><p>其中,$|X|$ 表示 $X$ 的取值，当 $X$ 的分布是均匀分布时，满足左边等号，当 $X$ 是确定事件时，满足左边的等号。</p><p>从上面可知，最大熵原理认为该求解的概率模型满足如下条件:</p><ol><li>满足事先已约束的条件；</li><li>然后在满足这些条件的模型中选择熵最大的模型，即让不确定的信息等可能的发生；</li></ol><h1 id="2-最大熵模型"><a href="#2-最大熵模型" class="headerlink" title="2. 最大熵模型"></a>2. 最大熵模型</h1><p>将最大熵原理应用到分类问题中即得到最大熵模型。假设分类模型的一个条件概率分布 $P(Y|X)$, $X\in\chi\subseteq R^n$ 表示输入，$X\in\gamma$表示输出。这个模型表示给定的输入 $X$,以条件概率$P(Y|X)$输出$Y$。</p><p>给定一个训练数据集</p><script type="math/tex; mode=display">T=\{(x_1, y_1), (x_2,y_2)...(x_N, y_N)\}</script><p>学习的目标是用最大熵原理选择最好的模型。</p><p>对于给定训练数据集，我们可以确定联合分布$P(X, Y)$的经验分布$\tilde P(X,Y)$和边缘分布 $P(X)$ 的经验分布 $\tilde P(X)$,即：</p><script type="math/tex; mode=display">\tilde P(X=x, Y=x) = \frac{v(X=x, Y=y)}{N} \\\tilde P(X=x) = \frac{X(X=x)}{N}\tag{2}</script><p>其中，$v(X=x, Y=y)$ 表示训练数据中样本$(x, y)$出现的频数， $V(X=x)$表示训练数据集中 $x$ 出现的频数。 $N$ 表示训练样本的总容量。</p><p><strong>特征函数$f(x, y)$</strong> 表示输入 $x$ 和输出$y$ 之间的某个约束。其定义为:</p><script type="math/tex; mode=display">f(x,y)=\begin{cases}    1,\quad x和y满足约束\\    0, \quad x和y不满足约束\end{cases}\tag{3}</script><p>特征函数 $f(x,y)$ 关于经验分布 $\tilde P(X, Y)$的期望值为:</p><script type="math/tex; mode=display">E_{\tilde p}(f) = \sum_{x, y} \tilde P(x,y)f(x, y)\tag{4}</script><p>特征函数 $f(x,y)$ 关于模型 $P(Y|X)$ 和经验分布 $\tilde P(X)$的期望值为:</p><script type="math/tex; mode=display">E_{p}(f) = \sum_{x, y} \tilde P(x)P(y|x)f(x, y)\tag{5}</script><p>因为机器学习的目的就是从数据集中学得数据中包含的某种内在信息，因此我们可以假设公式4和公式5相等，即</p><script type="math/tex; mode=display">E_{\tilde p}(f) = E_{p}(f) \\\sum_{x, y} \tilde P(x,y)f(x, y) = \sum_{x, y} \tilde P(x)P(y|x)f(x, y)\tag{6}</script><p>公式6就作为模型的约束条件，如果有 n 个特征函数，则就有 n 个约束条件。</p><p><strong>最大熵模型</strong> 假设满足所有约束条件的模型集合为</p><script type="math/tex; mode=display">C=\{P|E_{\tilde p}(f_i) = E_{p}(f_i), i=1,2...n\}</script><p>定义在条件概率分布 $P(Y|X)$ 上的条件熵为:</p><script type="math/tex; mode=display">H(P) = - \sum_{x,y}\tilde P(x)P(y|x)logP(y|x)\tag{7}</script><p>则模型集合 $C$ 条件熵最大的模型成为<strong>最大熵模型</strong></p><p><strong>补充</strong></p><p>条件概率的熵的公式为</p><script type="math/tex; mode=display">H(y|x)=-\sum_{x,y}p(x,y)logp(y|x)\tag{8}</script><p>因此最大熵模型如公式7所示。</p><p>总之，最大熵模型就是在满足约束的模型集合中选择条件概率分布 $P(Y|X)$ 最大的模型。</p><h1 id="3-最大熵模型的学习"><a href="#3-最大熵模型的学习" class="headerlink" title="3. 最大熵模型的学习"></a>3. 最大熵模型的学习</h1><p>通过上述上述的描述，最大熵模型可以形式化为约束最优化问题，即</p><script type="math/tex; mode=display">\max_{P \in C} H(P) = -\sum_{x,y}\tilde P(x)P(y|x)\log P(y|x) \\s.t. \quad E_{\tilde p}(f_i) = E_{p}(f_i), \quad i=1, 2...,n \\\quad \sum_yP(y|x) = 1\tag{9}</script><p>按照优化习惯，通常将最大值优化转换为最小值优化。即</p><script type="math/tex; mode=display">\max_{P \in C} -H(P) = \sum_{x,y}\tilde P(x)P(y|x)\log P(y|x) \\s.t. \quad E_{\tilde p}(f_i) = E_{p}(f_i), \quad i=1, 2...,n \\\quad \sum_yP(y|x) = 1\tag{10}</script><p>公式10所得出的解就是最大熵模型学习的模型。</p><p>解决上述约束最优化问题，我们通过拉格朗日对偶性来进行解决。</p><p>首先我们引入拉格朗日乘子$w_0, w_1,w_2…w_n$,定义拉格朗日函数 $L(p, w)$为</p><script type="math/tex; mode=display">    \begin{aligned}        L(P, w) &=-H(P) + w_0(1-\sum_yP(y|x))+\sum_{i=1}^{n}w_i(E_{\tilde p}(f_i) - E_{p}(f_i)) \\        &=\sum_{x,y}\tilde P(x)P(y|x)\log P(y|x) + w_0(1-\sum_yP(y|x)) \\        &+\sum_{i=1}^{n}w_i(\sum_{x,y}\tilde P(x,y)f(x, y) - \sum_{x, y} \tilde P(x)P(y|x)f(x, y))    \end{aligned}    \tag{11}</script><p>因此，最优化问题的原始问题为</p><script type="math/tex; mode=display">\min_{P\in C} \max_{w} L(P,w)\tag{12}</script><p>对偶问题</p><script type="math/tex; mode=display">\max_{w} \min_{P\in C}  L(P,w)\tag{13}</script><p>我们称公式10、11和12为原始问题，公式13为原始问题的对偶问题，且原始问题的解与对偶问题的解是等价的，因此，公式11的解就是我们求解的模型。</p><p>我们首先求解对偶问题公式13内部的极小化问题$\min_{P\in C}  L(P,w)$,该函数是关于 $w$ 的函数，我们将其记作:</p><script type="math/tex; mode=display">\psi (w) = \min_{P\in C}  L(P,w) = L(P_w, w)\tag{14}</script><p>$\psi (w)$ 称为对偶函数，同时其解记为:</p><script type="math/tex; mode=display">P_w = \arg \min_{p}L(P,w) = P_w(y|x)\tag{15}</script><p>我们可以利用偏导数来求解公式15，即</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial L(P,w)}{\partial P(y|x)} &= \sum_{x,y}(\tilde P(x) \log P(y|x) + \tilde{P}(x)) -\sum_{y}w_o+\sum_{i=1}^{n}w_i(-\sum_{x,y}\tilde{P}(x)f_{i}(x,y)) \\&= \sum_{x,y}\tilde{P}(x)(\log P(y|x) + 1) - \sum_{y}w_0-\sum_{x,y}{\tilde{P}(x)\sum_{i=1}^nw_if_i(x,y)} \\&=\sum_{x,y}\tilde{P}(x)(\log P(y|x) + 1) - \sum_x \tilde{P}(x)\sum_{y}w_0-\sum_{x,y}{\tilde{P}(x)\sum_{i=1}^nw_if_i(x,y)} \\&= \sum_{x,y}\tilde{P}(x)(\log P(y|x) + 1-w_0-\sum_{i=1}^nw_if_i(x,y))\end{aligned}\tag{16}</script><p>由于 $L(P,w)$是凸函数，我们我可令上式偏导数为0，在$\tilde P(x)&gt;0$的情况下，即可求出$P(y|x)$, 即：</p><script type="math/tex; mode=display">\begin{aligned}    P(y|x) &=\exp(\sum_{i=1}^{n}w_if_i(x,y)+w_0-1)\\        &=\frac{\exp{\sum_{i=1}^{n}w_if_i(x,y)}}{\exp(1-w_0)}\end{aligned}\tag{17}</script><p>由于在概率论中，$\sum_{y}P(y|x)=1$,因此需对公式17进行归一化，又$\exp(1-w_0)$为常数,因此：</p><script type="math/tex; mode=display">P_w(y|x)=\frac{1}{Z_w(x)}\exp(\sum_{i=1}^{n}w_if_i(x,y))\tag{18}</script><p>其中</p><script type="math/tex; mode=display">Z_w(x)=\sum_{y}\exp(\sum_{i=1}^nw_if_i(x,y))\tag{19}</script><p>公式18、19表示的模型就是最大熵模型$P_w=p_w(y|x)$.然后求解对偶函数的极大化问题</p><script type="math/tex; mode=display">\max_{w} \psi (w) \tag{20}</script><p>求得$w^*$,得到最终模型。</p><h1 id="4-极大似然估计"><a href="#4-极大似然估计" class="headerlink" title="4 极大似然估计"></a>4 极大似然估计</h1><p>通过上面一小节的计算，我们已经求出最大熵模型，但是此时该模型还是一个关于 $w$ 的函数，我们如何求出 $w$ 来求得最终的模型呢。</p><p>我们先来描述对数似然函数，通过前面一章的逻辑回归模型中的<a href="https://ranmaosong.github.io/2019/03/18/ML-ligistic-regression/" target="_blank" rel="noopener">最大似然估计</a>，我们可知对数似然函数和熵在值上互为相反数，又通过公式8得知条件概率的熵形式，因此我们可以得知，条件概率分布$P(Y|X)$的对数似然函数</p><script type="math/tex; mode=display">    L_{\tilde P}(P_w) = \sum_{x,y}\tilde{P}(x,y)\log P(y|x)    \tag{21}</script><p>我们再冲似然函数的定义方面证明上述公司的正确性。</p><p>在给定数据集$\{(x_1,y_1),(x_2,y_2)…(x_n,y_n)\}$,我们可求得当前模型的似然函数为:</p><script type="math/tex; mode=display">L(\theta)=\prod_{i=1}^n P(x_i, \theta)\tag{22}</script><p>我们假设$X_i$在训练集中出现了$C(x_i)$,因此公式22可以转化为:</p><script type="math/tex; mode=display">L(\theta)=\prod_{i=1}^k P(x_i, \theta)^{C(x_i)}\tag{23}</script><p>$k$ 表示训练数据集中总共有 k 种不同的输入特征，我们对上市求其 $\frac{1}{n}$次方，得：</p><script type="math/tex; mode=display">L(\theta)^{\frac{1}{n}}=\prod_{i=1}^k P(x_i, \theta)^{\frac{C(x_i)}{n}}\tag{24}</script><p>对公式24求对数的:</p><script type="math/tex; mode=display">\begin{aligned}  \log L(\theta)^{\frac{1}{n}} &=\log \prod_{i=1}^k P(x_i, \theta)^{\frac{C(x_i)}{n}}  \\  &= \sum_{i=1}^k \frac{C(x_i)}{n} \log P(x_i, \theta)\\\end{aligned}\tag{25}</script><p>因此对于$\log L(\theta)^{\frac{1}{n}}$ 和 $\log L(\theta)$是等价的，</p><p>因此对于条件概率分布$P(Y|X)$的对数似然函数</p><script type="math/tex; mode=display">    L_{\tilde P}(P_w) = \sum_{x,y}\tilde{P}(x,y)\log P(y|x)    \tag{26}</script><p>对于公式21-26的推导，不具有严格的树学理论，只是为了理解下面公式27做铺垫。</p><p>一直训练数据的经验概率分布$\tilde P(X,Y),$条件概率分布$P(Y|X)$的对数似然函数表示为</p><script type="math/tex; mode=display">\begin{aligned}    L_{\tilde P}(P_w) &=\log \prod_{xy}P(y|x)^{\tilde P(x,y)} = \sum_{x,y}\tilde{P}(x,y)\log P(y|x)\end{aligned}\tag{27}</script><p>将公式18和19带入公式27可得:</p><script type="math/tex; mode=display">\begin{aligned}    L_{\tilde P}(P_w) &=\sum_{x,y}\tilde{P}(x,y)(\sum_{i=1}^{n}w_if_i(x,y)-\log Z_w(x) \\    &=\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^{n}w_if_i(x,y)-\sum_{x,y}\tilde{P}(x,y)\log Z_w(x) \\    &=\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^{n}w_if_i(x,y)-\sum_{x}\tilde {P}(x)\log Z_w(x)\end{aligned}\tag{28}</script><p>公式28就是极大似然函数。上式最后两步最后一项的转化是因为 $Z_w(x)$ 是关于 x 的函数，所以可以对 $\tilde{P}(x,y)$ 对 $x$ 进行累加得到 $\tilde {P}(x)$.</p><p>我们再看看对偶函数 $\psi (w)$,我们将 $P_w(y|x)$ 带入公式11得:</p><script type="math/tex; mode=display">\begin{aligned}    \psi (w) =& \sum_{x,y}\tilde P(x)P_w(y|x)\log P_w(y|x) + w_0(1-\sum_yP(y|x)) \\        &+\sum_{i=1}^{n}w_i(\sum_{x,y}\tilde P(x,y)f(x, y) - \sum_{x, y} \tilde P(x)P_w(y|x)f(x, y)) \\        =&\sum_{x,y}\tilde P(x)P_w(y|x)\log P_w(y|x)  \\        &+\sum_{i=1}^{n}w_i(\sum_{x,y}\tilde P(x,y)f(x, y) - \sum_{x, y} \tilde P(x)P_w(y|x)f(x, y)) \\        =& \sum_{x,y}\tilde P(x,y)\sum_{i=1}^{n}w_if(x, y)+\sum_{x,y}\tilde{P}(x)P_w(y|x)(\log P_w(y|x)-\sum_{i=1}^{n}w_if_i{(x,y)}) \\        =& \sum_{x,y}\tilde P(x,y)\sum_{i=1}^{n}w_if(x, y)-\sum_{x,y}\tilde{P}(x)P_w(y|x)\log Z_w(x) \\        =& \sum_{x,y}\tilde P(x,y)\sum_{i=1}^{n}w_if(x, y)-\sum_{x,y}\tilde{P}(x)\log Z_w(x)\end{aligned}\tag{29}</script><p>公式29第三步到第四步用到下面公式进行推导:</p><script type="math/tex; mode=display"> P_w(y|x)=\frac{1}{Z_w(x)}\exp(\sum_{i=1}^{n}w_if_i(x,y)) \Rightarrow \log P_w(y|x)=\sum_{i=1}^{n}w_if_i(x,y)-\log Z_w(x)</script><p>倒数第二步到最后一步的推导用到了 $\sum_{y}P(y|x)=1$.</p><p>比较公式公式28和公式29，可得:</p><script type="math/tex; mode=display">\psi (w) = L_{\tilde P}(P_w)</script><p>因此，可以证明，最大熵模型学习中的对偶函数极大化等价于最大熵模型的极大似然估计。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2019/03/23/ML-MaxEntropy/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://www.jiansh
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>逻辑(斯谛)回归(Logistic Regression)</title>
    <link href="http://yoursite.com/2019/03/18/ML-ligistic-regression/"/>
    <id>http://yoursite.com/2019/03/18/ML-ligistic-regression/</id>
    <published>2019-03-18T06:14:13.000Z</published>
    <updated>2019-03-21T14:21:47.653Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2019/03/18/ML-ligistic-regression/" target="_blank" rel="noopener">GitHub</a><br><a href="https://www.jianshu.com/p/9f723c2ac52e" target="_blank" rel="noopener">简书</a><br><a href="https://blog.csdn.net/u014630987/article/details/88687924" target="_blank" rel="noopener">CSDN</a></p><p>在我们学习机器学习的过程中，我们所需解决的问题，大致可以分为两部分：分类和回归.其中,分类是指模型用来预测一个有限的离散值集合中的一个，比如猫狗分类，肿瘤的恶性或良性; 回归是指模型的输出是一个连续变量，比如预测房价、身高等.本篇内容讲解的是机器学习中经典的逻辑(斯谛)回归（Logistic Regression)，从名字上看，大家误以为该方法是一种回归方法，其实不然，它是分类方法的一种，常用于二元分类，但是为什么会取名回归，我个人理解大致有如下几点原因：</p><pre><code>1. 利用回归的思想来解决分类问题;2. 它的输出也是一个连续值，通过设定阈值来实现分类 </code></pre><h1 id="1-逻辑斯谛分布"><a href="#1-逻辑斯谛分布" class="headerlink" title="1. 逻辑斯谛分布"></a>1. 逻辑斯谛分布</h1><p><strong>定义</strong>：设X是连续随机变量，X服从逻辑斯谛分布是指X具有下列分布函数和密度函数：</p><script type="math/tex; mode=display">    F(x)=P(X \leq x)=\frac{1}{1+e^{-(x-u)/\gamma}} \tag{1}</script><script type="math/tex; mode=display">    f(x)=F^{'}(x)=\frac{e^{-(x-\mu)\gamma}}{\gamma(1+e^{-(x-u)/\gamma})^2} \tag{2}</script><p>其中,$\mu$为位置参数,$\gamma &gt; 0$为形状参数.</p><p>该函数以点$(\mu, \frac{1}{2})$为中对称，既有如下关系：</p><script type="math/tex; mode=display">\begin{aligned}F(-x+\mu) &= 1 - F(x+\mu)\\F(-x+\mu)-\frac{1}{2} &=  F(x + \mu) + \frac{1}{2}\end{aligned}\tag{3}</script><p>形状参数$\gamma$的值越小，曲线在中心附近增长的越快.该函数的图形如下图所示：</p><p><img src="/images/logistic_regression_sigmoid.png" alt="Sigmoid"><br>图一 逻辑斯谛分布的分布函数和密度函数</p><h1 id="2-二元逻辑斯谛回归"><a href="#2-二元逻辑斯谛回归" class="headerlink" title="2 二元逻辑斯谛回归"></a>2 二元逻辑斯谛回归</h1><p>二元逻辑斯谛回归模型是一种分类模型，有条件概率分布$P(Y|X)$表示，X取值为实数，随机变量 Y 取值为 1或0；</p><p><strong>逻辑斯谛回归模型</strong>的条件概率如下：</p><script type="math/tex; mode=display">\begin{aligned}    p(Y=1|x)&=\frac{exp(w\cdot x+b)}{1+exp(w\cdot x+b)}=\frac{1}{1+exp(-(w\cdot x+b))} \\    P(Y=0|x)&=\frac{1}{1+exp((w\cdot x+b))}\end{aligned}    \tag{4}</script><p>这里, $ x \in R^n $表示样本的特征向量，$Y \in {0, 1}$是输出表示样本的类别, $w \in R^n$ 和 $ b \in R$是模型的参数，其中，$w$ 表示权重向量,$b$表示偏置。$w \cdot x$表示$w$和$x$的内积.通常为了方便，我们将样本和权重向量进行扩充，仍记作$w$和$b$：</p><script type="math/tex; mode=display">w = (w^1, w^2... w^n, b)</script><script type="math/tex; mode=display">x = (x^1, x^2...x^n, 1)</script><p>此时逻辑斯蒂回归模型记作:</p><script type="math/tex; mode=display">\begin{aligned}    p(Y=1|x)&=\frac{exp(w\cdot x)}{1+exp(w\cdot x)} \\    P(Y=0|x)&=\frac{1}{1+exp((w\cdot x))}\end{aligned}\tag{5}</script><p><strong>几率</strong>是指一个事件发生与不发生的概率比值,即<br>$<br>\frac{p}{1-p}<br>$<br>则它的对数几率为$lnit(p)=log \frac{p}{1-p}$,对于逻辑斯蒂回归回归而言，其对数几率为</p><script type="math/tex; mode=display">logit(\frac{P(Y=1|x)}{1-P(Y=1|x)})=w \cdot x \tag{6}</script><h1 id="3-模型参数估计"><a href="#3-模型参数估计" class="headerlink" title="3 模型参数估计"></a>3 模型参数估计</h1><p>对于给定的训练数据集$T=\{(x_1, y_1), (x_2, y_2)…(x_n, y_x)\}$,可以应用极大似然估计(使模型预测的标签为真是标签的值最大化)模型参数，从而得到最优的逻辑斯蒂回归模型。</p><p>首先，设$P(Y=1|x)=\pi(x), P(Y=0|x)=1-\pi(x)$,则似然函数为:</p><script type="math/tex; mode=display">\begin{aligned}\prod_{i=1}^n[\pi(x)]^{y_i}[1-\pi(x)]^{1-y_i}=\prod_{i=1}^n{y_i\pi(x_i)+(1-y_i)(i-\pi(x_i))}\end{aligned}\tag{7}</script><p>极大似然函数和交叉熵的树学公式形式时一摸一样的，但是他们背后的数学原理略有不同。<br>通常在处理优化问题时，我们都利用对数函数来把连乘变成求和来简化问题，因此公式七的对数似然函数为：</p><script type="math/tex; mode=display">\begin{aligned}L(w) &= \sum_{i=1}^{n}[y_i ln \pi(x_i)+(1-y_i)ln(1-\pi(x_i))]\\    &=\sum_{i=1}^{n}[y_i ln \pi(x_i) - y_i ln(1-\pi(x_i)) + ln(1-\pi(x_i))] \\    &=\sum_{i=1}^{n}[y_iln\frac{\pi(x_i)}{1-\pi(x_i)}(注：这就是对数几率值）+ln(1-\pi(x_i))] \\    &=\sum_{i=1}^{n}[y_i(w*x_i)+ln(\frac{1}{1+exp((w\cdot x))})] \\    &=\sum_{i=1}^{n}[y_i(w*x_i)-ln(1+exp(w\cdot x_1))]\end{aligned}\tag{8}</script><p>通过梯度下降和拟牛顿法即可求的该函数，我们求$L(w)$对$w$的倒数:</p><script type="math/tex; mode=display">\begin{aligned}    \frac{\partial L(w)}{\partial w}&=\sum_{i=1}^{n}[y_ix_i-(\frac{1}{1+exp(w\cdot x_1)}*exp(w \cdot x_i)) * x_i]\\        &=\sum_{i=1}^{n}[y_ix_i-\pi(x_i)*x_i]\end{aligned}\tag{9}</script><p>通常我们在实际优化的时候，都是求取最小值，因此通常使用$-L(w)$作为损失函数.</p><p><strong>问题</strong>:在机器学习或深度学习中,我们通常以$L_2$作为损失函数，但是为什么这里是用了极大似然估计？</p><p>我们先观察一下使用$L_2$范数作为损失函数时，对$w$的求导公式:</p><script type="math/tex; mode=display">\begin{aligned}    L_2(w)&=\frac{1}{2}\sum_{i=1}^{n}(\pi(x_i) - y_i)^2 \\    \frac{\partial l_2(w)}{\partial w} &=\sum_{i=1}^{n}[(\pi(x_i) - y_i)*\frac{\partial\pi (x_i)}{\partial z} * \frac{\partial z}{\partial w}] \\    &=\sum_{i=1}^{n}[(\pi(x_i) - y_i)\pi(x_i) (1-\pi(x_i))  x_i]\end{aligned}\tag{10}</script><p>其中, $z=w \cdot x$, 则$\pi (x) = \frac{exp(x)}{1+exp(x)}$,其导数为$\pi^{‘} (x)=\pi(x)(1-\pi(x))$</p><p>这里主要考虑的是优化问题,极大似然估计函数是一个凸函数,这是优化问题再最容易优化的模型，我们可以得到全局最优解，而对于$L_2$，由于Sigmoig函数导数的特性，当$\pi (x)$接近0或者1时，此时的倒数就接近0，从而容易使函数陷入局部最优.</p><p>下图是两个损失函数以w为参数的简化图<br><img src="/images/logistic_regression_l2.png" alt="L2"><br><img src="/images/logistic_regression_cross_entropy.png" alt="likelihood"></p><h1 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h1><p><a href="https://github.com/RanMaosong/Machine-Learning-LiHang/blob/master/code/%E7%AC%AC%E5%85%AD%E7%AB%A0%20%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/plot.ipynb" target="_blank" rel="noopener">绘图源码</a></p><p>逻辑斯蒂回归实现源码: </p><p><a href="https://github.com/RanMaosong/Machine-Learning-LiHang/blob/master/code/%E7%AC%AC%E5%85%AD%E7%AB%A0%20%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/LogisticRegression.py" target="_blank" rel="noopener">Python</a></p><p><a href="https://github.com/RanMaosong/Machine-Learning-LiHang/blob/master/code/%E7%AC%AC%E5%85%AD%E7%AB%A0%20%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/LogisticRegression.ipynb" target="_blank" rel="noopener">Jupyter</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2019/03/18/ML-ligistic-regression/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://w
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>GloVe:Global Vectors for Word Representation</title>
    <link href="http://yoursite.com/2018/11/21/nlp-glove/"/>
    <id>http://yoursite.com/2018/11/21/nlp-glove/</id>
    <published>2018-11-21T14:33:10.000Z</published>
    <updated>2018-11-22T07:14:58.614Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2018/11/21/nlp-glove/" target="_blank" rel="noopener">Github</a><br><a href="https://www.jianshu.com/p/a6fb7e3530a0" target="_blank" rel="noopener">简书</a><br><a href="https://blog.csdn.net/u014630987/article/details/84339695" target="_blank" rel="noopener">CSDN</a></p><p>词作为自然语言处理中的一个基本单元，如何表示一个词对于后续的处理任务至关重要，最简单的表示方式是 One-hot，但是该方法表示的词之间是相互独立的，因此局限性很大，需要采用一种更合理的表示方法。</p><h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h1><p>目前，学习词向量的方法主流的有两种：</p><ol><li><p>全局矩阵分解的方法，比如LSA，HAL，这类方法首先统计语料库中的“词-文档”或者“词-词”共现矩阵，然后通过矩阵分解的方法来获得一个低维词向量。“词-文档”矩阵是指矩阵的行表示词，列表示文档，矩阵的元素是改词在文档中出现的次数。“词-词”共现矩阵的行和列都表示一个矩阵，</p></li><li><p>局部上下文窗口的方法， 比如skip-gram，通过神经网络的方法使语料库中给定上下文中共同出现的单词对的概率更大</p></li></ol><p>但是这两种方法都有一个缺点全局矩阵分解的方法虽然利用了全局统计信息，但是他会过度重视共现词频高的单词对，然而这些词并没有多大的语义联系，局部上下文窗口的方法虽然在词类比方面的任务表现的很好，但是没有充分利用全局统计信息。</p><p>该篇文章的主要思想就是两者取其长，结合各自的优点进行词向量学习。</p><h1 id="2-Glove"><a href="#2-Glove" class="headerlink" title="2.Glove"></a>2.Glove</h1><p>首先我们先定义一些符号</p><p>$X$ : 表示“词-词”共现矩阵，是一个对阵矩阵</p><p>$X_{ij}$: 表示词 j 出现在中心词 i 的上下文（基于窗口）的次数。</p><p>$X_i$: 表示任何词出现在词 i 上下文的总的次数。</p><p>$P_{ij}=\frac{X_{ij}}{X_i}$: 表示单词 j 出现在 单词 i 的上下文的概率。</p><p>假设我们有一个检测词 k，则存在如下关系</p><div class="table-container"><table><thead><tr><th>$ratio=\frac{P_{ik}}{P_{jk}}$</th><th style="text-align:right">j,k 相关</th><th style="text-align:center">j,k不相</th></tr></thead><tbody><tr><td>i,k相关</td><td style="text-align:right">1</td><td style="text-align:center">非常大</td></tr><tr><td>i,k不相关</td><td style="text-align:right">非常小</td><td style="text-align:center">1</td></tr></tbody></table></div><p>上面表格的意思就是当词 i，k相关时，如果j，k相关，则$ratio=\frac{P_{ik}}{P_{jk}}$接近于1，如果j，k不想管，此时$P_{jk}$非常小，则$ratio=\frac{P_{ik}}{P_{jk}}$的值会很大，如果i，k不相关，我们可以依次类推出上面的结果。</p><p>如果我们单纯地预测$P_{ij}$，此时就变回skip-gram，我们如何利用全局统计信息呢，此时我们可以利用上面表格思想，我们预测概率的比率，即：</p><script type="math/tex; mode=display">F(w_i, w_j, \hat w_k)=\frac{P_{ik}}{P_{ij}} \tag{1}</script><p>其中，$w$表示一个d维的词向量，$\hat w$ 表示上下文检测词。这里的F存在很多解，但是我们可以一步步地添加约束来得到最后的一个解</p><p>因为向量空间具有线性结构，因此我们只考虑函数 $F$ 在目标词的差异上进行运算（这个原因个人觉得理由有点牵强）</p><script type="math/tex; mode=display">F(w_i-w_j, \hat w_k)=\frac{P_{ik}}{P_{ij}} \tag{2}</script><p><strong>这句话是个人的理解</strong>：个人感觉这一部还有个原因是为了减少计算量。</p><p>从上面公式我们可以看到公式左边是个向量，右边是一个标量，因此我们可以使用向量的点乘来解决这个问题。</p><script type="math/tex; mode=display">F((w_i-w_j)^T\hat w_k)=\frac{P_{ik}}{P_{ij}}  \tag{3}</script><p>即</p><script type="math/tex; mode=display">F(w_i^T\hat w_k-w_j^T\hat w_k)=\frac{P_{ik}}{P_{ij}}  \tag{4}</script><p>在这里我们需要寻找一个在 + 和 × 之间的同态函数，比如：</p><script type="math/tex; mode=display">F(w_i^T\hat w_k-w_j^T\hat w_k)=\frac{F(w_i^T\hat w_k)}{F(w_j^T\hat w_k)} \tag{5}</script><p>该式在结合上面地公式(4)：即</p><script type="math/tex; mode=display">\frac{F(w_i^T\hat w_k)}{F(w_i^T\hat w_k)}=\frac{P_{ik}}{P_{ij}} \tag{6}</script><p>因此：</p><script type="math/tex; mode=display">F(w_i^T\hat w_k)=P_{ik}=\frac{X_{ik}}{X_i} \tag{7}</script><p>对于满足公式(5)的函数，我们可以想到有指数函数 exp，即：</p><script type="math/tex; mode=display">\exp(w_i^T\hat w_k-w_j^T\hat w_k)=\frac{\exp(w_i^T\hat w_k)}{\exp(w_j^T\hat w_k)} \tag{8}</script><p>结合公式(7)和(8)我们可以得到</p><script type="math/tex; mode=display">exp(w_i^T\hat w_k)=P_{ik}=\frac{X_{ik}}{X_i} \tag{9}</script><p>我们对其取对数可以得到：</p><script type="math/tex; mode=display">w_i^T\hat w_k=\log(P_{ik})=\log(X_{ik}) - \log(X_i) \tag{10}</script><p>由于上面左边具有交换性，而右边不具有，同时$X_i$和k无关因此可以作为一个偏织项，因此公式(10)可以演变为</p><script type="math/tex; mode=display">w_i^T\hat w_k + b_i + \hat b_k=log(X_{ik}) \tag{11}</script><p>其中$\hat b_k$ 作为 k 的偏置项来保持公式的对称性。</p><p>同事为了防止$X_{ik}$接近0时，log 趋向于负无穷大，因此我们对右式加上一个平滑项，即$\log(X_{ik}) \to log(1+X_{ik})$</p><p>因此该模型的损失函数为：</p><script type="math/tex; mode=display">J = \sum _{i,j}^{V} (w_i^T\hat w_k + b_i + \hat b_k-log(X_{ik}) )^2 \tag{12}</script><p>在这里有个问题就是他对每个单词对平等对待，但是这是不合理的，应该对 $X_{ij}$值较大的，权重应该更大，同时，我们又不应该过度重视高频词，该论文选取的权重函数如下：</p><script type="math/tex; mode=display">f(x)=\begin{cases}-(x/x_{max})^\alpha,\quad if x<x_{max} \\1, otherwise\end{cases}\tag{13}</script><p>最终该损失函数如下：</p><script type="math/tex; mode=display">J = \sum _{i,j}^{V} f(X_{ij})(w_i^T\hat w_k + b_i + \hat b_k-log(X_{ik}) )^2 \tag{14}</script><p>论文中$\alpha$ 和 $x_{max}$分别取值$3/4$ 和 100.</p><h1 id="3-和Skip-gram的联系"><a href="#3-和Skip-gram的联系" class="headerlink" title="3. 和Skip-gram的联系"></a>3. 和Skip-gram的联系</h1><p>前面提过，Glove 结合了之前两种方法的优点，但是他是如何结合或者如何从Skip-gram演变而来的呢？</p><p>总所周知，Skip-gram模型可以通过如下一个公式来表达，即Softmax：</p><script type="math/tex; mode=display">Q_{ij}=\frac{exp(w_i^T\hat w_j)}{\sum_{k=1}^V exp(w_i^T\hat w_k)} \tag{15}</script><p>则该模型的损失函数为：</p><script type="math/tex; mode=display">J =-\sum_{i \in corpus, j \in context(i)} \log Q_{ij} \tag{16}</script><p>由于我们需要利用全局统计信息，因此一个单词对可能出现很多次，因此，我们首先把所有相同的单词对先进行计算，即</p><script type="math/tex; mode=display">\begin{align}J &= -\sum_{i=1}^V\sum_{j=1}^V X_{ij} \log Q_{ij}\\&=-\sum_{i=1}^V X_i \sum_{j=1}^V P_{ij} \log Q_{ij}\\&=\sum_{i=1}^V X_iH(P_i, Q_i)\end{align}\tag {17}</script><p>由于交叉熵具有某些缺点，交叉熵具有长尾效应，当过分重视不太可能的事件时建模效果不是很好，而且交叉熵需要Q归一化，这样计算很大，因此使用这样一个未归一化的最小二成损失来替代 </p><script type="math/tex; mode=display">\hat J = \sum_{i,j} X_i (X_{ij} - \exp(w_i^T \hat w _j))^2\tag{18}</script><p>这里是因为18式中当$X_{ij}$变得很大时，训练变得复杂，于是使用对数可以缓解或补交这个问题,即：</p><script type="math/tex; mode=display">\hat J = \sum_{i,j} X_i (w_i^T \hat w _j- \log X_{ij} )^2\tag{19}</script><p>这里$X_i$可以视为一个权重项，我们通过调整这个权重和添加权重项公式19就演变为公式14.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/11/21/nlp-glove/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://www.jianshu.co
      
    
    </summary>
    
      <category term="NLP(自然语言处理)" scheme="http://yoursite.com/categories/NLP-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="NLP(自然语言处理)" scheme="http://yoursite.com/tags/NLP-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>util.md</title>
    <link href="http://yoursite.com/2018/11/05/util-md/"/>
    <id>http://yoursite.com/2018/11/05/util-md/</id>
    <published>2018-11-05T12:43:15.000Z</published>
    <updated>2018-11-05T12:47:06.151Z</updated>
    
    <content type="html"><![CDATA[<p>收集一些常用的工具包教程链接：</p><ol><li>[Visdom]:      <a href="https://zhuanlan.zhihu.com/p/32025746(知乎" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32025746(知乎</a>)</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;收集一些常用的工具包教程链接：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;[Visdom]:      &lt;a href=&quot;https://zhuanlan.zhihu.com/p/32025746(知乎&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://zhu
      
    
    </summary>
    
      <category term="Util" scheme="http://yoursite.com/categories/Util/"/>
    
    
      <category term="Util" scheme="http://yoursite.com/tags/Util/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode 172.FactorialTrailingZeroes(阶乘后的零)</title>
    <link href="http://yoursite.com/2018/10/07/LeetCode-172-FactorialTrailingZeroes/"/>
    <id>http://yoursite.com/2018/10/07/LeetCode-172-FactorialTrailingZeroes/</id>
    <published>2018-10-07T09:14:00.000Z</published>
    <updated>2018-10-07T11:00:04.527Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2018/10/07/LeetCode-172-FactorialTrailingZeroes/" target="_blank" rel="noopener">GitHub链接</a><br><a href="https://www.jianshu.com/p/2d441cf3f049" target="_blank" rel="noopener">简书链接</a><br><a href="https://blog.csdn.net/u014630987/article/details/82960232" target="_blank" rel="noopener">CSDN链接</a></p><h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><p>给定一个整数 n， 返回 $n!$ 结果尾数中零的个数。<br><strong>示例1</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入: 3</span><br><span class="line">输出: 0</span><br><span class="line">解释: 3! = 6, 尾数中没有零。</span><br></pre></td></tr></table></figure></p><p><strong>示例2</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入: 5</span><br><span class="line">输出: 1</span><br><span class="line">解释: 5! = 120, 尾数中有 1 个零.</span><br></pre></td></tr></table></figure></p><p><strong>说明:</strong> 算法的时间复杂度应为 $O(log n)$</p><h1 id="方法一（错误的）"><a href="#方法一（错误的）" class="headerlink" title="方法一（错误的）"></a>方法一（错误的）</h1><p>求出 $n!$ 的值或者在求的过程中，遇到末尾有0，先除以10来减小数，虽然该算法原理上可以行，但是有两个问题，一、时间复杂度为 $O(n)$，二、结果不对，出现这个问题的原因是编程语言每种类型有自己的数组范围，因此会溢出。所以该方法实际不可行</p><h1 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h1><p>考虑一个问题，位数为零是由2*5产生的，而吧每个数分解，出现5的次数必出现2的次数要少很多，因此，我们通过统计有多少个 5 即可判断尾数有多少个 0.先上代码在将原理<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LeetCode_172</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">trailingZeroes2</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (n &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            count += n / <span class="number">5</span>;</span><br><span class="line">            n /= <span class="number">5</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        LeetCode_172 leetCode = <span class="keyword">new</span> LeetCode_172();</span><br><span class="line">        System.out.println(leetCode.trailingZeroes(<span class="number">3</span>));</span><br><span class="line">        System.out.println(leetCode.trailingZeroes(<span class="number">5</span>));</span><br><span class="line">        System.out.println(leetCode.trailingZeroes(<span class="number">16</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>首先我们先统计n 包含一个5的个数，然后统计包含两个5的个数，依次类推。比如：35！，包含一个5的个数为5,10， 15， 20， 25， 30， 35，即35/5 = 7个，然而当我们遇到25的元素时，里面包含两个5，所以通过 $n/5^2$ ，更大的数一次类推，当 $5^k &gt; n$时即停止，在上面代码中表现为n&gt;0；</p><h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p><a href="https://github.com/RanMaosong/RanMaosong.github.io" target="_blank" rel="noopener">感谢大家的阅读和支持, 欢迎大家上星.</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/10/07/LeetCode-172-FactorialTrailingZeroes/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub链接&lt;/a&gt;&lt;br&gt;&lt;a
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>169. Majority Element(求众数)</title>
    <link href="http://yoursite.com/2018/10/06/LeetCode-169-MajorityElement/"/>
    <id>http://yoursite.com/2018/10/06/LeetCode-169-MajorityElement/</id>
    <published>2018-10-06T14:03:52.000Z</published>
    <updated>2018-10-07T08:11:02.871Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2018/10/06/LeetCode-136-SingleNumber/" target="_blank" rel="noopener">GIthub</a><br><a href="https://www.jianshu.com/p/cf75842c46c0" target="_blank" rel="noopener">简书</a><br><a href="https://blog.csdn.net/u014630987/article/details/82958884" target="_blank" rel="noopener">CSDN</a></p><h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><p>给定一个大小为 n 的数组，找到其中的众数。众数是指在数组中出现次数大于 ⌊ n/2 ⌋ 的元素。<br>你可以假设数组是非空的，并且给定的数组总是存在众数。<br><strong>示例1：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入: [3,2,3]</span><br><span class="line">输出: 3</span><br></pre></td></tr></table></figure></p><p><strong>示例2：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入: [2,2,1,1,1,2,2]</span><br><span class="line">输出: 2</span><br></pre></td></tr></table></figure></p><h1 id="方法一-暴力破解"><a href="#方法一-暴力破解" class="headerlink" title="方法一: 暴力破解"></a>方法一: 暴力破解</h1><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>最简单最直接的方法是统计每个数出现的次数，如果它出现的次数 大于 $\lfloor n/2 \rfloor$, 则这个数就为这个数组的众数。因此实现此算法，需要两个嵌套的 for 循环，外层循环遍历数组来确定当前值，内层循环同样遍历数组，它则用来统计当前值出现的次数。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LeetCode_169</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">majorityElement</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * O(n^2)</span></span><br><span class="line"><span class="comment">         *  Time Limit Exceeded</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">int</span> max = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.length; ++i) &#123;</span><br><span class="line">            <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; nums.length; ++j) &#123;</span><br><span class="line">                <span class="keyword">if</span> (nums[i] == nums[j])</span><br><span class="line">                    ++count;</span><br><span class="line">            &#125;</span><br><span class="line">            max = Math.max(max, count);</span><br><span class="line">            <span class="keyword">if</span> (max &gt; nums.length / <span class="number">2</span>)</span><br><span class="line">                <span class="keyword">return</span> nums[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h2><ul><li><p>时间复杂度： $O(n^2)$</p><p>由于该方法包含两个嵌套的 for 循环，每个循环迭代n次，因此该算法的时间复杂度为 $O(n^2)$</p></li><li><p>空间复杂度： $O(1)$</p></li></ul><h1 id="方法二：-HashMap"><a href="#方法二：-HashMap" class="headerlink" title="方法二： HashMap"></a>方法二： HashMap</h1><h2 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h2><p>在统计元素出现次数的时候， 我们可以使用一个 HashMap 来保存当前已出现的元素出现的次数，这样可以避免重复的统计，从而在一个 for 循环里完成任务。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Maosong Ran</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2018/10/06</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@email</span> maosongran@gmail.com</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LeetCode_169</span> </span>&#123; </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">majorityElement</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * O(n) O(n)</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        HashMap&lt;Integer, Integer&gt; count = <span class="keyword">new</span> HashMap&lt;Integer, Integer&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i &lt; nums.length; ++i) &#123;</span><br><span class="line">            Integer num = count.get(nums[i]);</span><br><span class="line">            <span class="keyword">if</span> (num == <span class="keyword">null</span>)</span><br><span class="line">                num = <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                ++num;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (num &gt; nums.length / <span class="number">2</span>)</span><br><span class="line">                <span class="keyword">return</span> nums[i];</span><br><span class="line">            count.put(nums[i], num);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nums[<span class="number">0</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="复杂度分析-1"><a href="#复杂度分析-1" class="headerlink" title="复杂度分析"></a>复杂度分析</h2><ul><li><p>时间复杂度： $O(n)$</p><p>由于我们只需要遍历一次数组，即可完成统计任务，因此，时间复杂度为$O(n)$</p></li><li><p>空间复杂度： $O(n)$<br>由于数组总存在众数，而众数的条件是其出现次数大于 $\lfloor n/2 \rfloor$,因此最坏情况，HashMap包含 $n - (\lfloor n/2 \rfloor + 1)$个元素，最好情况包含 1 个元素，因此空间复杂度为O(n)</p></li></ul><h1 id="方法三：-排序"><a href="#方法三：-排序" class="headerlink" title="方法三： 排序"></a>方法三： 排序</h1><p>由于众数的是其出现次数大于 $\lfloor n/2 \rfloor$ 的值，因此无论该值的大小是多少，该值总会出现在中心位置：对于数组长度为偶数时，为最中间两个数，为基数时，为最中间一个数。<br>！<a href="/images/leetcode169.png">Sorting</a></p><p>上图中，数组下面的线表示当众数出现在排序数组的最左侧，数组上面的线表示当总数出现在排序数组的最右侧时，测试是众数出现的两个极端情况，其余情况在这两种情况之间，因此总数总是出现在排序数组的最中心位置。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LeetCode_169</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">majorityElement</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        Arrays.sort(nums);</span><br><span class="line">        <span class="keyword">return</span> nums[nums.length/<span class="number">2</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="复杂度分析-2"><a href="#复杂度分析-2" class="headerlink" title="复杂度分析"></a>复杂度分析</h2><ul><li><p>时间复杂度： $O(nlog n)$</p><p>在Java和Python中，数组排序的时间复杂度为$O(nlog n)$</p></li><li><p>空间复杂度： $O(n)$ 或 $O(1)$</p><p>如果允许在原地进行排序时，我们不需要额外的空间，因此，空间复杂度为$O(1)$,如果不允许，则需要额外的等大的数组来存放该有序数组，因此空间复杂度为 $O(n)$</p></li></ul><h1 id="方法三：-分治法"><a href="#方法三：-分治法" class="headerlink" title="方法三： 分治法"></a>方法三： 分治法</h1><h2 id="算法-2"><a href="#算法-2" class="headerlink" title="算法"></a>算法</h2><p>分治法是算法中经常遇到的一种求解问题的方法，它将大问题小化，复杂问题简单化，因此可以很容易得出问题的解。</p><p>在本题中，我们将数组递归地从中间将大数组分成左右两个小数组，然后求左右两个小数组的的众数，如果这两个众数相等，则这个数也是大数组的众数，如果两个数不相等，则这二者之一必有一个是大数组的众数，为什么呢？</p><p>若假设这两个数之一不是大数组的众数，由于他们是小数组的众数，因此，他们出现次数各占小数组的一半以上，因此这两个数在大数组中的出现次数必大于 $\lfloor n/2 \rfloor$,因此剩下位置的值即便是相同，他们出现的次数也不大于 $\lfloor n/2 \rfloor$</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LeetCode_169</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">majorityElement</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> divideAndConquer(nums, <span class="number">0</span>, nums.length-<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">divideAndConquer</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (left == right)</span><br><span class="line">            <span class="keyword">return</span> nums[left];</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">int</span> mid = (left + right)/<span class="number">2</span>;</span><br><span class="line">            <span class="keyword">int</span> leftMajority = divideAndConquer(nums, left, mid);</span><br><span class="line">            <span class="keyword">int</span> rightMajority = divideAndConquer(nums, mid + <span class="number">1</span>, right);</span><br><span class="line"><span class="comment">//            System.out.println(leftMajority + "-&gt;" + rightMajority);</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (leftMajority == rightMajority)</span><br><span class="line">                <span class="keyword">return</span> leftMajority;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">int</span> leftCount = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">int</span> rightCount = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=left; i &lt;= right; ++i) &#123;</span><br><span class="line">                <span class="keyword">if</span> (nums[i] == leftMajority)</span><br><span class="line">                    ++leftCount;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (nums[i] == rightMajority) &#123;</span><br><span class="line">                    ++rightCount;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"><span class="comment">//            System.out.println(leftMajority + ":" + leftCount + ", " + rightMajority + ": " + rightCount);</span></span><br><span class="line">            <span class="keyword">return</span> leftCount &gt; rightCount ? leftMajority : rightMajority;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="复杂度分析-3"><a href="#复杂度分析-3" class="headerlink" title="复杂度分析"></a>复杂度分析</h2><ul><li><p>时间复杂度： $O(nlog n)$</p></li><li><p>空间复杂度： $O(ll0g n)$</p></li></ul><h1 id="方法五：-摩尔投票法"><a href="#方法五：-摩尔投票法" class="headerlink" title="方法五： 摩尔投票法"></a>方法五： 摩尔投票法</h1><h2 id="算法-3"><a href="#算法-3" class="headerlink" title="算法"></a>算法</h2><p>大致思路，首先有一个统计当前投票数的变量 count， 当 count == 0 时，我们以当前变量作为候选众数，然后从当前变量位置开始，若值等于候选众数的值，则count加1，若不相等，则减一，依次遍历玩数组，当遍历完数组，若 count == 0，则该数组不存在众数，不等于0时，此时的候选众数即位真正的众数。</p><p>原理： 由于众数票数大于 $\lfloor n/2 \rfloor$,因此即便它的票数减去其他所有的票数，它的票数也大于零。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LeetCode_169</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">majorityElement</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>  count = <span class="number">0</span>;</span><br><span class="line">        Integer candiate = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> num : nums) &#123;</span><br><span class="line">            <span class="keyword">if</span> (count == <span class="number">0</span>) &#123;</span><br><span class="line">                candiate = num;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            count += (candiate == num) ? <span class="number">1</span> : -<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> candiate;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="复杂度分析-4"><a href="#复杂度分析-4" class="headerlink" title="复杂度分析"></a>复杂度分析</h2><ul><li><p>时间复杂度： $O(n)$</p></li><li><p>空间复杂度： $O(1)$</p></li></ul><h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p><a href="https://github.com/RanMaosong/RanMaosong.github.io" target="_blank" rel="noopener">感谢大家的阅读和支持, 欢迎大家上星.</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/10/06/LeetCode-136-SingleNumber/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GIthub&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https:
      
    
    </summary>
    
      <category term="Weekly Algorithm" scheme="http://yoursite.com/categories/Weekly-Algorithm/"/>
    
    
      <category term="Weekly Algorithm" scheme="http://yoursite.com/tags/Weekly-Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode 136.Single Number(只出现一次的数字)</title>
    <link href="http://yoursite.com/2018/10/06/LeetCode-136-SingleNumber/"/>
    <id>http://yoursite.com/2018/10/06/LeetCode-136-SingleNumber/</id>
    <published>2018-10-06T05:49:39.000Z</published>
    <updated>2018-10-07T09:14:11.348Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2018/10/06/LeetCode-136-SingleNumber/" target="_blank" rel="noopener">GitHub</a><br><a href="https://www.jianshu.com/p/df7d256e2bea" target="_blank" rel="noopener">简书</a><br><a href="https://blog.csdn.net/u014630987/article/details/82951316" target="_blank" rel="noopener">CSDN</a></p><h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><p>给定一个非空整数数组，除了某个元素只出现一次以外，其余每个元素均出现两次。找出那个只出现了一次的元素。</p><p><strong>说明</strong><br>你的算法应该具有线性时间复杂度。 你可以不使用额外空间来实现吗？</p><p><strong>示例1</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入: [2,2,1]</span><br><span class="line">输出: 1</span><br></pre></td></tr></table></figure></p><p><strong>示例2</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入: [4,1,2,1,2]</span><br><span class="line">输出: 4</span><br></pre></td></tr></table></figure></p><p><strong>难度系数:</strong> 简单</p><h1 id="解题思路一"><a href="#解题思路一" class="headerlink" title="解题思路一"></a>解题思路一</h1><p>常规方法是,遍历数组,然后统计每个值出现的次数,最后在选择出现次数为1的那个值.该算法的时间复杂度为O(N),首先是统计数组,此时要遍历整个数组,然后是要遍历我们的统计数组,此时有事一个O(N),由于我们使用了一个统计数组来保存每个值出现的次数,此时需要的空间复杂度为O(n),因此不符合要求.</p><h1 id="解题思路二"><a href="#解题思路二" class="headerlink" title="解题思路二"></a>解题思路二</h1><p>为了解决不需要额外的空间这个要求,我们可以使用位操作中的异或规则来进行处理.异或运算法则如下</p><ol><li>a $\oplus$ a = 0, a $\oplus$ 0=a</li><li>a $\oplus$ b = b $\oplus$ a</li><li>a $\oplus$ b $\oplus$ c = a $\oplus$ (b $\oplus$ c) = a $\oplus$ (c $\oplus$ b) = (a $\oplus$ b) $\oplus$ c</li></ol><p>其中,第一条规则说明,当某个数出现两次时,通过 $\oplus$ 变为0,出现一次时依然保持原来的数,第二、三条的交换律和分配律说明通过多次 $\oplus$ 操作最终解决本题。</p><p><strong>注意</strong>: 本体题目中指出除了某个元素值出现一次外其余的均出现两次，根据法则一可以看出本算法只适合除了某个元素出现一次外，其余元素出现偶数次的情况。</p><p>比如在示例二中的 $\oplus$ 操作：</p><script type="math/tex; mode=display">\begin{aligned}    4 \oplus 1 \oplus 2 \oplus 1 \oplus 2 &=  4 \oplus 1 \oplus 1 \oplus 2 \oplus 2 \\    &= 4 \oplus (1 \oplus 1) \oplus (2 \oplus 2)\\    &=4\end{aligned}</script><h1 id="Java实现代码"><a href="#Java实现代码" class="headerlink" title="Java实现代码"></a>Java实现代码</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Maosong Ran</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2018/10/06</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@email</span> maosongran@gmail.com</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LeetCode_136</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">singleNumber</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> len = nums.length;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i &lt; len; ++i)</span><br><span class="line">            result ^= nums[i];</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        LeetCode_136 leetCode = <span class="keyword">new</span> LeetCode_136();</span><br><span class="line">        System.out.println(leetCode.singleNumber(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>&#125;));</span><br><span class="line">        System.out.println(leetCode.singleNumber(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>&#125;));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>输出</strong>:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">4</span><br></pre></td></tr></table></figure></p><h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p><a href="https://github.com/RanMaosong/RanMaosong.github.io" target="_blank" rel="noopener">感谢大家的阅读和支持, 欢迎大家上星.</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/10/06/LeetCode-136-SingleNumber/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https:
      
    
    </summary>
    
      <category term="Weekly Algorithm" scheme="http://yoursite.com/categories/Weekly-Algorithm/"/>
    
    
      <category term="Weekly Algorithm" scheme="http://yoursite.com/tags/Weekly-Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>数据结构第二次作业</title>
    <link href="http://yoursite.com/2018/09/17/ta-work-02/"/>
    <id>http://yoursite.com/2018/09/17/ta-work-02/</id>
    <published>2018-09-17T13:09:48.000Z</published>
    <updated>2018-09-22T04:25:57.776Z</updated>
    
    <content type="html"><![CDATA[<h1 id="实现在单向链表中-返回某个节点的前驱"><a href="#实现在单向链表中-返回某个节点的前驱" class="headerlink" title="实现在单向链表中,返回某个节点的前驱."></a>实现在单向链表中,返回某个节点的前驱.</h1><h2 id="原理说明"><a href="#原理说明" class="headerlink" title="原理说明"></a>原理说明</h2><p>单向链表的特点是链表只有一个方向,即只有从前驱结点指向后继结点的指针,而没有后继节点指向前驱结点的指针,结构图大致如下:<br><img src="https://upload-images.jianshu.io/upload_images/5208761-03db3a5a57fd70ed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="linklist1.png"><br>从图可以看出,如果我们要访问第三个结点,我们只能从头指针依次便利每个后继结点,直至访问到第三个结点,因此在单向链表中,我们查找某个元素的时间复杂度 $O(n)$.</p><p>基于链表的单向特点,因此我们必须从头开始遍历才能找到某个节点的前驱.以上图为例进行讲解,假如我们要寻找第三个结点的前驱,我们如何实现呢?</p><ol><li>首先我们需要申明两个临时变量pre 和 cur,分别指向Head 头结点和Head-&gt;next(为null或为第一个结点),这样pre和cur就构成一个相关的对,pre表示cur的前驱,当cur到达第三个结点时,此时pre就是我们要找的前驱.<br><img src="https://upload-images.jianshu.io/upload_images/5208761-bb412a44ed651d75.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="linklist2.png"></li><li>再循环中,我们依次把pre和cur向前同时推进,直至cur到达事先定义的结点.<ul><li>在上图中,cur=1,不是我们要找的结点3,因此pre和cur向前移动,即pre=cur, cur=next;<br><img src="https://upload-images.jianshu.io/upload_images/5208761-ba2086541433a47a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="linklist3.png"></li></ul></li></ol><ul><li>此时,cur=2,不符合我们的条件,我们继续执行上面的操作;<br><img src="https://upload-images.jianshu.io/upload_images/5208761-54cc3f96b5132267.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="linklist4.png"><br>现在cur=3,满足我们的条件,此时pre就是他的前驱,返回该节点即可.<h2 id="下面是实现代码-仅供参考"><a href="#下面是实现代码-仅供参考" class="headerlink" title="下面是实现代码,仅供参考"></a>下面是实现代码,仅供参考</h2><h3 id="c语言"><a href="#c语言" class="headerlink" title="　c语言"></a>　<strong>c语言</strong></h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;malloc.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 数据类型</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">int</span> ElementType;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 链表数据结构</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    ElementType data;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span>* <span class="title">next</span>;</span></span><br><span class="line">&#125;Node, *Linklist;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建链表</span></span><br><span class="line"><span class="function">Linklist <span class="title">createList</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> len;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"请输入链表长度:"</span>);</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;len);</span><br><span class="line">    Linklist head = (Linklist)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(Node));</span><br><span class="line">    Linklist tail = head;</span><br><span class="line">    tail-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;len; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> val;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"第%d个节点内容:"</span>, (i+<span class="number">1</span>));</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;val);</span><br><span class="line">        Linklist node = (Linklist)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(Node));</span><br><span class="line">        node-&gt;data = val;</span><br><span class="line">        node-&gt;next = tail-&gt;next;</span><br><span class="line">        tail-&gt;next = node;</span><br><span class="line">        tail = node;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> head;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 查找给定节点的前驱</span></span><br><span class="line"><span class="function">Node* <span class="title">preNodeOf</span><span class="params">(Linklist <span class="built_in">list</span>, Node* node)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Node* cur = <span class="built_in">list</span>-&gt;next;</span><br><span class="line">    Node* pre = <span class="built_in">list</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> (cur != <span class="literal">NULL</span> &amp;&amp; cur-&gt;data != node-&gt;data)</span><br><span class="line">    &#123;</span><br><span class="line">        </span><br><span class="line">        pre = cur;</span><br><span class="line">        cur = cur-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (pre != <span class="built_in">list</span>)</span><br><span class="line">        <span class="keyword">return</span> pre;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 根据索引查找指定节点,index 从1开始</span></span><br><span class="line"><span class="function">Node* <span class="title">getNode</span><span class="params">(Linklist <span class="built_in">list</span>, <span class="keyword">int</span> index)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Node* node = <span class="built_in">list</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;index; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        node = node-&gt;next;</span><br><span class="line">        <span class="keyword">if</span> (node == <span class="literal">NULL</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> node;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 显示链表内容</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">showLinklist</span><span class="params">(Linklist <span class="built_in">list</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Node* node = <span class="built_in">list</span>-&gt;next;</span><br><span class="line">    <span class="keyword">while</span> (node != <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d  "</span>, node-&gt;data);</span><br><span class="line">        node = node-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">length</span><span class="params">(Linklist <span class="built_in">list</span>)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> len = <span class="number">0</span>;</span><br><span class="line">    Node* node = <span class="built_in">list</span>-&gt;next;</span><br><span class="line">    <span class="keyword">while</span> (node != <span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        ++len;</span><br><span class="line">        node = node-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> len;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Linklist <span class="built_in">list</span> = createList();</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"链表内容:"</span>);</span><br><span class="line">    showLinklist(<span class="built_in">list</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> len = length(<span class="built_in">list</span>);</span><br><span class="line">    <span class="keyword">int</span> index;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"\n输入需要查找前驱的节点的索引(0&lt; n &lt;=%d):"</span>, len);</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;index);</span><br><span class="line">    Node* node = getNode(<span class="built_in">list</span>, index);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"第%d个节点的内容:%d\n"</span>, index, node-&gt;data);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    node = preNodeOf(<span class="built_in">list</span>, node);</span><br><span class="line">    <span class="keyword">if</span> (node != <span class="literal">NULL</span>)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, node-&gt;data);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"该节点为第一个节点,无前驱节点!"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h3 id="C-代码-这里使用了模板类"><a href="#C-代码-这里使用了模板类" class="headerlink" title="　C++代码:这里使用了模板类"></a>　<strong>C++代码</strong>:这里使用了模板类</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 节点类, 使用了模板类,方便数据类型</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">Node</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    T data;</span><br><span class="line">    Node&lt;T&gt;* next;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    Node()&#123;&#125;</span><br><span class="line">    Node(T v, Node&lt;T&gt;* node)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">this</span>-&gt;data = v;</span><br><span class="line">        <span class="keyword">this</span>-&gt;next = node;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">LinkList</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    Node&lt;T&gt;* head;</span><br><span class="line">    Node&lt;T&gt;* tail;</span><br><span class="line">    <span class="keyword">int</span> count;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    LinkList();</span><br><span class="line">    ~LinkList();</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">size</span><span class="params">()</span></span>;</span><br><span class="line">    Node&lt;T&gt;* getNode(<span class="keyword">int</span>);</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">append</span><span class="params">(T data)</span></span>;</span><br><span class="line">    Node&lt;T&gt;* preNodeOf(Node&lt;T&gt;*);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 初始化链表</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">LinkList</span>&lt;T&gt;:</span>:LinkList(): count(<span class="number">0</span>)</span><br><span class="line">&#123;</span><br><span class="line">    head = <span class="keyword">new</span> Node&lt;T&gt;;</span><br><span class="line">    head-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">    tail = head;</span><br><span class="line">    <span class="keyword">int</span> length;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"请输入链表初始长度:"</span>;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; length;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; length; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        T data;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"第"</span> &lt;&lt; (i+<span class="number">1</span>) &lt;&lt; <span class="string">"个节点的内容:"</span>;</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; data;</span><br><span class="line">        append(data);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">LinkList</span>&lt;T&gt;:</span>:~LinkList()</span><br><span class="line">&#123;</span><br><span class="line">    Node&lt;T&gt;* node = head-&gt;next;</span><br><span class="line">    Node&lt;T&gt;* tmp;</span><br><span class="line">    <span class="keyword">while</span> (node != <span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        tmp = node;</span><br><span class="line">        node = node-&gt;next;</span><br><span class="line">        <span class="keyword">delete</span> tmp;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">delete</span> head;</span><br><span class="line">    head = <span class="literal">NULL</span>;</span><br><span class="line">    tail = <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 返回链表的长度</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">int</span> <span class="title">LinkList</span>&lt;T&gt;:</span>:size()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> count;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">Node</span>&lt;T&gt;* <span class="title">LinkList</span>&lt;T&gt;:</span>:getNode(<span class="keyword">int</span> index)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (index &gt; count || index &lt;= <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt; <span class="string">"Index Error!"</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    Node&lt;T&gt;* node = head;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;index &amp;&amp; node-&gt;next; ++i) </span><br><span class="line">    &#123;</span><br><span class="line">        node = node-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> node;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">LinkList</span>&lt;T&gt;:</span>:print()</span><br><span class="line">&#123;</span><br><span class="line">    Node&lt;T&gt;* node = head-&gt;next;</span><br><span class="line">    <span class="keyword">while</span> (node)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; node-&gt;data &lt;&lt; <span class="string">"  "</span>;</span><br><span class="line">        node = node-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">LinkList</span>&lt;T&gt;:</span>:append(T data)</span><br><span class="line">&#123;</span><br><span class="line">    Node&lt;T&gt;* node = <span class="keyword">new</span> Node&lt;T&gt;();</span><br><span class="line">    node-&gt;data = data;</span><br><span class="line">    node-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">    tail-&gt;next = node;</span><br><span class="line">    tail = node;</span><br><span class="line">    ++count;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">Node</span>&lt;T&gt;* <span class="title">LinkList</span>&lt;T&gt;:</span>:preNodeOf(Node&lt;T&gt;* node)</span><br><span class="line">&#123;</span><br><span class="line">    Node&lt;T&gt;* pre = head;</span><br><span class="line">    Node&lt;T&gt;* cur = head-&gt;next;</span><br><span class="line">    <span class="keyword">while</span> (cur-&gt;data != node-&gt;data)</span><br><span class="line">    &#123;</span><br><span class="line">        pre = cur;</span><br><span class="line">        cur = cur-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (pre == head) </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> pre;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    LinkList&lt;<span class="keyword">int</span>&gt; <span class="built_in">list</span> = LinkList&lt;<span class="keyword">int</span>&gt;();</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"链表内容:"</span>;</span><br><span class="line">    <span class="built_in">list</span>.print();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> size = <span class="built_in">list</span>.size();</span><br><span class="line">    <span class="keyword">int</span> index;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"输入需要查找前驱的节点的索引(1 &lt;= n &lt;="</span> &lt;&lt; size &lt;&lt; <span class="string">"):"</span>;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; index;</span><br><span class="line">    Node&lt;<span class="keyword">int</span>&gt;* node = <span class="built_in">list</span>.getNode(index);</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"第"</span> &lt;&lt; index &lt;&lt; <span class="string">"个节点的内容:"</span> &lt;&lt; node-&gt;data &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    Node&lt;<span class="keyword">int</span>&gt;* pre = <span class="built_in">list</span>.preNodeOf(node);</span><br><span class="line">    <span class="keyword">if</span> (pre)</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"第"</span> &lt;&lt; index &lt;&lt; <span class="string">"个节点的前驱:"</span> &lt;&lt; pre-&gt;data &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"该节点为第一个节点,无前驱"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面的代码仅供学习使用,课程要求为C/C++变成语言.</p><h3 id="Python3"><a href="#Python3" class="headerlink" title="　Python3"></a>　<strong>Python3</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data, next=None)</span>:</span></span><br><span class="line">        self.data = data</span><br><span class="line">        self.next = next</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinkList</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self._head = Node(<span class="number">0</span>)</span><br><span class="line">        self._tail = self._head</span><br><span class="line">        self._count = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">append</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        node = Node(data, <span class="keyword">None</span>)</span><br><span class="line">        self._tail.next = node</span><br><span class="line">        self._tail = node</span><br><span class="line">        self._count += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._count</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">show</span><span class="params">(self)</span>:</span></span><br><span class="line">        node = self._head.next</span><br><span class="line">        <span class="keyword">while</span> (node):</span><br><span class="line">            print(node.data, end=<span class="string">"  "</span>)</span><br><span class="line">            node = node.next</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_n_elems</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            data = int(input(<span class="string">"请输入第%d个节点的内容:"</span> % (i+<span class="number">1</span>)))</span><br><span class="line">            self.append(data)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pre_node_of</span><span class="params">(self, node)</span>:</span></span><br><span class="line">        pre = self._head</span><br><span class="line">        cur = pre.next</span><br><span class="line">        <span class="keyword">while</span>(cur.data != node.data):</span><br><span class="line">            pre = cur</span><br><span class="line">            cur = cur.next</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (pre == self._head):</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> pre</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_node</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> (<span class="number">1</span> &lt;= index &lt;= self.size() ):</span><br><span class="line">            <span class="keyword">raise</span> Exception(<span class="string">"Index Error!"</span>)</span><br><span class="line">        node = self._head</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(index):</span><br><span class="line">            node = node.next</span><br><span class="line">        <span class="keyword">return</span> node</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    linklist = LinkList()</span><br><span class="line">    length = int(input(<span class="string">"请输入链表长度:"</span>))</span><br><span class="line">    linklist.add_n_elems(length)</span><br><span class="line">    print(<span class="string">"链表内容:"</span>, end=<span class="string">""</span>)</span><br><span class="line">    linklist.show()</span><br><span class="line"></span><br><span class="line">    index = int(input(<span class="string">"\n输入需要查找前驱的节点的索引(0&lt; n &lt;=%d):"</span> % linklist.size()))</span><br><span class="line">    node = linklist.get_node(index)</span><br><span class="line">    pre_node = linklist.pre_node_of(node)</span><br><span class="line">    print(<span class="string">"第%d个节点的内容:%d"</span> % (index, node.data))</span><br><span class="line">    <span class="keyword">if</span> pre_node <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        print(<span class="string">"第%d个节点的前驱的内容内容:%d"</span> % (index, pre_node.data))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"该节点无前驱节点"</span>)</span><br></pre></td></tr></table></figure><h3 id="Java"><a href="#Java" class="headerlink" title="　Java"></a>　<strong>Java</strong></h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Scanner;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> T data;</span><br><span class="line">    <span class="keyword">public</span> Node&lt;T&gt; next;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Node</span><span class="params">()</span></span>&#123;&#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Node</span><span class="params">(T data)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.data = data;</span><br><span class="line">        <span class="keyword">this</span>.next = <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linklist</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Node&lt;T&gt; head;</span><br><span class="line">    <span class="keyword">private</span> Node&lt;T&gt; tail;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> count;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Linklist</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        head = <span class="keyword">new</span> Node&lt;T&gt;();</span><br><span class="line">        tail = head;</span><br><span class="line">        count = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">append</span><span class="params">(T data)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        Node&lt;T&gt; node = <span class="keyword">new</span> Node(data);</span><br><span class="line">        tail.next = node;</span><br><span class="line">        tail = node;</span><br><span class="line">        count++;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">size</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Node&lt;T&gt; <span class="title">getNode</span><span class="params">(<span class="keyword">int</span> index)</span>  <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (index &lt; <span class="number">1</span> || index &gt; count)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> Exception(<span class="string">"Index Error!"</span>);</span><br><span class="line">        Node&lt;T&gt; node = head;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;index; ++i) &#123;</span><br><span class="line">            node = node.next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> node;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Node&lt;T&gt; <span class="title">preNodeOf</span><span class="params">(Node&lt;T&gt; node)</span> </span>&#123;</span><br><span class="line">        Node&lt;T&gt; pre = head;</span><br><span class="line">        Node&lt;T&gt; cur = pre.next;</span><br><span class="line">        <span class="keyword">while</span> (cur.data != node.data) &#123;</span><br><span class="line">            pre = cur;</span><br><span class="line">            cur = cur.next;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (pre == head)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> pre;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">show</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Node&lt;T&gt; node = head.next;</span><br><span class="line">        <span class="keyword">while</span> (node != <span class="keyword">null</span>) &#123;</span><br><span class="line">            System.out.print(node.data + <span class="string">"  "</span>);</span><br><span class="line">            node = node.next;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Demo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Scanner scan = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line"></span><br><span class="line">        Linklist&lt;Integer&gt; list = <span class="keyword">new</span> Linklist&lt;Integer&gt;();</span><br><span class="line">        <span class="keyword">int</span> length;</span><br><span class="line">        System.out.print(<span class="string">"请输入链表长度:"</span>);</span><br><span class="line">        length = scan.nextInt();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;length; ++i) &#123;</span><br><span class="line">            System.out.print(<span class="string">"请输入第"</span> + (i+<span class="number">1</span>) + <span class="string">"个节点的内容:"</span>);</span><br><span class="line">            <span class="keyword">int</span> data = scan.nextInt();</span><br><span class="line">            list.append(data);</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.print(<span class="string">"链表内容:"</span>);</span><br><span class="line">        list.show();</span><br><span class="line"></span><br><span class="line">        System.out.print(<span class="string">"\n输入需要查找前驱的节点的索引(1&lt;= n &lt;="</span> + list.size() + <span class="string">")"</span>);</span><br><span class="line">        <span class="keyword">int</span> index = scan.nextInt();</span><br><span class="line">        Node&lt;Integer&gt; node;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            node = list.getNode(index);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            <span class="comment">// e.printStack();</span></span><br><span class="line">            node = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">            </span><br><span class="line">        Node&lt;Integer&gt; pre_node = list.preNodeOf(node);</span><br><span class="line">        System.out.println(<span class="string">"第"</span> + index + <span class="string">"个节点的内容:"</span> + node.data);</span><br><span class="line">        <span class="keyword">if</span> (pre_node != <span class="keyword">null</span>)</span><br><span class="line">            System.out.println(<span class="string">"第"</span> + index + <span class="string">"个节点的前驱的内容内容:"</span>  + pre_node.data);</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            System.out.println(<span class="string">"该节点无前驱节点"</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;实现在单向链表中-返回某个节点的前驱&quot;&gt;&lt;a href=&quot;#实现在单向链表中-返回某个节点的前驱&quot; class=&quot;headerlink&quot; title=&quot;实现在单向链表中,返回某个节点的前驱.&quot;&gt;&lt;/a&gt;实现在单向链表中,返回某个节点的前驱.&lt;/h1&gt;&lt;h2 id=
      
    
    </summary>
    
      <category term="Teaching Assistant" scheme="http://yoursite.com/categories/Teaching-Assistant/"/>
    
    
      <category term="Teaching Assistant" scheme="http://yoursite.com/tags/Teaching-Assistant/"/>
    
  </entry>
  
  <entry>
    <title>数据结构第一次作业</title>
    <link href="http://yoursite.com/2018/09/17/ta-work-01/"/>
    <id>http://yoursite.com/2018/09/17/ta-work-01/</id>
    <published>2018-09-17T13:09:42.000Z</published>
    <updated>2018-09-22T04:25:47.127Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-逻辑结构和物理结构有什么不同"><a href="#1-逻辑结构和物理结构有什么不同" class="headerlink" title="1.    逻辑结构和物理结构有什么不同?"></a>1.    逻辑结构和物理结构有什么不同?</h1><p>逻辑结构是指从操作对象抽象出的数学模型，其结构定义中的关系用于描述数据元素之间的逻辑关系。<br>逻辑结构在计算机中的表示称为物理结构或存储结构，根据数据元素在计算机中的表示方法，可分为顺序存储结构与链式存储结构。</p><h1 id="2-算法和程序有什么不同？"><a href="#2-算法和程序有什么不同？" class="headerlink" title="2.    算法和程序有什么不同？"></a>2.    算法和程序有什么不同？</h1><p>算法是指解决问题的一种方法或一个过程。<br>算法是若干指令的有穷序列，满足<strong>性质</strong>：</p><blockquote><p>(1)输入：由外部提供的量作为算法的输入.<br>(2)输出：算法产生至少一个量作为输出.<br>(3)确定性：算法的每一步骤必须有确切的定义.<br>(4)有限性：算法的有穷性是指算法必须能在执行有限个步骤之后终止.<br>(5) 可行性，算法需要考虑设计的可能，程序则具体是实现算法上的设计</p></blockquote><p>程序是算法在计算机上用某种程序设计语言的具体实现。<br>程序可以不满足算法的性质(4)。<br>例如操作系统，是一个在无限循环中执行的程序，因而不是一个算法。<br>操作系统的各种任务可看成是单独的问题，每一个问题由操作系统中的一个子程序通过特定的算法来实现。该子程序得到输出结果后便终止。</p><h1 id="3-什么是ADT？"><a href="#3-什么是ADT？" class="headerlink" title="3.    什么是ADT？"></a>3.    什么是ADT？</h1><p>抽象数据类型（ADT）是一个实现包括储存数据元素的存储结构以及实现基本操作的算法，是数据结构作为一个软件组件的实现。ADT的接口用一种类型上的一组操作来定义，每一个操作由它的输入和输出定义。ADT并不会指定数据类型如何实现，这些实现细节对于ADT的用户是隐藏的，并且通过封装来阻止外部对它的访问。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-逻辑结构和物理结构有什么不同&quot;&gt;&lt;a href=&quot;#1-逻辑结构和物理结构有什么不同&quot; class=&quot;headerlink&quot; title=&quot;1.    逻辑结构和物理结构有什么不同?&quot;&gt;&lt;/a&gt;1.    逻辑结构和物理结构有什么不同?&lt;/h1&gt;&lt;p&gt;逻辑结构
      
    
    </summary>
    
      <category term="Teaching Assistant" scheme="http://yoursite.com/categories/Teaching-Assistant/"/>
    
    
      <category term="Teaching Assistant" scheme="http://yoursite.com/tags/Teaching-Assistant/"/>
    
  </entry>
  
  <entry>
    <title>多元线性回归——Day3</title>
    <link href="http://yoursite.com/2018/09/13/ML-100-Days-003/"/>
    <id>http://yoursite.com/2018/09/13/ML-100-Days-003/</id>
    <published>2018-09-13T13:43:18.000Z</published>
    <updated>2018-09-13T14:40:14.231Z</updated>
    
    <content type="html"><![CDATA[<p><strong>相关链接:</strong><br><a href="https://ranmaosong.github.io/2018/09/13/ML-100-Days-003/" target="_blank" rel="noopener">Github地址</a><br><a href="https://www.jianshu.com/p/56b31d24823c" target="_blank" rel="noopener">简书地址</a><br><a href="https://blog.csdn.net/u014630987/article/details/82695498" target="_blank" rel="noopener">CSDN地址</a></p><hr><p>上一节我们讲解了简单现行回归,该回归的输入特征只有 1 个.本节我们对多元线性回归进行讲解,其输入具有多个特征.多元线性回归通过对训练数据拟合一个多元线性方程来对2或多个特征和一个响应值之间进行建模.多元线性回归的处理步骤和上一节的简单线性回归类似,只是在评估阶段存在差异.你可以通过多元线性回归来发现那个因素或特征对预测结果具有较大的影响和找到不同特征是如何相互影响的.多元线性回归数学表达如下:</p><script type="math/tex; mode=display">y = b_0 + b_1x_1 + b_2x_2 ...... + b_nx_n</script><p>从公式我们可以发现, 当n=1时,多元线性回归就变成简单线性回归,因此,简单线性回归是多元线性回归的一个特例.</p><h1 id="说在前面的话"><a href="#说在前面的话" class="headerlink" title="说在前面的话"></a>说在前面的话</h1><h2 id="假设"><a href="#假设" class="headerlink" title="假设"></a>假设</h2><p>在我们进行线性回归建模时,</p><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><h2 id="1-数据预处理"><a href="#1-数据预处理" class="headerlink" title="1. 数据预处理"></a>1. 数据预处理</h2><h2 id="2-训练模型"><a href="#2-训练模型" class="headerlink" title="2. 训练模型"></a>2. 训练模型</h2><h2 id="3-预测测试集"><a href="#3-预测测试集" class="headerlink" title="3. 预测测试集"></a>3. 预测测试集</h2><h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p><a href="https://github.com/RanMaosong/RanMaosong.github.io" target="_blank" rel="noopener">感谢大家的阅读和支持, 欢迎大家上星.</a>.该博客的原始Github项目地址<a href="https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day3_Multiple_Linear_Regression.md" target="_blank" rel="noopener">点击这里</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;相关链接:&lt;/strong&gt;&lt;br&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/09/13/ML-100-Days-003/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github地址&lt;/a&gt;&lt;
      
    
    </summary>
    
      <category term="机器学习100天" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    
    
      <category term="机器学习100天" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    
  </entry>
  
  <entry>
    <title>简单线性回归——Day2</title>
    <link href="http://yoursite.com/2018/09/13/ML-100-Days-002/"/>
    <id>http://yoursite.com/2018/09/13/ML-100-Days-002/</id>
    <published>2018-09-13T06:22:04.000Z</published>
    <updated>2018-09-13T13:36:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>相关链接:</strong><br><a href="https://ranmaosong.github.io/2018/09/13/ML-100-Days-002/" target="_blank" rel="noopener">Github地址</a><br><a href="https://www.jianshu.com/p/ff991ff8d4c7" target="_blank" rel="noopener">简书地址</a><br><a href="https://blog.csdn.net/u014630987/article/details/82687507" target="_blank" rel="noopener">CSDN地址</a></p><hr><p>在预测问题中,我们会经常遇到两种常用术语:回归(Regression)和分类(classification),他们的区别是回归算法解决的是预测连续值,而分类问题则是预测的是离散值,因此回归模型的输出是无限的,而分类问题的输出是有限的.</p><p>在统计学中，线性回归是利用称为线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。</p><p>本文首先介绍简单线性回归，下一节介绍多元线性回归。简单线性回归就是根据一个特征 <code>X</code> 来预测一个与其相关的变量 <code>Y</code>。通常我们我们假设这两种变量之间是现行相关的,即可以通过一条直线把这些变量区分开.因此,简单线性回归就是我们试图寻找一个线性函数,该函数以特征 <code>X</code> 为输入,输出一个变量,且在训练集中,使其预测的值尽可能接近目标值.</p><p>在本文我们以一个学生学习时间(hours),来预测的他该门课程的分数(scores).我们假设这两个变量之间存在某种线性关系,如图所示<br><img src="/images/simple_linear_regression.png" alt="simple_linear_regression"></p><p>我们的目标是找到最有的 $b_0$ 和 $b_1$,使我们训练集中的数据,我们的预测值和真实值之间的误差最小.即</p><script type="math/tex; mode=display">b_0^*,b_1^* = argmax_{b_0, b_1} sum\{(y_i - y_p)^2\}</script><p>其中, $y_p$ 为我们预测的值, $y_i$为真是值.</p><h1 id="1-数据预处理"><a href="#1-数据预处理" class="headerlink" title="1. 数据预处理"></a>1. 数据预处理</h1><p>数据预处理,我们将按照第一天介绍的模型进行处理:</p><ol><li>导入相关库</li><li>导入数据集</li><li>检查缺失值</li><li>划分数据集</li><li>特征标准化</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">dataset = pd.read_csv(&quot;studentscores.csv&quot;)</span><br><span class="line">X = dataset.iloc[:, 0].values</span><br><span class="line">Y = dataset.iloc[:, 1].values</span><br><span class="line"></span><br><span class="line">from sklearn.cross_validation import train_test_split</span><br><span class="line">X_train, Y_train, X_test, Y_test = train_test_split(X, Y, test_size=1/4, random_state=0)</span><br></pre></td></tr></table></figure><h1 id="2-训练线性回归模型"><a href="#2-训练线性回归模型" class="headerlink" title="2. 训练线性回归模型"></a>2. 训练线性回归模型</h1><p>sklearn 机器学习库为我们提供了许多的常用机器学习模型,线性回归模型 LinearRegression 存在于 sklearn.linear_model 文件中, 该文件为我们提供了许多的线性模型.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">regressor = LinearRegression()</span><br><span class="line">regressor.fit(X_train, Y_train)</span><br></pre></td></tr></table></figure><p>我们首先通过 LinearRegression() 初始化一个 regressor 实例来表示线性回归模型.然后通过给 <code>.fit</code> 传入我们的训练集的特征和标签来训练 regressor.</p><p><strong>注意:</strong> 在 sklearn 中对训练数据的格式有一个规定,对于输入 X, 要求其格式是 N*M,其中 N 表示样本数, M 表示每个样本的特征数, 此示例中 M=1.对于标签 Y, 其格式是N*F, N表示样本数, F表示输出值的个数,此处F=1.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(regressor.coef_, regressor.intercept_)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[9.94167834]] [1.93220425]</span><br></pre></td></tr></table></figure></p><p>regressor.coef_表示模型的权重, regressor.intercept_ 表示模型的偏执,分别表示上面模型公式的$b_1$ 和 $b_0$.</p><h1 id="3-预测结果"><a href="#3-预测结果" class="headerlink" title="3. 预测结果"></a>3. 预测结果</h1><p>当我们通过 <code>.fit</code> 函数训练好后模型,我们可以通过 <code>.predict</code> 函数来预测我们未知的数据.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Y_pred = regressor.predict(X_test)</span><br></pre></td></tr></table></figure></p><p>我们通过手动预测来验证上面提到的 regressor.coef_, regressor.intercept_ 表示的意义:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">temp = regressor.intercept_[<span class="number">0</span>] + regressor.coef_[<span class="number">0</span>] * X_test[<span class="number">0</span>]</span><br><span class="line">print(temp, Y_pred[<span class="number">0</span>])</span><br></pre></td></tr></table></figure></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[16.84472176] [16.84472176]</span><br></pre></td></tr></table></figure></p><h1 id="4-可视化"><a href="#4-可视化" class="headerlink" title="4. 可视化"></a>4. 可视化</h1><p>首先我们可视化训练集的结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X_train, Y_train, color=&quot;red&quot;)</span><br><span class="line">plt.plot(X_train, regressor.predict(X_train), color=&apos;blue&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="/images/simple_linear_regression_fig1.png" alt="figure1"><br>对测试集进行可视化<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X_test, Y_test, color=&quot;red&quot;)</span><br><span class="line">plt.plot(X_test, regressor.predict(X_test), color=&quot;blue&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="/images/simple_linear_regression_fig2.png" alt="figure2"></p><p>其中 <code>.scatter</code> 用于画散点图, <code>.plot</code> 用于画直线.</p><h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p><a href="https://github.com/RanMaosong/RanMaosong.github.io" target="_blank" rel="noopener">感谢大家的阅读和支持, 欢迎大家上星.</a>.该博客的原始Github项目地址<a href="https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day2_Simple_Linear_Regression.md" target="_blank" rel="noopener">点击这里</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;相关链接:&lt;/strong&gt;&lt;br&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/09/13/ML-100-Days-002/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github地址&lt;/a&gt;&lt;
      
    
    </summary>
    
      <category term="机器学习100天" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    
    
      <category term="机器学习100天" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    
  </entry>
  
</feed>
