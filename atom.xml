<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>技术闲谈</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-05-28T07:15:49.425Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>茂松</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>GBDT算法原理以及实例理解</title>
    <link href="http://yoursite.com/2019/04/27/ML-GBDT/"/>
    <id>http://yoursite.com/2019/04/27/ML-GBDT/</id>
    <published>2019-04-27T05:54:40.000Z</published>
    <updated>2019-05-28T07:15:49.425Z</updated>
    
    <content type="html"><![CDATA[<p>该内容完全转载自<a href="http://blog.csdn.net/zpalyq110/article/details/79527653" target="_blank" rel="noopener">同名CSDN博客</a>(<a href="http://blog.csdn.net/zpalyq110/article/details/79527653" target="_blank" rel="noopener">http://blog.csdn.net/zpalyq110/article/details/79527653</a>)</p><p><a href="https://ranmaosong.github.io/2019/04/27/ML-GBDT/" target="_blank" rel="noopener">GitHub</a></p><p><a href="https://www.jianshu.com/p/dcc5124fa35a" target="_blank" rel="noopener">简书</a></p><p><a href="https://blog.csdn.net/u014630987/article/details/89602217" target="_blank" rel="noopener">CSDN</a></p><p><strong>写在前面：</strong> 去年学习GBDT之初，为了加强对算法的理解，整理了一篇笔记形式的文章，发出去之后发现阅读量越来越多，渐渐也有了评论，评论中大多指出来了笔者理解或者编辑的错误，故重新编辑一版文章，内容更加翔实，并且在GitHub上实现了和本文一致的GBDT简易版（包括回归、二分类、多分类以及可视化），供大家交流探讨。感谢各位的点赞和评论，希望继续指出错误</p><p><strong>Github：</strong><br><strong><a href="https://github.com/Freemanzxp/GBDT_Simple_Tutorial" target="_blank" rel="noopener">https://github.com/Freemanzxp/GBDT_Simple_Tutorial</a></strong></p><p><strong>简介：</strong><br>GBDT 的全称是 Gradient Boosting Decision Tree，梯度提升树，在传统机器学习算法中，GBDT算的上TOP3的算法。想要理解GBDT的真正意义，那就必须理解GBDT中的Gradient Boosting 和Decision Tree分别是什么？</p><h1 id="1-Decision-Tree：CART回归树"><a href="#1-Decision-Tree：CART回归树" class="headerlink" title="1. Decision Tree：CART回归树"></a>1. Decision Tree：CART回归树</h1><p>首先，GBDT使用的决策树是CART回归树，无论是处理回归问题还是二分类以及多分类，GBDT使用的决策树通通都是都是CART回归树。为什么不用CART分类树呢？因为GBDT每次迭代要拟合的是<strong>梯度值</strong>，是连续值所以要用回归树。</p><p>  对于回归树算法来说最重要的是寻找最佳的划分点，那么回归树中的可划分点包含了所有特征的所有可取的值。在分类树中最佳划分点的判别标准是熵或者基尼系数，都是用纯度来衡量的，但是在回归树中的样本标签是连续数值，所以再使用熵之类的指标不再合适，取而代之的是平方误差，它能很好的评判拟合程度。</p><p><strong>回归树生成算法:</strong><br>输入: 训练数据集 $D$<br>输出: 回归树 $f(x)$<br>在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树：</p><p>(1) 选择最优切分变量 $j$ 与切分点 $s$， 求解:</p><script type="math/tex; mode=display">\min_{j,s}[\min_{c_1}\sum_{x_i \in R_1(j, s)} (y_i-c_1)^2+\min_{c_2}\sum_{x_i \in R_2(j, s)} (y_i-c_2)^2]</script><p>遍历变量 $j$，对固定的切分变量j扫描切分点 $s$，选择使得上式达到最小值的对  $(j,s)$.<strong>简要解释一下上述公式</strong>：中括号里面的公式是求出每个特征变量在哪一个划分点时损失函数最小，最外面的 $\min$ 是在所有特征值，求得使损失函数全局最小的特征及其切分点$(j^<em>, s^</em>)$;</p><p>(2) 用选定的对 $(j,s)$ 划分区域并决定相应的输出值：</p><script type="math/tex; mode=display">R_1(j, s)=\{x|x^{(j)}\leq s\},R_2(j, s)=\{x|x^{(j)} > s\}</script><script type="math/tex; mode=display">\hat {c_m}=\frac{1}{N}\sum_{x_1 \in R_m(j, s)}y_i, x \in R_m, m=1,2</script><p>求划分区域的输出值就是将该区域的所有样本的输出值求平均。</p><p>(3)继续对两个子区域调用步骤（1）和（2），直至满足停止条件。</p><p>(4)将输入空间划分为M个区域$R_1, R_2…R_M$，得到决策树</p><script type="math/tex; mode=display">f(x)=\sum_{m=1}^M \hat{c_m}I(x \in R_m)</script><h1 id="2-Gradient-Boosting：拟合负梯度"><a href="#2-Gradient-Boosting：拟合负梯度" class="headerlink" title="2. Gradient Boosting：拟合负梯度"></a>2. Gradient Boosting：拟合负梯度</h1><p>梯度提升树（Grandient Boosting）是提升树（Boosting Tree）的一种改进算法，所以在讲梯度提升树之前先来说一下提升树。</p><p>先来个通俗理解：假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。最后将每次拟合的岁数加起来便是模型输出的结果。</p><p><strong>提升树算法:</strong></p><p>(1) 初始化 $f_0(x)=0$</p><p>(2) 对$m=1, 2…M$</p><p>(a)计算残差</p><script type="math/tex; mode=display">r_{mi}=y_i-f_{m-1}(x_i), i=1, 2,...,M</script><p>(b) 拟合残差 $r_{mi}$ 学习一个回归树，得到 $h_m(x)$</p><p>(c) 更新 $f_m(x)=f_{m-1}(x)+h_m(x)$</p><p>(3)得到回归树</p><script type="math/tex; mode=display">f_{M}(x)=\sum_{m=1}^Mh_m(x)</script><p>上面伪代码中的残差是什么？</p><p>在提升树算法中，假设我们前一轮迭代得到的强学习器是</p><script type="math/tex; mode=display">f_{t-1}(x)</script><p>损失函数是</p><script type="math/tex; mode=display">L(y, f_{t-1}(x))</script><p>我们本轮迭代的目标是找到一个弱学习器</p><script type="math/tex; mode=display">h_{t}(x)</script><p>当采用平方损失函数时</p><script type="math/tex; mode=display">\begin{aligned}    & L(y, f_{t-1}(x)+h_t(x)) \\    & = (y - f_{t-1}(x) - h_t(x))^2 \\    & =(r - h_t(x))^2 \\\end{aligned}</script><p>这里，</p><script type="math/tex; mode=display">r = y - f_{t-1}(x)</script><p>是当前模型拟合数据的残差（residual）。所以，对于提升树来说只需要简单地拟合当前模型的残差。</p><p>回到我们上面讲的那个通俗易懂的例子中，第一次迭代的残差是10岁，第二次残差4岁…</p><p>当损失函数是平方损失和指数损失函数时，梯度提升树每一步优化是很简单的，但是对于一般损失函数而言，往往每一步优化起来不那么容易，针对这一问题，Freidman提出了梯度提升树算法，这是利用最速下降的近似方法，<strong>其关键是利用损失函数的负梯度作为提升树算法中的残差的近似值。</strong></p><p>那么负梯度长什么样呢？</p><p>第t轮的第i个样本的损失函数的负梯度为：</p><script type="math/tex; mode=display">-[\frac{\partial {L(y, f(x_i))}}{\partial {f(x_i)}}]_{f(x)=f_{t-1}(x)}</script><p>此时不同的损失函数将会得到不同的负梯度，如果选择平方损失</p><script type="math/tex; mode=display">L(y, f(x_i))=\frac{1}{2}(y-f(x_i))^2</script><p>负梯度为</p><script type="math/tex; mode=display">-[\frac{\partial {L(y, f(x_i))}}{\partial {f(x_i)}}]_{f(x)=f_{t-1}(x)}=-[\frac{\partial \frac{1}{2}(y-f(x_i))^2}{\partial {f(x_i)}}]_{f(x)=f_{t-1}(x)}=y-f(x_i)</script><p>此时我们发现GBDT的<strong>负梯度就是残差</strong>，所以说对于回归问题，我们要拟合的就是残差。<br> <br>那么对于分类问题呢？二分类和多分类的损失函数都是log loss，<strong>本文以回归问题为例进行讲解。</strong></p><h1 id="3-GBDT算法原理"><a href="#3-GBDT算法原理" class="headerlink" title="3. GBDT算法原理"></a>3. GBDT算法原理</h1><p> <br>上面两节分别将Decision Tree和Gradient Boosting介绍完了，下面将这两部分组合在一起就是我们的GBDT了。</p><p>GBDT算法：<br>（1）初始化弱学习器</p><script type="math/tex; mode=display">f_0(x)=\arg \min_{c}\sum_{i=1}^{N}L(y_i, c)</script><p>后面有证明，党委平方损失时，$f_0(x)=\frac{\sum_{i=1}^N y_i}{N}$</p><p>(2) 对m=1,2,…,M有：</p><p>（a）对每个样本i=1,2,…,N，计算负梯度，即残差</p><script type="math/tex; mode=display">r_{im}=-[\frac{\partial{L(y_i, f(x_i))}}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}</script><p>（b）将上步得到的残差作为样本新的真实值，并将数据 $(x_i, x_im), i=1, 2,…,N$ 作为下棵树的训练数据，得到一颗新的回归树 $f_m(x)$，其对应的叶子节点区域为 $R_jm, j=1, 2,…,J$。其中J为回归树t的叶子节点的个数。</p><p>（c）对叶子区域 $j =1,2,..J$ 计算最佳拟合值</p><script type="math/tex; mode=display">\gamma_{jm}=\arg \min_{\gamma}\sum_{x_i \in R_{jm}}L(y_i, f_{m-1}(x_i)+\gamma) (对 \gamma求导并令导数为0即可求得)</script><p>（d）更新强学习器</p><script type="math/tex; mode=display">f_m{x}=f_{m-1}(x)+\sum_{j=1}^{J}\gamma_{jm}I(x \in R_{jm})</script><p>(3)得到最终学习器</p><script type="math/tex; mode=display">f(x)=f_M{x}=f_{0}(x)+\sum_{m=1}^{M}\sum_{j=1}^{J}\gamma_{jm}I(x \in R_{jm})</script><h1 id="4-实例详解"><a href="#4-实例详解" class="headerlink" title="4. 实例详解"></a>4. 实例详解</h1><p><strong>==本人用python以及pandas库实现GBDT的简易版本，在下面的例子中用到的数据都在github可以找到，大家可以结合代码和下面的例子进行理解，欢迎star~== </strong></p><p><strong>Github：<a href="https://github.com/Freemanzxp/GBDT_Simple_Tutorial" target="_blank" rel="noopener">https://github.com/Freemanzxp/GBDT_Simple_Tutorial</a></strong></p><p><strong>数据介绍：</strong></p><p>如下表所示：一组数据，特征为年龄、体重，身高为标签值。共有5条数据，前四条为训练样本，最后一条为要预测的样本。</p><p><img src="\images\ml_gbdt_01.webp" alt=""></p><p><strong>训练阶段：</strong></p><p><strong>参数设置：</strong></p><ul><li><p>学习率：learning_rate=0.1</p></li><li><p>迭代次数：n_trees=5</p></li><li><p>树的深度：max_depth=3</p></li></ul><p><strong>1.初始化弱学习器:</strong></p><script type="math/tex; mode=display">f_0(x)=arg min_c\sum_{i=1}^N L(y_i, c)</script><p>损失函数为平方损失，因为平方损失函数是一个凸函数，直接求导，倒数等于零，得到 $c$。</p><script type="math/tex; mode=display">\sum_{i=1}^N\frac{\partial {L(y_i, c)}}{\partial c}=\sum_{i=1}^N \frac{\partial {\frac{1}{2}(y_i-c)^2}}{\partial c}=\sum_{i=1}^N(c - y_i)</script><p>令导数等于0</p><script type="math/tex; mode=display">\sum_{i=1}^N(c - y_i)=c - \sum_{i=1}^N y_i = 0</script><script type="math/tex; mode=display">c = (\sum_{i=1}^N y_i)/N</script><p>所以初始化时，$c$ 取值为所有训练样本标签值的均值。$c=(1.1+1.3+1.7+1.8)/4=1.475$，此时得到初始学习器 $f_0(x)$</p><script type="math/tex; mode=display">f_0(x)=c=1.475</script><p><strong>2.对迭代轮数m=1，2,…,M:</strong><br> <br>由于我们设置了迭代次数：n_trees=5，这里的M=5。</p><p>计算负梯度，根据上文损失函数为平方损失时，负梯度就是残差残差，再直白一点就是 y与上一轮得到的学习器 $f_{m-1}$ 的差值</p><script type="math/tex; mode=display">r_{i1}=-[\frac{\partial{L(y_i, f(x_i))}}{\partial f(x_i)}]_{f(x)=f_{0}(x)}</script><p>残差在下表列出</p><p><img src="\images\ml_gbdt_02.webp" alt=""></p><p>此时将残差作为样本的真实值来训练弱学习器 $f_1(x)$，即下表数据：</p><p><img src="\images\ml_gbdt_03.webp" alt=""></p><p>接着，寻找回归树的最佳划分节点，遍历每个特征的每个可能取值。从年龄特征的5开始，到体重特征的70结束，分别计算分裂后两组数据的平方损失（Square Error），$SE_l$左节点平方损失，$SE_r$ 右节点平方损失，找到使平方损失和 $SE_{sum}=SE_l+SE_r$ 最小的那个划分节点，即为最佳划分节点。</p><p>例如：以年龄7为划分节点，将小于7的样本划分为到左节点，大于等于7的样本划分为右节点。</p><p>左节点包括 $x_0$，右节点包括样本 $x_1,x_2,x_3$，$SE_l=0, SE_r=0.047, SE_{sum}=0.047$，所有可能划分情况如下表所示：<br><img src="\images\ml_gbdt_04.webp" alt=""></p><p>以上划分点是的总平方损失最小为0.025有两个划分点：年龄21和体重60，所以随机选一个作为划分点，这里我们选 年龄21<br> 现在我们的第一棵树长这个样子：</p><p><img src="\images\ml_gbdt_05.webp" alt=""></p><p>我们设置的参数中树的深度max_depth=3，现在树的深度只有2，需要再进行一次划分，这次划分要对左右两个节点分别进行划分：</p><p>对于<strong>左节点</strong>，只含有0,1两个样本，根据下表我们选择<strong>年龄7</strong>划分</p><p><img src="\images\ml_gbdt_06.webp" alt=""></p><p>对于<strong>右节点</strong>，只含有2,3两个样本，根据下表我们选择<strong>年龄30</strong>划分（也可以选<strong>体重70</strong>）</p><p><img src="\images\ml_gbdt_07.webp" alt=""></p><p>现在我们的第一棵树长这个样子：</p><p><img src="\images\ml_gbdt_08.webp" alt=""></p><p>此时我们的树深度满足了设置，还需要做一件事情，给这每个叶子节点分别赋一个参数Υ，来拟合残差。</p><script type="math/tex; mode=display">\gamma_{j1}=\arg \min_{\gamma}\sum_{x_i \in R_{j1}}L(y_i, f_{0}(x_i)+\gamma)</script><p>这里其实和上面初始化学习器是一个道理，平方损失求导，令导数等于零，化简之后得到每个叶子节点的参数 $Υ$，其实就是标签值的均值。这个地方的标签值不是原始的 $y$，而是本轮要拟合的标残差 $y-f_0(x)$。</p><p>根据上述划分结果，为了方便表示，规定从左到右为第1,2,3,4个叶子结点</p><p><img src="\images\ml_gbdt_09.webp" alt=""></p><p>此时的树长这个样子：</p><p><img src="\images\ml_gbdt_10.webp" alt=""></p><p>此时可更新强学习器，需要用到参数学习率：learning_rate=0.1，用lr表示。</p><script type="math/tex; mode=display">f_1(x)=f_0(x)+ lr * \sum_{j=1}^4\gamma_{j1}I(x \in R_{j1})</script><p>为什么要用学习率呢？这是<strong>Shrinkage</strong>的思想，如果每次都全部加上（学习率为1）很容易一步学到位导致过拟合。</p><p><strong>重复此步骤，直到 m&gt;5 结束，最后生成5棵树。</strong></p><p>下面将展示每棵树最终的结构，这些图都是GitHub上的代码生成的，感兴趣的同学可以去一探究竟</p><p><strong>Github：<a href="https://github.com/Freemanzxp/GBDT_Simple_Tutorial" target="_blank" rel="noopener">https://github.com/Freemanzxp/GBDT_Simple_Tutorial</a></strong><br><strong>第一棵树：</strong></p><p><img src="\images\ml_gbdt_11.webp" alt=""></p><p><strong>第二棵树：</strong><br><img src="\images\ml_gbdt_12.webp" alt=""></p><p><strong>第三棵树：</strong><br><img src="\images\ml_gbdt_13.webp" alt=""></p><p><strong>第四棵树：</strong><br><img src="\images\ml_gbdt_14.webp" alt=""></p><p><strong>第五棵树：</strong><br><img src="\images\ml_gbdt_15.webp" alt=""></p><p><strong>3.得到最后的强学习器： </strong></p><script type="math/tex; mode=display">f(x)=f_5(x)=f_0(x)+lr*(\sum_{m=1}^5\sum_{j=1}^4\gamma_{jm}I(x \in R_{jm}))</script><p>5.预测样本5：</p><script type="math/tex; mode=display">f_0(x)=1.475</script><p>$f_1(x)$在中，样本4的年龄为25，大于划分节点21岁，又小于30岁，所以被预测为<strong>0.2250</strong>。</p><p>$f_2(x)$在中，样本4的…此处省略…所以被预测为<strong>0.2025</strong></p><p>==为什么是0.2025？这是根据第二颗树得到的，可以GitHub简单运行一下代码==</p><p>$f_3(x)$在中，样本4的…此处省略…所以被预测为<strong>0.1823</strong></p><p>$f_4(x)$在中，样本4的…此处省略…所以被预测为<strong>0.1640</strong></p><p>$f_5(x)$在中，样本4的…此处省略…所以被预测为<strong>0.1476</strong></p><p>最终预测结果：</p><script type="math/tex; mode=display">f(x)=1.475+0.1*(0.225+0.2025+0.1823+0.164+0.1476)=1.56714</script><p><strong>5. 总结</strong></p><p>本文章从GBDT算法的原理到实例详解进行了详细描述，但是目前只写了回归问题，GitHub上的代码也是实现了回归、二分类、多分类以及树的可视化，希望大家继续批评指正，感谢各位的关注，如果对你有用欢迎star。</p><p><strong>参考：</strong></p><ol><li><p>李航 《统计学习方法》</p></li><li><p>Friedman J H . Greedy Function Approximation: A Gradient Boosting Machine[J]. The Annals of Statistics, 2001, 29(5):1189-1232.</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;该内容完全转载自&lt;a href=&quot;http://blog.csdn.net/zpalyq110/article/details/79527653&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;同名CSDN博客&lt;/a&gt;(&lt;a href=&quot;http://blo
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习中的熵、条件熵、相对熵(KL散度)和交叉熵</title>
    <link href="http://yoursite.com/2019/04/26/ML-entropy/"/>
    <id>http://yoursite.com/2019/04/26/ML-entropy/</id>
    <published>2019-04-26T02:47:10.000Z</published>
    <updated>2019-04-27T05:54:28.287Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2019/04/26/entropy/" target="_blank" rel="noopener">GitHub</a><br><a href="https://www.jianshu.com/p/31a683cddb94" target="_blank" rel="noopener">简书</a><br><a href="https://blog.csdn.net/u014630987/article/details/89544279" target="_blank" rel="noopener">CSDN</a></p><p>该文章转载自<a href="https://www.cnblogs.com/kyrieng/p/8694705.html" target="_blank" rel="noopener">详解机器学习中的熵、条件熵、相对熵和交叉熵</a></p><h1 id="1、信息熵-information-entropy"><a href="#1、信息熵-information-entropy" class="headerlink" title="1、信息熵 (information entropy)"></a>1、信息熵 (information entropy)</h1><p>熵 (entropy) 这一词最初来源于热力学。1948年，克劳德·爱尔伍德·香农将热力学中的熵引入信息论，所以也被称为香农熵 (Shannon entropy)，信息熵 (information entropy)。本文只讨论信息熵。首先，我们先来理解一下信息这个概念。信息是一个很抽象的概念，百度百科将它定义为：指音讯、消息、通讯系统传输和处理的对象，泛指人类社会传播的一切内容。那信息可以被量化么？可以的！香农提出的“信息熵”概念解决了这一问题。</p><p>一条信息的信息量大小和它的不确定性有直接的关系。我们需要搞清楚一件非常非常不确定的事，或者是我们一无所知的事，就需要了解大量的信息。相反，如果我们对某件事已经有了较多的了解，我们就不需要太多的信息就能把它搞清楚。所以，从这个角度，我们可以认为，信息量的度量就等于不确定性的多少。比如，有人说广东下雪了。对于这句话，我们是十分不确定的。因为广东几十年来下雪的次数寥寥无几。为了搞清楚，我们就要去看天气预报，新闻，询问在广东的朋友，而这就需要大量的信息，信息熵很高。再比如，中国男足进军2022年卡塔尔世界杯决赛圈。对于这句话，因为确定性很高，几乎不需要引入信息，信息熵很低。</p><p>考虑一个离散的随机变量 $x$，由上面两个例子可知，信息的量度应该依赖于概率分布$p(x)$ ，因此我们想要寻找一个函数 $I(x)$，它是概率 $p(x)$ 的单调函数，表达了信息的内容。怎么寻找呢？如果我们有两个不相关的事件 $x$ 和 $y$，那么观察两个事件同时发生时获得的信息量应该等于观察到事件各自发生时获得的信息之和，即：</p><script type="math/tex; mode=display">I(x,y)=I(x)+I(y)</script><p>因为两个事件是独立不相关的，因此 $p(x,y)=p(x)p(y)$。根据这两个关系，很容易看出 $I(x)$一定与 $p(x)$ 的对数有关 (因为对数的运算法则是 $log_a(mn)=log_am+log_an$)。因此，我们有</p><script type="math/tex; mode=display">I(x)=−\log_p(x)</script><p>其中负号是用来保证信息量是正数或者零。而 $log$ 函数基的选择是任意的（信息论中基常常选择为2，因此信息的单位为比特bits；而机器学习中基常常选择为自然常数，因此单位常常被称为奈特nats）。$I(x)$ 也被称为随机变量 x 的自信息 (self-information)，描述的是随机变量的某个事件发生所带来的信息量。图像如图：<br><img src="\images\entropy_01.png" alt=""></p><p>最后，我们正式引出信息熵。 现在假设一个发送者想传送一个随机变量的值给接收者。那么在这个过程中，他们传输的平均信息量可以通过求 $I(x)=−logp(x)$ 关于概率分布 $p(x)$ 的期望得到，即：</p><script type="math/tex; mode=display">H(X)=−∑xp(x)logp(x)=−∑i=1np(xi)logp(xi)</script><p>$H(X)$ 就被称为随机变量 $x$ 的<strong>熵,它是表示随机变量不确定的度量，是对所有可能发生的事件产生的信息量的期望。</strong></p><p>从公式可得，随机变量的取值个数越多，状态数也就越多，信息熵就越大，混乱程度就越大。当随机分布为均匀分布时，熵最大，且 $0≤H(X)≤logn$。稍后证明。将一维随机变量分布推广到多维随机变量分布，则其联合熵 (Joint entropy) 为：</p><script type="math/tex; mode=display">H(X,Y)=−∑x,yp(x,y)logp(x,y)=−∑i=1n∑j=1mp(xi,yi)logp(xi,yi)</script><p>注意点：1、熵只依赖于随机变量的分布,与随机变量取值无关，所以也可以将 $X$ 的熵记作 $H(p)$。2、令$0log0=0$(因为某个取值概率可能为0)。</p><p>那么这些定义有着什么样的性质呢？考虑一个随机变量 $x$。这个随机变量有4种可能的状态，每个状态都是等可能的。为了把 x 的值传给接收者，我们需要传输2比特的消息。$H(X)=−4×14log214=2 bits$</p><p>现在考虑一个具有4种可能的状态 {a,b,c,d} 的随机变量，每个状态各自的概率为 (12,14,18,18)</p><p>这种情形下的熵为：</p><script type="math/tex; mode=display">H(X)=−12log212−14log214−18log218−18log218=1.75 bits</script><p>我们可以看到，<strong>非均匀分布比均匀分布的熵要小</strong>。现在让我们考虑如何把变量状态的类别传递给接收者。与之前一样，我们可以使用一个2比特的数字来完成这件事情。然而，我们可以利用非均匀分布这个特点，<strong>使用更短的编码来描述更可能的事件，使用更长的编码来描述不太可能的事件</strong>(这点和Huffman编码的原理一样，不知道Huffman树是不是根据熵的原理发明出来的）。我们希望这样做能够得到一个更短的平均编码长度。我们可以使用下面的编码串（哈夫曼编码）：0、10、110、111来表示状态 {a,b,c,d}。传输的编码的平均长度就是：</p><script type="math/tex; mode=display">average code length = 12×1+14×2+2×18×3=1.75 bits</script><p>这个值与上方的随机变量的熵相等。熵和最短编码长度的这种关系是一种普遍的情形。</p><p><a href="https://baike.baidu.com/item/Shannon%20%E7%BC%96%E7%A0%81%E5%AE%9A%E7%90%86/15585931?fr=aladdin" target="_blank" rel="noopener">Shannon 编码定理</a> 表明熵是传输一个随机变量状态值所需的比特位下界（最短平均编码长度）。因此，信息熵可以应用在数据压缩方面。这里<a href="http://www.ruanyifeng.com/blog/2014/09/information-entropy.html" target="_blank" rel="noopener">这篇文章</a>讲的很详细了，我就不赘述了。</p><p>证明<script type="math/tex">0≤H(X)≤logn</script></p><p>利用拉格朗日乘子法证明：</p><p>因为 <script type="math/tex">p(1)+p(2)+⋯+p(n)=1</script></p><p>所以有</p><p><strong>目标函数</strong>：$f(p(1),p(2),…,p(n))=−(p(1)logp(1)+p(2)logp(2)+⋯+p(n)logp(n))$</p><p><strong>约束条件</strong>：$g(p(1),p(2),…,p(n),λ)=p(1)+p(2)+⋯+p(n)−1=0$</p><p>1、定义拉格朗日函数：</p><script type="math/tex; mode=display">L(p(1),p(2),…,p(n),λ)=−(p(1)logp(1)+p(2)logp(2)+⋯+p(n)logp(n))+λ(p(1)+p(2)+⋯+p(n)−1)</script><p>　2、$L(p(1),p(2),…,p(n),λ)$分别对 $p(1),p(2),p(n),λ$ 求偏导数，令偏导数为 0：</p><script type="math/tex; mode=display">λ−log(e⋅p(1))=0</script><script type="math/tex; mode=display">λ−log(e⋅p(2))=0</script><script type="math/tex; mode=display">……</script><script type="math/tex; mode=display">λ−log(e⋅p(n))=0</script><script type="math/tex; mode=display">p(1)+p(2)+⋯+p(n)−1=0</script><p>　　3、求出 $p(1),p(2),…,p(n)$ 的值：</p><p>解方程得，$p(1)=p(2)=⋯=p(n)=1n$</p><p>代入 $f(p(1),p(2),…,p(n))$ 中得到目标函数的极值为 $f(1n,1n,…,1n)=−(1nlog1n+1nlog1n+⋯+1nlog1n)=−log(1n)=logn$</p><p><strong>由此可证 logn 为最大值。</strong></p><p>个人感觉上述证明不是很严谨的拉格朗日推导，见<a href="https://wanghuaishi.wordpress.com/2017/02/21/%E5%9B%BE%E8%A7%A3%E6%9C%80%E5%A4%A7%E7%86%B5%E5%8E%9F%E7%90%86%EF%BC%88the-maximum-entropy-principle%EF%BC%89/" target="_blank" rel="noopener">另一篇文章</a></p><h1 id="2、条件熵-Conditional-entropy"><a href="#2、条件熵-Conditional-entropy" class="headerlink" title="2、条件熵 (Conditional entropy)"></a>2、条件熵 (Conditional entropy)</h1><p>条件熵 $H(Y|X)$ 表示在已知随机变量 $X$ 的条件下随机变量 $Y$ 的不确定性。条件熵 $H(Y|X)$ 定义为 $X$ 给定条件下 $Y$ 的条件概率分布的熵对  $X$ 的数学期望：</p><p><img src="\images\entropy_03.png" alt=""></p><p>条件熵 $H(Y|X)$ 相当于联合熵 $H(X,Y)$ 减去单独的熵 $H(X)$，即</p><p>$H(Y|X)=H(X,Y)−H(X)$，证明如下：</p><p><img src="\images\entropy_03.png" alt=""></p><p>举个例子，比如环境温度是低还是高，和我穿短袖还是外套这两个事件可以组成联合概率分布 $H(X,Y)$，因为两个事件加起来的信息量肯定是大于单一事件的信息量的。假设 $H(X)$ 对应着今天环境温度的信息量，由于今天环境温度和今天我穿什么衣服这两个事件并不是独立分布的，所以在已知今天环境温度的情况下，我穿什么衣服的信息量或者说不确定性是被减少了。当已知 $H(X)$ 这个信息量的时候，$H(X,Y)$ 剩下的信息量就是条件熵：</p><script type="math/tex; mode=display">H(Y|X)=H(X,Y)−H(X)</script><p>因此，可以这样理解，描述 $X$ 和 $Y$ 所需的信息是描述 $X$ 自己所需的信息,加上给定  $X$ 的条件下具体化  $Y$ 所需的额外信息。关于条件熵的例子可以看这篇文章，讲得很详细。<a href="https://zhuanlan.zhihu.com/p/26551798" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26551798</a></p><h1 id="3、相对熵-Relative-entropy-，也称KL散度-Kullback–Leibler-divergence"><a href="#3、相对熵-Relative-entropy-，也称KL散度-Kullback–Leibler-divergence" class="headerlink" title="3、相对熵 (Relative entropy)，也称KL散度 (Kullback–Leibler divergence)"></a>3、相对熵 (Relative entropy)，也称KL散度 (Kullback–Leibler divergence)</h1><p>设 $p(x)、q(x)$ 是 离散随机变量 $X$ 中取值的两个概率分布，则 $p$ 对 $q$ 的相对熵是：</p><script type="math/tex; mode=display">DKL(p||q)=\sum_xp(x)log\frac{p(x)}{q(x)}=E_{p(x)}log\frac{p(x)}{q(x)}</script><p>性质：</p><p>1、如果 $p(x)$ 和 $q(x)$ 两个分布相同，那么相对熵等于0</p><p>2、$DKL(p||q)≠DKL(q||p)$ ，相对熵具有不对称性。大家可以举个简单例子算一下。</p><p>3、$DKL(p||q)≥0$ 证明如下（利用Jensen不等式<a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Jensen%27s_inequality</a>):</p><p><img src="\images\entropy_05.png" alt=""></p><p>因为：</p><script type="math/tex; mode=display">\sum_xp(x)=1</script><p>所以：</p><script type="math/tex; mode=display">DKL(p||q)≥0</script><p> <strong>总结：相对熵可以用来衡量两个概率分布之间的差异，上面公式的意义就是求 $p$ 与 $q$ 之间的对数差在 $p$ 上的期望值。</strong></p><h1 id="4、交叉熵-Cross-entropy"><a href="#4、交叉熵-Cross-entropy" class="headerlink" title="4、交叉熵 (Cross entropy)"></a>4、交叉熵 (Cross entropy)</h1><p> 现在有关于样本集的两个概率分布 $p(x)$ 和 $q(x)$，其中  $p(x)$ 为真实分布， $q(x)$ 非真实分布。如果用真实分布 $p(x)$ 来衡量识别别一个样本所需要编码长度的期望（平均编码长度）为:</p><script type="math/tex; mode=display">H(p)=\sum_xp(x)log\frac{1}{p(x)}=-\sum_xp(x)log{p(x)}</script><p>如果使用非真实分布 $q(x)$ 来表示来自真实分布 $p(x)$ 的平均编码长度，则是：</p><p>$H(p)=\sum_xp(x)log\frac{1}{q(x)}=-\sum_xp(x)log{q(x)}$。（因为用 $q(x)$ 来编码的样本来自于分布 $q(x)$ ，所以 $H(p,q)$ 中的概率是 $p(x)$）。此时就将 $H(p,q)$ 称之为交叉熵。举个例子。考虑一个随机变量 $x$，真实分布$p(x)=(12,14,18,18)$，非真实分布 $q(x)=(14,14,14,14)$， 则$H(p)=1.75 bits$（最短平均码长），交叉熵 $H(p,q)=12log24+14log24+18log24+18log24=2 bits$。由此可以看出根据非真实分布 $q(x)$ 得到的平均码长大于根据真实分布 $p(x)$ 得到的平均码长。</p><p>我们再化简一下相对熵的公式。</p><script type="math/tex; mode=display">DKL(p||q)=\sum_xp(x)log \frac{p(x)}{q(x)}=\sum_xp(x)logp(x)−\sum_x p(x)logq(x)</script><p>有没有发现什么？</p><p>熵的公式 $H(p)=−\sum_xp(x)logp(x)$</p><p>交叉熵的公式 $H(p,q)=\sum_xp(x)log\frac{1}{q(x)}=−\sum_xp(x)logq(x)$</p><p>所以有：</p><p>$DKL(p||q)=H(p,q)−H(p)$（当用非真实分布 $q(x)$ 得到的平均码长比真实分布 p(x) 得到的平均码长多出的比特数就是相对熵）</p><p>又因为 $DKL(p||q)≥0$</p><p>所以 $H(p,q)≥H(p)$（当 $p(x)=q(x)$ 时取等号，此时交叉熵等于信息熵）</p><p>并且当 $H(p)$ 为常量时（注：在机器学习中，训练数据分布是固定的），最小化相对熵 $DKL(p||q)$ 等价于最小化交叉熵 $H(p,q)$ 也等价于最大化似然估计（具体参考Deep Learning 5.5）。</p><p>在机器学习中，我们希望在训练数据上模型学到的分布 $P(model)$ 和真实数据的分布  $P(real)$ 越接近越好，所以我们可以使其相对熵最小。但是我们没有真实数据的分布，所以只能希望模型学到的分布 $P(model)$ 和训练数据的分布 $P(train)$ 尽量相同。假设训练数据是从总体中独立同分布采样的，那么我们可以通过最小化训练数据的经验误差来降低模型的泛化误差。即：</p><p>希望学到的模型的分布和真实分布一致，$P(model)≃P(real)$<br>但是真实分布不可知，假设训练数据是从真实数据中独立同分布采样的，$P(train)≃P(real)$<br>因此，我们希望学到的模型分布至少和训练数据的分布一致，$P(train)≃P(model)$<br>根据之前的描述，最小化训练数据上的分布  $P(train)$ 与最小化模型分布 $P(model)$ 的差异等价于最小化相对熵，即 $DKL(P(train)||P(model))$。此时，$ P(train) $就是$DKL(p||q)$ 中的 $p$，即真实分布，$P(model)$ 就是 $q$。又因为训练数据的分布 $p$ 是给定的，所以求  $DKL(p||q)$  等价于求 $H(p,q)$。得证，交叉熵可以用来计算学习模型分布与训练分布之间的差异。交叉熵广泛用于逻辑回归的Sigmoid和Softmax函数中作为损失函数使用。这篇文章先不说了。</p><h1 id="5、总结"><a href="#5、总结" class="headerlink" title="5、总结"></a>5、总结</h1><p>信息熵是衡量随机变量分布的混乱程度，是随机分布各事件发生的信息量的期望值，随机变量的取值个数越多，状态数也就越多，信息熵就越大，混乱程度就越大。当随机分布为均匀分布时，熵最大；信息熵推广到多维领域，则可得到联合信息熵；条件熵表示的是在 X 给定条件下，Y 的条件概率分布的熵对 X的期望。<br>相对熵可以用来衡量两个概率分布之间的差异。<br>交叉熵可以来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小。</p><p>或者：</p><p>信息熵是传输一个随机变量状态值所需的比特位下界（最短平均编码长度）。<br>相对熵是指用 q 来表示分布 p  额外需要的编码长度。<br>交叉熵是指用分布 q 来表示本来表示分布 p 的平均编码长度。</p><h1 id="6、参考"><a href="#6、参考" class="headerlink" title="6、参考"></a>6、参考</h1><p>1、吴军《数学之美》</p><p>2、李航《统计学习方法》</p><p>3、马春鹏《模式识别与机器学习》</p><p>3、<a href="https://www.zhihu.com/question/41252833" target="_blank" rel="noopener">如何通俗的解释交叉熵与相对熵</a></p><p>4、<a href="https://www.zhihu.com/question/65288314/answer/244557337" target="_blank" rel="noopener">为什么交叉熵（cross-entropy）可以用于计算代价？</a></p><p>5、<a href="https://baike.baidu.com/item/%E4%BA%A4%E5%8F%89%E7%86%B5/8983241?fr=aladdin" target="_blank" rel="noopener">交叉熵的百度百科解释</a></p><p>6、<a href="https://blog.csdn.net/saltriver/article/details/53056816" target="_blank" rel="noopener">信息熵到底是什么</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2019/04/26/entropy/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://www.jianshu.com/
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习面试题库——笔试题</title>
    <link href="http://yoursite.com/2019/04/16/machine-learning-question/"/>
    <id>http://yoursite.com/2019/04/16/machine-learning-question/</id>
    <published>2019-04-16T04:30:48.000Z</published>
    <updated>2019-04-16T05:18:30.133Z</updated>
    
    <content type="html"><![CDATA[<p>该资源来自<a href="http://www.julyedu.com/" target="_blank" rel="noopener">七月在线网站</a></p><h1 id="1、-如果线性回归模型中的随机误差存在异方差性，那么参数的OLS估计量是（-）"><a href="#1、-如果线性回归模型中的随机误差存在异方差性，那么参数的OLS估计量是（-）" class="headerlink" title="1、 如果线性回归模型中的随机误差存在异方差性，那么参数的OLS估计量是（ ）"></a>1、 如果线性回归模型中的随机误差存在异方差性，那么参数的OLS估计量是（ ）</h1><ol><li>无偏的，有效的</li><li><strong>无偏的，非有效的</strong></li><li>有偏的，有效的</li><li>有偏的，非有效的</li></ol><blockquote><p>OLS即普通最小二乘法。由高斯—马尔可夫定理，在给定经典线性回归的假定下，最小二乘估计量是具有最小方差的线性无偏估计量。根据证明过程可知，随机误差中存在异方差性不会影响其无偏性，而有效性证明中涉及同方差性，即异方差会影响参数OLS估计量的有效性。</p><p>经典线性回归模型的一个重要假定：总体回归函数中的随机误差项满足同方差性，即它们都有相同的方差。如果这一假定不满足，即：随机误差项具有不同的方差，则称线性回归模型存在异方差性。</p></blockquote><h1 id="2-信息增益"><a href="#2-信息增益" class="headerlink" title="2 信息增益"></a>2 信息增益</h1><p><img src="/images/ml-question-002.png" alt=""></p><h1 id="3-在二分类问题中，当测试集的正例和负例数量不均衡时，以下评价方案哪个是相对不合理的（-）（假设precision-TP-TP-FP-recall-TP-TP-FN-。）"><a href="#3-在二分类问题中，当测试集的正例和负例数量不均衡时，以下评价方案哪个是相对不合理的（-）（假设precision-TP-TP-FP-recall-TP-TP-FN-。）" class="headerlink" title="3  在二分类问题中，当测试集的正例和负例数量不均衡时，以下评价方案哪个是相对不合理的（ ）（假设precision=TP/(TP+FP),recall=TP/(TP+FN)。）"></a>3  在二分类问题中，当测试集的正例和负例数量不均衡时，以下评价方案哪个是相对不合理的（ ）（假设precision=TP/(TP+FP),recall=TP/(TP+FN)。）</h1><ol><li><strong>Accuracy:(TP+TN)/all</strong></li><li>F-value:2<em>recall</em>precision/(recall+precision)</li><li>G-mean:sqrt(precision*recall)</li><li>AUC:ROC曲线下面积</li></ol><blockquote><p>解析：对于分类器，主要的评价指标有precision，recall，F-score，以及ROC曲线等。<br>在二分类问题中，我们主要关注的是测试集的正样本能否正确分类。当样本不均衡时，比如样本中负样本数量远远多于正样本，此时如果负样本能够全部正确分类，而正样本只能部分正确分类，那么(TP+TN)可以得到很高的值，也就是Accuracy是个较大的值，但是正样本并没有取得良好的分类效果。因此A选项是不合理的。在样本不均衡时，可以采用BCD选项方法来评价。<br>ROC曲线以True Positive rate为纵轴，False Positive Rate 为横轴，曲线离（0，1）点越近越好。</p></blockquote><h1 id="4-HMM中维特比算法的复杂度"><a href="#4-HMM中维特比算法的复杂度" class="headerlink" title="4 HMM中维特比算法的复杂度"></a>4 HMM中维特比算法的复杂度</h1><p><img src="/images/ml-question-004.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;该资源来自&lt;a href=&quot;http://www.julyedu.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;七月在线网站&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;1、-如果线性回归模型中的随机误差存在异方差性，那么参数的OLS估计量是（-）&quot;&gt;&lt;a 
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>期望最大算法(EM算法)</title>
    <link href="http://yoursite.com/2019/03/30/ML-EM/"/>
    <id>http://yoursite.com/2019/03/30/ML-EM/</id>
    <published>2019-03-30T04:25:08.000Z</published>
    <updated>2019-04-01T07:39:24.601Z</updated>
    
    <content type="html"><![CDATA[<p>最大期望算法（Expectation-Maximization algorithm, EM）是一类通过迭代进行极大似然估计（Maximum Likelihood Estimation, MLE）的优化算法。用于含有隐变量（Latent Variable) 的概率模型参数的极大似然估计或极大后验概率估计。Em算法的每次迭代包含两个步骤：1. E步： 求期望；2. M步：求极大，因此该算法被称作EM算法。</p><h1 id="1-极大似然估计"><a href="#1-极大似然估计" class="headerlink" title="1 极大似然估计"></a>1 极大似然估计</h1><p>极大似然估计是一种利用极大似然函数来求解模型参数 $\theta$ 的一种估计方法。 在数理统计学中，<strong>似然函数</strong>是一种关于统计模型的参数的函数。在给定一个样本 $x$ 时，关于参数 $\theta$ 的似然函数 $L(\theta|x)$ (在数值上)等于给定参数 $\theta$ 后变量 $x$ 的概率(似然函数与概率的一种关系):</p><script type="math/tex; mode=display">L(\theta|x) = P(X=x|\theta)\tag{1}</script><p>在给定样本集 $\bar x=\{x_1,x_2,…,x_N\}$, 在这个样本集上的似然函数为:</p><script type="math/tex; mode=display">L(\theta|\bar x) = P(\bar x|\theta) = \prod_{i=1}^{N}P(x_i|\theta)\tag{2}</script><p>由于公式2中含有连乘运算，因此不方便计算，通常我们利用对数的特性，来对公式2取对数来得到<strong>对数似然函数</strong>，为了方便，我们还是将 $\log L(\theta|\bar x)$ 记作 $L(\theta|\bar x)$:</p><script type="math/tex; mode=display">L(\theta|\bar x) = \log P(\bar x|\theta) = \log \prod_{i=1}^{N}P(x_i|\theta) = \sum_{i=1}^{N} \log P(x_i|\theta)\tag{3}</script><p>通过 $L(\theta|\bar x)$ 对 $\theta$ 求导，并取倒数为0，此时得到最优的 $\theta$.</p><p>下面以高斯模型为例来简单说明极大似然估计，并引导出EM算法。</p><p><strong>示例一</strong></p><p>如下图，我们采集到一些数据(黄色的+)服从单个高斯分布，由于高斯分布中有两个参数，均值和方差,我们将其表示为 $\theta =\{\mu,\sigma\}$,通过极大似然估计方法我们来求得最优的$\theta^{*} =\{\mu^{*},\sigma^{*}\}$,如下图我们就是从$\theta_1,\theta_2,\theta_3$中求得 $\theta_1$</p><p><img src="/images/ML_single_Gaussia.png" alt="单一高斯极大似然估计"><br>图1 单一高斯模型</p><p>大致思路就是利用公式3求得对数似然函数，然后对其求最大值，即求得 $\theta^*$,该过程通过如两个公式来分别求出 $\mu$ 和 $\sigma$:</p><script type="math/tex; mode=display">L(\theta|\bar x) = \sum_{i=1}^{N}\log N(x_i|\mu, \sigma)\tag{4}</script><script type="math/tex; mode=display">\frac{\partial L(\theta|\bar x)}{\partial \mu} =0\tag{5}</script><script type="math/tex; mode=display">\frac{\partial L(\theta|\bar x)}{\partial \sigma} =0\tag{6}</script><p>我们通过上式可以求得</p><script type="math/tex; mode=display">\begin{aligned}    \mu_{MLE}=&\frac{1}{N}\sum_{i=1}^Nx_i \\    \sigma_{MLE}=&\frac{\sum_{i=1}^N(x_i-\mu_{MLE})^2}{N}\end{aligned}\tag{7}</script><p><strong>示例二</strong></p><p>如图二所示的样本，从图中我们可以明显看出，黄色和黑色样本分别位于两个类中，此时如果我们还是用单一的高斯模型来为该样本集建模，是行不通的，主要有两个原因：</p><ol><li>高斯分布的特点是在 $\mu$ 附近的概率很大，而从徒儿看出，在 $\mu$ 附近没有样本，样本基本位于两端，距离 $\mu$ 很远，这不符合高斯分布特点；</li><li>黄色的样本比黑色的样本更密集，也就是他的概率更大，因此我们不应该同等对待。</li></ol><p><img src="/images/ML_EM_pic2.png" alt="混合样本单一模型"></p><p>既然单一高斯模型已经无法适用于该样本集，那我们用多个高斯模型呢，再此示例中，我们可以看出样本集分布在两个分布中较为合理，其结果如图 3所示.通过比较图2和图3，可以得出，图3时更合理的分布，此时得到高斯混合模型，其混合模型如图4所示。</p><p><img src="/images/ML_EM_pic3.png" alt="图3 双高斯模型"><br><img src="/images/ML_EM_pic4.png" alt="图4 混合高斯模型"></p><p>根据上面的描述，我们对每个高斯叠加起来，即:</p><script type="math/tex; mode=display">    P(x_i|\theta) = \sum_{l=1}^kN(x_i|\mu_l,\sigma_l)    \tag{8}</script><p>但是，我们把公式8对变量 $x$ 进行积分，按照概率，该积分应该为1，事实上，该积分不为一，而为2.为了解决这个问题，我们可以平均每个高斯，即给一个权重 $\frac{1}{2}$, 但是这部理想，有时样本在不同分布中权重不一致，因此我们可以给每个分布一个权重比例，且比例之和为1，我们将其推广到一般形式，即：</p><script type="math/tex; mode=display">\begin{aligned}    & P(x|\bar \theta) = \sum_{i=1}^{k}\alpha_i N(x_i|\mu_l,\sigma_l) \\    & s.t. \sum_{i=1}^k \alpha_i = 1\end{aligned}\tag{9}</script><p>该公式的参数变为 $\theta=\{\mu_1,\mu_2,…\mu_k, \sigma_1,\sigma_2,…,\sigma_k, \alpha_1, \alpha_2,…,\alpha_k-1\}$<br>此时，我们将上式代入公式3得到该样本集的似然函数为:</p><script type="math/tex; mode=display">L(\theta|\bar x) = \sum_{i=1}^{N}\log P(x_i|\theta)=\sum_{i=1}^{N}\log\sum_{l=1}^{k}\alpha_l N(x_i|\mu_l,\sigma_l)\tag{10}</script><script type="math/tex; mode=display">\begin{aligned}    \bar \theta_{MLE}=&\arg \max_{\theta}L(\theta|\bar x) \\            =&\arg \max_{\theta}\sum_{i=1}^{N}\log P(x_i|\theta)\\            =&\arg \max_{\theta} \sum_{i=1}^{N}\log\sum_{l=1}^{k}\alpha_l N(x_i|\mu_l,\sigma_l)\end{aligned}\tag{11}</script><p>由于 $\log$ 函数里面存在一个求和公式，如果我们再用极大似然估计法来求解上式的解，将变得很复杂，甚至求不出来，因此极大似然估计发在这里已行不通。</p><p>以上就是期望最大算法被提出的一个简单原因。</p><h1 id="2-期望最大算法"><a href="#2-期望最大算法" class="headerlink" title="2 期望最大算法"></a>2 期望最大算法</h1><p>期望最大算法通过迭代求解来求得最后的一个近似最优的 $\theta$ 值，由“迭代”二字可以看出，当前步的 $\theta$肯定与上一步的 $\theta$ 有关，即存在一种关系:</p><script type="math/tex; mode=display">\theta ^{(g+1)}=f(\theta ^{(g)})\tag{12}</script><p>EM算法对函数 $f$ 的定义为:</p><script type="math/tex; mode=display">\theta ^{(g+1)}=\arg \max_{\theta}\int_{z}\log P(x, z|\theta)P(z|x,\theta^{(g)})dz\tag{13}</script><p>其中 $z$ 是为了简化计算，而引入的一个隐变量（Latent Variable), $x$ 为观测变量(Observable Variable).这两个变量对应到高斯混合模型中，分别为: 我们收集到的样本集和隐藏的模型，每个样本 $x_i$ 对应一个 $z_i$,来表示该样本来自哪个分布。因此 $x_i$ 的概率分可表示为:</p><script type="math/tex; mode=display">P(x_i)=\int_{z_l}P_{\bar \theta}(x|z_l)P(z_l)dz_l\tag{14}</script><p>将上式右边分开描述，$P_{\bar \theta}(x|z_i)$ 表示 $x_i$ 在 $l$ 个高斯分布下的分布，$P(z_l)$ 表示分布为第 $l$ 个高斯分布的概率，即 $\alpha_l$,因此上式为:</p><script type="math/tex; mode=display">P(x_i)=\sum_{z_l=1}^{k}\alpha_{z_l}N(x_i|\mu_{z_i}, \sigma_{z_i})\tag{15}</script><p>该式和公式9保持一致，因此隐变量的引入，并没有改变函数的意义。</p><p>一般地，我们将 $x$ 和 $z$ 连在一起成为完全数据(complete-data),观测数据 $x$ 称为不完全数据。</p><h1 id="3-EM算法的收敛性"><a href="#3-EM算法的收敛性" class="headerlink" title="3 EM算法的收敛性"></a>3 EM算法的收敛性</h1><p>前一节讲解了通过引入隐变量并没有改变 $P(x)$ 的表示，同事给出了 EM 算法的公式，但是EM算法的收敛性如何呢？下面我们就讲解EM算法的收敛性，要使EM算法收敛同时满足公式11的极大似然函数。在迭代过程中，要使 $\theta$ 逐渐收敛，同时满足公式11，则必存在:</p><script type="math/tex; mode=display">\log P(x|\theta^{(g+1)}) \geq \log P(x|\theta^{(g)})\tag{16}</script><p>根据对数函数和条件概率一个特征 $\log \frac{A}{B}=\log A - \log B$ 和 $P(x)=\frac{P(x, z)}{P(z|x)}$可得：</p><script type="math/tex; mode=display">\log P(x|\theta)= \log (x,z|\theta) - \log(z|x,\theta)\tag{17}</script><p>我们对公式两边分别对分布 $P(z|x, \theta^{(g)})$ 求期望，左边为:</p><script type="math/tex; mode=display">\begin{aligned}   \int _z \log P(x|\theta) P(z|x, \theta^{(g)}) dz=& \log P(x|\theta) \int _z P(z|x, \theta^{(g)}) dz \\   =& \log P(x|\theta)\end{aligned}\tag{18}</script><p>右边为:</p><script type="math/tex; mode=display">    \int _zlogP(x,z|\theta)P(z|x, \theta^{(g)}) dz-\int_z\log P(z|x,\theta)P(z|x, \theta^{(g)}) dz =Q(\theta, \theta^{(g)}) -H(\theta, \theta^{(g)})\tag{19}</script><p>我们通常称 $Q(\theta, \theta^{(g)})$ 为 <strong>Q函数</strong>：完全数据的对数似然函数$\log P(x, z|\theta)$关于给定观测数据和当前参数 $\theta^{(g)}$下对隐藏数据 $z$ 的条件概率的期望称为Q函数。</p><p>根据公式17、18和19我们可以得出:</p><script type="math/tex; mode=display">\begin{aligned}    \log P(x|\theta)=&\int _z \log P(x,z|\theta)P(z|x, \theta^{(g)}) dz-\int_z\log P(z|x,\theta)P(z|x, \theta^{(g)}) dz\\    =& Q(\theta, \theta^{(g)}) -H(\theta, \theta^{(g)})\end{aligned}\tag{20}</script><p>极大似然估计就是对 $\log P(x|\theta)$ 进行极大化，对上面的右边公式进行极大化，可不可以呢？不可以，如果这样做就又回到最大似然函数了，因此得不了解，EM算法只是对 $Q(\theta, \theta^{(g)})$ 进行极大化，因此很容易得出:</p><script type="math/tex; mode=display">Q(\theta^{(g+1)}, \theta^{(g)}) \geq Q(\theta^{(g)}, \theta^{(g)})\tag{21}</script><p>我们要想通过优化$Q(\theta, \theta^{(g)})$ 使 $\log P(x|\theta)$ 逐渐增大，则存在:</p><script type="math/tex; mode=display">Q(\theta^{(g+1)}, \theta^{(g)}) - Q(\theta^{(g)}, \theta^{(g)})\geq H(\theta^{(g+1)}, \theta^{(g)}) - H(\theta^{(g)}, \theta^{(g)})\tag{22}</script><p>如果，我们假设，$\forall\theta,H(\theta^{(g)}, \theta^{(g)}) - H(\theta, \theta^{(g)}) \geq 0 \Rightarrow H(\theta^{(g)}, \theta^{(g)}) \geq H(\theta^{(g+1)}, \theta^{(g)})$</p><p>下面，就对这个假设进行证明:</p><script type="math/tex; mode=display">\begin{aligned}    & H(\theta^{(g)}, \theta^{(g)}) - H(\theta, \theta^{(g)}) \\    & =\int_z\log P(z|x,\theta^{(g)})P(z|x, \theta^{(g)})dz - \int_z\log p(z|x,\theta)P(z|x, \theta^{(g)})dz \\    & = \int_z\log [\frac{P(z|x,\theta^{(g)})}{p(z|x,\theta)}]P(z|x, \theta^{(g)})dz \\    & = \int_z -\log [\frac{p(z|x,\theta)}{P(z|x,\theta^{(g)})]}P(z|x, \theta^{(g)})dz \\    & \geq -\log [\int_z\frac{p(z|x,\theta)}{P(z|x,\theta^{(g)})]}P(z|x, \theta^{(g)})dz] \\    & = -log \int_z p(z|x,\theta) dz \\    & =-\log1=0 \\    & \Rightarrow H(\theta^{(g)}, \theta^{(g)}) - H(\theta, \theta^{(g)}) \geq 0\end{aligned}\tag{23}</script><p>$\geq$ 那一步利用了 Jensen 不等式（Jensen Inequality),这其实是凸函数的一个特点，如图5所示。我们求两个点的函数值的期望，即为直线上的黑点，如果我们先对自变量 x 求期望，则 $-\log(Ex)$ 为log函数曲线上的黑点，根据曲线图，我们可以得出</p><script type="math/tex; mode=display">\begin{aligned}    pf(x_1)+(1-p)f(x_2) \geq & f(px_1+(1-p)x_2) \\    E[f(x)] \geq & f[E(x)]\end{aligned}\tag{24}</script><p>即，函数的均值大于等于均值的函数。</p><p><img src="/images/ML_EM_pic5.png" alt="Jensen 不等式"></p><p>通过以上证明可以得出，EM算法通过迭代最大化 $\int _z \log P(x,z|\theta)P(z|x, \theta^{(g)}) dz$，从而使 $P(x|\theta)$ 逐渐增大。</p><script type="math/tex; mode=display">\begin{aligned}    &\log P(x|\theta^{(t+1)}) - \log P(x|\theta^{(t+1)}) \\     &= Q(\theta^{(t+1)}, \theta^{(t)})-H(\theta^{(t+1)},\theta^{(t)}) - [Q(\theta^{(t)}, \theta^{(t)})-H(\theta^{(t)},\theta^{(t)})] \\    &= [Q(\theta^{(t+1)}, Q(\theta^{(t)})-Q(\theta^{(t)}, \theta^{(t)})]-[H(\theta^{(t+1)},\theta^{(t)})-H(\theta^{(t)},\theta^{(t)})] \\    & \geq 0\end{aligned}\tag{25}</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最大期望算法（Expectation-Maximization algorithm, EM）是一类通过迭代进行极大似然估计（Maximum Likelihood Estimation, MLE）的优化算法。用于含有隐变量（Latent Variable) 的概率模型参数的极
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>支持向量机——线性可分支持向量机</title>
    <link href="http://yoursite.com/2019/03/26/ML-hard-margin-svm/"/>
    <id>http://yoursite.com/2019/03/26/ML-hard-margin-svm/</id>
    <published>2019-03-26T14:21:04.000Z</published>
    <updated>2019-03-27T07:05:56.548Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2019/03/26/ML-hard-margin-svm/" target="_blank" rel="noopener">GitHub</a><br><a href="https://www.jianshu.com/p/3938e0f22692" target="_blank" rel="noopener">简书</a><br><a href="https://blog.csdn.net/u014630987/article/details/88845317" target="_blank" rel="noopener">CSDN</a></p><h1 id="1-线性可分支持向量机"><a href="#1-线性可分支持向量机" class="headerlink" title="1 线性可分支持向量机"></a>1 线性可分支持向量机</h1><p>支持向量机(Support vector machines, SVM)是一种<strong>二分类模型</strong>，它的基本模型是定义在特征空间上的间隔最大的线性分类器，他的学习策略就是间隔最大化，同时该方法可以形式化为一个求解图二次规划。</p><p>支持向量机可分为三类:</p><ol><li>线性可分支持向量机、硬间隔（hard-margin svm)</li><li>线性支持向量机、软间隔(soft-margin svm)</li><li>非线性支持向量机、Kernel SVM</li></ol><p>支持向量机模型中存在三宝:</p><ol><li>间隔</li><li>对偶</li><li>核技巧</li></ol><p>支持向量机和感知机在某些方面很相似，其相同点:</p><ol><li>都是二分类模型</li><li>都是通过一个分离超平面对特征进行分类</li></ol><p>不同点：</p><ol><li>SVM 是特殊的感知机</li><li>感知机是用误分类最小的策略，求得分离超平面，这时存在无穷个解，感知机利用间隔最大化求得最优分离超平面。如下图所示</li></ol><p><img src="/images/mlp_svm.png" alt="SVM与感知机区别"><br>图1 感知机与支持向量机区别</p><p>图中的蓝色和黄色圆点分别表示正负样本，对于这个二分类，从图中我们可知，在最上面的黄线和最下面的绿线之间的线条都是可以把训练样本集完全分开的，这就是感知机的原理，通过这些分离超平面把训练集分开，这样的分离超平面存在很多条，比如图中的虚线，从视觉上中间那条实线应该是众多线条中最优的一条，感知机对于学习的分离超平面由于优化算法、学习率等不同因素，会随机地学习到这众多分离超平面中的一条，当学习到的是靠近上下边缘的分离超平面是，对于一个未知样本，当这个样本稍微浮动一下，模型就很可能对他进行误分类了，因此鲁棒性很低，而支持向量机的目标是找到图中中间那条最优的分离超平面。</p><p><strong>定义(线性可分支持向量机)</strong>:给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到一个分离超平面:</p><script type="math/tex; mode=display"> w^* \cdot x + b^* =0\tag{1}</script><p>即相应的决策模型:</p><script type="math/tex; mode=display">    f(x)=sign(w^* \cdot x + b^*)    \tag{2}</script><p>此模型就为线性可分支持向量机。其中 $ w^<em>$ 表示分离超平面的法向量， $b^</em>$ 表示截距，位于分离超平面之上的样本为正样本，之下的为负样本。</p><h1 id="2-函数间隔和几何间隔"><a href="#2-函数间隔和几何间隔" class="headerlink" title="2 函数间隔和几何间隔"></a>2 函数间隔和几何间隔</h1><p>一般来说，一个点到分离超平面的远近可以表示分类预测的确信程度，在给定分离超平面$w \cdot x + b = 0$的情况下， $|w \cdot x + b|$能够相对地表示点 $x$ 到分离超平面的远近。同时 $w \cdot x + b$的符号与类别标记 $y$ 是否保持一致来表示分类是否正确，所以，可以用$y(w \cdot x + b)$ 来表示分类的正确性及确信度，这就是函数间隔（functional margin)的概念。</p><p><strong>定义(函数间隔)</strong>:对于给定训练数据集 $T$ 和超平面 $(w, b)$,定义超平面 $(w, b)$ 关于样本点 $(x_i,y_i)$ 的函数间隔为:</p><script type="math/tex; mode=display">\hat \gamma_i = y_i(w \cdot x_i + b)\tag{3}</script><p>分离超平面关于训练数据集 $T$ 的函数间隔为超平面关于 $T$ 中所有样本点 $(x_i,y_i)$ 的函数间隔最小值:</p><script type="math/tex; mode=display">\hat \gamma = \min_{i=1...N}y_i(w \cdot x_i + b)\tag{4}</script><p>上述定义是在给定超平面 $(w,b)$ 的时候计算，然而在实际支持向量机的学习过程中，只有函数间隔是不够的，因为当 $w$ 和 $b$ 按比例同时扩大 $n$ 倍，此时函数间隔也扩大 $n$ 倍，而超平面并没有改变。因此我们需要对分离超平面加以约束，如规范化，$||w||=1$,使得间隔不随 $w$ 和 $b$ 成比例扩大而改变。这时函数间隔就成为了几何间隔（geometric margin)</p><p><strong>定义(几何间隔)</strong>:对于给定训练数据集 $T$ 和超平面 $(w, b)$,定义超平面 $(w, b)$ 关于样本点 $(x_i,y_i)$ 的几何间隔为:</p><script type="math/tex; mode=display"> \gamma_i = y_i(\frac{w}{||w||} \cdot x_i + \frac{b}{||w||})\tag{5}</script><p>分离超平面关于训练数据集 $T$ 的函数间隔为超平面关于 $T$ 中所有样本点 $(x_i,y_i)$ 的函数间隔最小值:</p><script type="math/tex; mode=display">\gamma = \min_{i=1...N} \gamma_i\tag{6}</script><p>$||w||$ 为 $w$ 的 $L_2$ 范数。其实上述公式就是我们中学时候学习的点到直线的距离公式的推广，或者说点到直线的距离公式是该公式在二位平面下的表示。</p><p>通过公式4和公式6的比较，我们可以得出函数间隔和几何间隔有如下关系:</p><script type="math/tex; mode=display">\begin{aligned}    \gamma_i =& \frac{\hat \gamma_i}{||w||} \\    \gamma =& \frac{\hat \gamma}{||w||}\end{aligned}\tag{7}</script><h1 id="3-间隔最大化"><a href="#3-间隔最大化" class="headerlink" title="3 间隔最大化"></a>3 间隔最大化</h1><p>支持向量机学习的基本思想是求解能够<strong>正确划分训练数据集</strong>且<strong>几何间隔最大</strong>的分离超平面。间隔最大化的直观解释是：使分类决策模型以较大的确信度来对数据集分类，同时对离超平面较近的点也有很大的确信度。</p><p>因此，最大间隔支持向量机形式化为:</p><script type="math/tex; mode=display">\begin{aligned}    &\max_{w,b}\quad\gamma \\    &s.t. \quad y_i(\frac{w}{||w||} \cdot x_i + \frac{b}{||w||}) \geq\gamma, i=1,2,...,N\end{aligned}\tag{8}</script><p>也即:</p><script type="math/tex; mode=display">\begin{aligned}    &\max_{w,b}\quad \frac{\hat \gamma}{||w||} \\    &s.t. \quad y_i(\frac{w}{||w||} \cdot x_i + \frac{b}{||w||}) \geq \hat \gamma, i=1,2,...,N\end{aligned}\tag{9}</script><p>我们得知函数间隔$\hat \gamma$的取值并不影响模型的最优化问题，将 $w$ 和 $b$ 成比例的改变 $\lambda$ 倍，函数间隔也变成 $\lambda \hat \gamma$,这一改变对上面最优化的不等式约束并没有印象，因此，我们可以令 $\hat \gamma=1$,于是上述公式就等价于:</p><script type="math/tex; mode=display">\begin{aligned}    & \min_{w,b} \quad \frac{1}{2}||w||^2 \Rightarrow \frac{1}{2}w^T    w\\    & s.t. \quad y_i(w \cdot x_i + b)-1 \geq 0, i=1,2,...,N\end{aligned}\tag{10}</script><p>此时，SVM优化问题变为一个凸二次规划问题，利用拉格朗日乘子法即可求出最优的 $(w^<em>,b^</em>)$</p><h1 id="4-学习的对偶算法"><a href="#4-学习的对偶算法" class="headerlink" title="4 学习的对偶算法"></a>4 学习的对偶算法</h1><p>为求解支持向量机的最优化问题，我们将公式10作为原始问题，应用拉格朗日对偶性，通过求解对偶问题(dual problem)得到原始问题(primal problem)的最优解，这就是支持向量机的对偶算法。这样做的<strong>优点</strong>:</p><ol><li>对偶问题往往更容易求解；</li><li>自然引入核函数，进而推广到非线性可分分类问题；</li></ol><p>通过对公式10的约束条件引入拉格朗日乘子$\alpha_i\geq 0,i=1,2,…,N$,构建出拉格朗日函数：</p><script type="math/tex; mode=display">\begin{aligned}    L(w, b, \alpha)=&\frac{1}{2}w^Tw+\sum_{i=1}^{N}\alpha_i(1-y_i(w^T x_i + b)) \\    =& \frac{1}{2}w^Tw-\sum_{i=1}^{N}\alpha_i y_i(w^T x_i + b) +\sum_{i=1}^{N}\alpha_i\end{aligned}\tag{11}</script><p>我们称公式10为带约束的原始问题，根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题</p><script type="math/tex; mode=display">\max_{\alpha}\min_{w,b}L(w, b, \alpha)\tag{12}</script><p>公式12和原始公式存在一种弱对偶关系，当等号成立时为强对偶关系:</p><script type="math/tex; mode=display">\min_{w,b}\max_{\alpha} L(w, b, \alpha) \geq \max_{\alpha}\min_{w,b} L(w, b, \alpha)\tag{13}</script><p>此时我们就可以按照利用拉格朗日对偶性求解问题的标准模型，求解出$w, b, \alpha$.</p><ol><li><p><strong>求 $\min_{w,b}L(w, b, \alpha)$</strong></p><p>将拉格朗日函数 $L(w, b, \alpha)$ 分别对 $w,b$求偏导，并令其值为0.</p></li></ol><script type="math/tex; mode=display">\begin{aligned}    & \frac{\partial L}{\partial w} = w - \sum_{i=1}^{N}\alpha_i y_i x_i =0 \\    & \Rightarrow w = \sum_{i=1}^{N}\alpha_i y_i x_i\end{aligned}\tag{14}</script><script type="math/tex; mode=display">\begin{aligned}    & \frac{\partial L}{b}=-\sum_{i=1}^{N}\alpha_iy_i=0 \\    & \Rightarrow \sum_{i=1}^{N}\alpha_iy_i=0\end{aligned}\tag{15}</script><p>将公式14和15带入公式11得:</p><script type="math/tex; mode=display">\begin{aligned}    \min_{w,b} L(w, b,\alpha)=&\frac{1}{2} (\sum_{i=1}^{N}\alpha_i y_i x_i)^T \sum_{i=1}^{N}\alpha_i y_i x_i-\sum_{i=1}^{N}\alpha_i y_i((\sum_{i=1}^{N}\alpha_i y_i x_i)^T x_i + b) +\sum_{i=1}^{N}\alpha_i \\    =&\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i y_i\alpha_j y_jx_i^Tx_j-\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i y_i\alpha_j y_jx_j^Tx_i+\sum_{i=1}^{N}\alpha_i \\    =& -\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i y_i\alpha_j y_jx_i^Tx_j+\sum_{i=1}^{N}\alpha_i\end{aligned}\tag{16}</script><ol><li><strong>求 $\min_{w,b}L(w, b,\alpha)$</strong>对 $\alpha$ 的极大值，即对偶问题:</li></ol><script type="math/tex; mode=display">\begin{aligned}    & \max_{\alpha} -\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i y_i\alpha_j y_jx_i^Tx_j+\sum_{i=1}^{N}\alpha_i \\    & s.t. \quad \sum_{i=1}^{N}\alpha_iy_i=0 \\    & \quad \quad \alpha_i \geq 0,i=1, 2...N\end{aligned}\tag{17}</script><p>将上式转化为求极小值</p><script type="math/tex; mode=display">\begin{aligned}    & \min_{\alpha} \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i y_i\alpha_j y_jx_i^Tx_j-\sum_{i=1}^{N}\alpha_i \\    & s.t. \quad \sum_{i=1}^{N}\alpha_iy_i=0 \\    & \quad \quad \alpha_i \geq 0,i=1, 2...N\end{aligned}\tag{18}</script><p>上式可以继续利用凸二次规划来求解 $\alpha^<em>=(\alpha_1^</em>,\alpha_2^<em>,…,\alpha_N^</em>)$,然后可由$\alpha^<em>$求得原始问题对 $(w,b)$ 的解 $w^</em>,b^*$。</p><p><strong>定理</strong> 设 $\alpha^<em>=(\alpha_1^</em>,\alpha_2^<em>,…,\alpha_N^</em>)$ 是对偶问题（即公式18）的解，则存在下标 $j$,使得 $\alpha_j &gt; 0$,并按如下公式求得$w^<em>,b^</em>$</p><script type="math/tex; mode=display">    w^*=\sum_{i=1}^{N} \alpha_i^*y_ix_i    \tag{19}</script><script type="math/tex; mode=display">    b^*=y_j-\sum_{i=1}^{N} \alpha_i^*y_i(x_i^Tx_j)    \tag{19}</script><p><strong>证明</strong><br>根据拉格朗日某定理，KKT条件成立，即:</p><script type="math/tex; mode=display">\begin{aligned}    & \frac{\partial L(w^*,b^*,\alpha ^*)}{\partial w} = w^*-\sum_{i=1}^{N}\alpha_i^* y_i x_i=0 \\    & \frac{\partial L(w^*,b^*,\alpha ^*)}{\partial b} = -\sum_{i=1}^{N}\alpha_i^*y_i=0 \\    & \frac{\partial L(w^*,b^*,\alpha ^*)}{\partial \alpha} = 0 \\    & \alpha_i(1-y_i(w^Tx_i+b)) =0  \\    & \alpha_i \geq 0 , i=1,2,...,N \\    & 1-y_i(w^Tx_i+b) \leq 0 , i=1,2,...,N\end{aligned}\tag{20}</script><p>此时，公式13具有强对偶关系，即等号成立。根据支持向量机的特点，至少存在一个 $\alpha_j &gt;0$,即对于支持向量（后面讲解），对此j有</p><script type="math/tex; mode=display">1-y_j(w^Tx_j+b) = 0 \Rightarrow y_j(w^{*^T}x_j+b) = 1\tag{21}</script><p>由于 $y_j$ 为1或-1，上式两边同乘以一个 $y_i$ 得：</p><script type="math/tex; mode=display">\begin{aligned}    b^* =& y_j - w^{*^T}x_j \\        =& y_j - \sum_{i=1}^{N} \alpha_i^*y_i(x_i^Tx_j)\end{aligned}\tag{22}</script><p>从上面的推导我们可以看出，$w^<em>$ 和 $b^</em>$只依赖于训练数据中对应于 $\alpha_i^<em> &gt; 0$的样本点 $(x_i, y_i)$,而其他样本点对 $w^</em>$ 和 $b^<em>$ 没有印象，我们把这些$\alpha_i^</em> &gt; 0$ 的样本点称为支持向量。这些样本点一定位于间隔边界上。</p><p><a href="https://github.com/RanMaosong/Machine-Learning-LiHang/blob/master/code/%E7%AC%AC%E4%B8%83%E7%AB%A0%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/Plot.ipynb" target="_blank" rel="noopener">文中绘图源码</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2019/03/26/ML-hard-margin-svm/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://www.j
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>最大熵模型</title>
    <link href="http://yoursite.com/2019/03/23/ML-MaxEntropy/"/>
    <id>http://yoursite.com/2019/03/23/ML-MaxEntropy/</id>
    <published>2019-03-23T13:44:38.000Z</published>
    <updated>2019-03-26T14:07:16.377Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2019/03/23/ML-MaxEntropy/" target="_blank" rel="noopener">GitHub</a><br><a href="https://www.jianshu.com/p/b5c57b80ee5f" target="_blank" rel="noopener">简书</a><br><a href="https://blog.csdn.net/u014630987/article/details/88831663" target="_blank" rel="noopener">CSDN</a></p><h1 id="1-最大熵原理"><a href="#1-最大熵原理" class="headerlink" title="1 最大熵原理"></a>1 最大熵原理</h1><p>最大熵模型（Maximum Entropy Model)是通过最大熵原理推导实现,那什么是最大熵原理？</p><p>熵是随机变量不确定性大的度量，不确定性越大，熵值越大；若随机变量变为定值，即某个值发生的概率为1，而其它事件都为0， 此时熵值为0，均匀分布是熵值最大的分布，也即“最不确定的分布”。</p><p>假设离散随机变量 $X$ 的概率分布是 $P(X)$,则其熵为:</p><script type="math/tex; mode=display"> H(P)=-\sum_{x}p(x)logp(x) \tag{1}</script><p>熵满足如下条件:</p><script type="math/tex; mode=display">0 \leq H(P) \leq log|X|</script><p>其中,$|X|$ 表示 $X$ 的取值，当 $X$ 的分布是均匀分布时，满足左边等号，当 $X$ 是确定事件时，满足左边的等号。</p><p>从上面可知，最大熵原理认为该求解的概率模型满足如下条件:</p><ol><li>满足事先已约束的条件；</li><li>然后在满足这些条件的模型中选择熵最大的模型，即让不确定的信息等可能的发生；</li></ol><h1 id="2-最大熵模型"><a href="#2-最大熵模型" class="headerlink" title="2. 最大熵模型"></a>2. 最大熵模型</h1><p>将最大熵原理应用到分类问题中即得到最大熵模型。假设分类模型的一个条件概率分布 $P(Y|X)$, $X\in\chi\subseteq R^n$ 表示输入，$X\in\gamma$表示输出。这个模型表示给定的输入 $X$,以条件概率$P(Y|X)$输出$Y$。</p><p>给定一个训练数据集</p><script type="math/tex; mode=display">T=\{(x_1, y_1), (x_2,y_2)...(x_N, y_N)\}</script><p>学习的目标是用最大熵原理选择最好的模型。</p><p>对于给定训练数据集，我们可以确定联合分布$P(X, Y)$的经验分布$\tilde P(X,Y)$和边缘分布 $P(X)$ 的经验分布 $\tilde P(X)$,即：</p><script type="math/tex; mode=display">\tilde P(X=x, Y=x) = \frac{v(X=x, Y=y)}{N} \\\tilde P(X=x) = \frac{X(X=x)}{N}\tag{2}</script><p>其中，$v(X=x, Y=y)$ 表示训练数据中样本$(x, y)$出现的频数， $V(X=x)$表示训练数据集中 $x$ 出现的频数。 $N$ 表示训练样本的总容量。</p><p><strong>特征函数$f(x, y)$</strong> 表示输入 $x$ 和输出$y$ 之间的某个约束。其定义为:</p><script type="math/tex; mode=display">f(x,y)=\begin{cases}    1,\quad x和y满足约束\\    0, \quad x和y不满足约束\end{cases}\tag{3}</script><p>特征函数 $f(x,y)$ 关于经验分布 $\tilde P(X, Y)$的期望值为:</p><script type="math/tex; mode=display">E_{\tilde p}(f) = \sum_{x, y} \tilde P(x,y)f(x, y)\tag{4}</script><p>特征函数 $f(x,y)$ 关于模型 $P(Y|X)$ 和经验分布 $\tilde P(X)$的期望值为:</p><script type="math/tex; mode=display">E_{p}(f) = \sum_{x, y} \tilde P(x)P(y|x)f(x, y)\tag{5}</script><p>因为机器学习的目的就是从数据集中学得数据中包含的某种内在信息，因此我们可以假设公式4和公式5相等，即</p><script type="math/tex; mode=display">E_{\tilde p}(f) = E_{p}(f) \\\sum_{x, y} \tilde P(x,y)f(x, y) = \sum_{x, y} \tilde P(x)P(y|x)f(x, y)\tag{6}</script><p>公式6就作为模型的约束条件，如果有 n 个特征函数，则就有 n 个约束条件。</p><p><strong>最大熵模型</strong> 假设满足所有约束条件的模型集合为</p><script type="math/tex; mode=display">C=\{P|E_{\tilde p}(f_i) = E_{p}(f_i), i=1,2...n\}</script><p>定义在条件概率分布 $P(Y|X)$ 上的条件熵为:</p><script type="math/tex; mode=display">H(P) = - \sum_{x,y}\tilde P(x)P(y|x)logP(y|x)\tag{7}</script><p>则模型集合 $C$ 条件熵最大的模型成为<strong>最大熵模型</strong></p><p><strong>补充</strong></p><p>条件概率的熵的公式为</p><script type="math/tex; mode=display">H(y|x)=-\sum_{x,y}p(x,y)logp(y|x)\tag{8}</script><p>因此最大熵模型如公式7所示。</p><p>总之，最大熵模型就是在满足约束的模型集合中选择条件概率分布 $P(Y|X)$ 最大的模型。</p><h1 id="3-最大熵模型的学习"><a href="#3-最大熵模型的学习" class="headerlink" title="3. 最大熵模型的学习"></a>3. 最大熵模型的学习</h1><p>通过上述上述的描述，最大熵模型可以形式化为约束最优化问题，即</p><script type="math/tex; mode=display">\max_{P \in C} H(P) = -\sum_{x,y}\tilde P(x)P(y|x)\log P(y|x) \\s.t. \quad E_{\tilde p}(f_i) = E_{p}(f_i), \quad i=1, 2...,n \\\quad \sum_yP(y|x) = 1\tag{9}</script><p>按照优化习惯，通常将最大值优化转换为最小值优化。即</p><script type="math/tex; mode=display">\max_{P \in C} -H(P) = \sum_{x,y}\tilde P(x)P(y|x)\log P(y|x) \\s.t. \quad E_{\tilde p}(f_i) = E_{p}(f_i), \quad i=1, 2...,n \\\quad \sum_yP(y|x) = 1\tag{10}</script><p>公式10所得出的解就是最大熵模型学习的模型。</p><p>解决上述约束最优化问题，我们通过拉格朗日对偶性来进行解决。</p><p>首先我们引入拉格朗日乘子$w_0, w_1,w_2…w_n$,定义拉格朗日函数 $L(p, w)$为</p><script type="math/tex; mode=display">    \begin{aligned}        L(P, w) &=-H(P) + w_0(1-\sum_yP(y|x))+\sum_{i=1}^{n}w_i(E_{\tilde p}(f_i) - E_{p}(f_i)) \\        &=\sum_{x,y}\tilde P(x)P(y|x)\log P(y|x) + w_0(1-\sum_yP(y|x)) \\        &+\sum_{i=1}^{n}w_i(\sum_{x,y}\tilde P(x,y)f(x, y) - \sum_{x, y} \tilde P(x)P(y|x)f(x, y))    \end{aligned}    \tag{11}</script><p>因此，最优化问题的原始问题为</p><script type="math/tex; mode=display">\min_{P\in C} \max_{w} L(P,w)\tag{12}</script><p>对偶问题</p><script type="math/tex; mode=display">\max_{w} \min_{P\in C}  L(P,w)\tag{13}</script><p>我们称公式10、11和12为原始问题，公式13为原始问题的对偶问题，且原始问题的解与对偶问题的解是等价的，因此，公式11的解就是我们求解的模型。</p><p>我们首先求解对偶问题公式13内部的极小化问题$\min_{P\in C}  L(P,w)$,该函数是关于 $w$ 的函数，我们将其记作:</p><script type="math/tex; mode=display">\psi (w) = \min_{P\in C}  L(P,w) = L(P_w, w)\tag{14}</script><p>$\psi (w)$ 称为对偶函数，同时其解记为:</p><script type="math/tex; mode=display">P_w = \arg \min_{p}L(P,w) = P_w(y|x)\tag{15}</script><p>我们可以利用偏导数来求解公式15，即</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial L(P,w)}{\partial P(y|x)} &= \sum_{x,y}(\tilde P(x) \log P(y|x) + \tilde{P}(x)) -\sum_{y}w_o+\sum_{i=1}^{n}w_i(-\sum_{x,y}\tilde{P}(x)f_{i}(x,y)) \\&= \sum_{x,y}\tilde{P}(x)(\log P(y|x) + 1) - \sum_{y}w_0-\sum_{x,y}{\tilde{P}(x)\sum_{i=1}^nw_if_i(x,y)} \\&=\sum_{x,y}\tilde{P}(x)(\log P(y|x) + 1) - \sum_x \tilde{P}(x)\sum_{y}w_0-\sum_{x,y}{\tilde{P}(x)\sum_{i=1}^nw_if_i(x,y)} \\&= \sum_{x,y}\tilde{P}(x)(\log P(y|x) + 1-w_0-\sum_{i=1}^nw_if_i(x,y))\end{aligned}\tag{16}</script><p>由于 $L(P,w)$是凸函数，我们我可令上式偏导数为0，在$\tilde P(x)&gt;0$的情况下，即可求出$P(y|x)$, 即：</p><script type="math/tex; mode=display">\begin{aligned}    P(y|x) &=\exp(\sum_{i=1}^{n}w_if_i(x,y)+w_0-1)\\        &=\frac{\exp{\sum_{i=1}^{n}w_if_i(x,y)}}{\exp(1-w_0)}\end{aligned}\tag{17}</script><p>由于在概率论中，$\sum_{y}P(y|x)=1$,因此需对公式17进行归一化，又$\exp(1-w_0)$为常数,因此：</p><script type="math/tex; mode=display">P_w(y|x)=\frac{1}{Z_w(x)}\exp(\sum_{i=1}^{n}w_if_i(x,y))\tag{18}</script><p>其中</p><script type="math/tex; mode=display">Z_w(x)=\sum_{y}\exp(\sum_{i=1}^nw_if_i(x,y))\tag{19}</script><p>公式18、19表示的模型就是最大熵模型$P_w=p_w(y|x)$.然后求解对偶函数的极大化问题</p><script type="math/tex; mode=display">\max_{w} \psi (w) \tag{20}</script><p>求得$w^*$,得到最终模型。</p><h1 id="4-极大似然估计"><a href="#4-极大似然估计" class="headerlink" title="4 极大似然估计"></a>4 极大似然估计</h1><p>通过上面一小节的计算，我们已经求出最大熵模型，但是此时该模型还是一个关于 $w$ 的函数，我们如何求出 $w$ 来求得最终的模型呢。</p><p>我们先来描述对数似然函数，通过前面一章的逻辑回归模型中的<a href="https://ranmaosong.github.io/2019/03/18/ML-ligistic-regression/" target="_blank" rel="noopener">最大似然估计</a>，我们可知对数似然函数和熵在值上互为相反数，又通过公式8得知条件概率的熵形式，因此我们可以得知，条件概率分布$P(Y|X)$的对数似然函数</p><script type="math/tex; mode=display">    L_{\tilde P}(P_w) = \sum_{x,y}\tilde{P}(x,y)\log P(y|x)    \tag{21}</script><p>我们再冲似然函数的定义方面证明上述公司的正确性。</p><p>在给定数据集$\{(x_1,y_1),(x_2,y_2)…(x_n,y_n)\}$,我们可求得当前模型的似然函数为:</p><script type="math/tex; mode=display">L(\theta)=\prod_{i=1}^n P(x_i, \theta)\tag{22}</script><p>我们假设$X_i$在训练集中出现了$C(x_i)$,因此公式22可以转化为:</p><script type="math/tex; mode=display">L(\theta)=\prod_{i=1}^k P(x_i, \theta)^{C(x_i)}\tag{23}</script><p>$k$ 表示训练数据集中总共有 k 种不同的输入特征，我们对上市求其 $\frac{1}{n}$次方，得：</p><script type="math/tex; mode=display">L(\theta)^{\frac{1}{n}}=\prod_{i=1}^k P(x_i, \theta)^{\frac{C(x_i)}{n}}\tag{24}</script><p>对公式24求对数的:</p><script type="math/tex; mode=display">\begin{aligned}  \log L(\theta)^{\frac{1}{n}} &=\log \prod_{i=1}^k P(x_i, \theta)^{\frac{C(x_i)}{n}}  \\  &= \sum_{i=1}^k \frac{C(x_i)}{n} \log P(x_i, \theta)\\\end{aligned}\tag{25}</script><p>因此对于$\log L(\theta)^{\frac{1}{n}}$ 和 $\log L(\theta)$是等价的，</p><p>因此对于条件概率分布$P(Y|X)$的对数似然函数</p><script type="math/tex; mode=display">    L_{\tilde P}(P_w) = \sum_{x,y}\tilde{P}(x,y)\log P(y|x)    \tag{26}</script><p>对于公式21-26的推导，不具有严格的树学理论，只是为了理解下面公式27做铺垫。</p><p>一直训练数据的经验概率分布$\tilde P(X,Y),$条件概率分布$P(Y|X)$的对数似然函数表示为</p><script type="math/tex; mode=display">\begin{aligned}    L_{\tilde P}(P_w) &=\log \prod_{xy}P(y|x)^{\tilde P(x,y)} = \sum_{x,y}\tilde{P}(x,y)\log P(y|x)\end{aligned}\tag{27}</script><p>将公式18和19带入公式27可得:</p><script type="math/tex; mode=display">\begin{aligned}    L_{\tilde P}(P_w) &=\sum_{x,y}\tilde{P}(x,y)(\sum_{i=1}^{n}w_if_i(x,y)-\log Z_w(x) \\    &=\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^{n}w_if_i(x,y)-\sum_{x,y}\tilde{P}(x,y)\log Z_w(x) \\    &=\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^{n}w_if_i(x,y)-\sum_{x}\tilde {P}(x)\log Z_w(x)\end{aligned}\tag{28}</script><p>公式28就是极大似然函数。上式最后两步最后一项的转化是因为 $Z_w(x)$ 是关于 x 的函数，所以可以对 $\tilde{P}(x,y)$ 对 $x$ 进行累加得到 $\tilde {P}(x)$.</p><p>我们再看看对偶函数 $\psi (w)$,我们将 $P_w(y|x)$ 带入公式11得:</p><script type="math/tex; mode=display">\begin{aligned}    \psi (w) =& \sum_{x,y}\tilde P(x)P_w(y|x)\log P_w(y|x) + w_0(1-\sum_yP(y|x)) \\        &+\sum_{i=1}^{n}w_i(\sum_{x,y}\tilde P(x,y)f(x, y) - \sum_{x, y} \tilde P(x)P_w(y|x)f(x, y)) \\        =&\sum_{x,y}\tilde P(x)P_w(y|x)\log P_w(y|x)  \\        &+\sum_{i=1}^{n}w_i(\sum_{x,y}\tilde P(x,y)f(x, y) - \sum_{x, y} \tilde P(x)P_w(y|x)f(x, y)) \\        =& \sum_{x,y}\tilde P(x,y)\sum_{i=1}^{n}w_if(x, y)+\sum_{x,y}\tilde{P}(x)P_w(y|x)(\log P_w(y|x)-\sum_{i=1}^{n}w_if_i{(x,y)}) \\        =& \sum_{x,y}\tilde P(x,y)\sum_{i=1}^{n}w_if(x, y)-\sum_{x,y}\tilde{P}(x)P_w(y|x)\log Z_w(x) \\        =& \sum_{x,y}\tilde P(x,y)\sum_{i=1}^{n}w_if(x, y)-\sum_{x,y}\tilde{P}(x)\log Z_w(x)\end{aligned}\tag{29}</script><p>公式29第三步到第四步用到下面公式进行推导:</p><script type="math/tex; mode=display"> P_w(y|x)=\frac{1}{Z_w(x)}\exp(\sum_{i=1}^{n}w_if_i(x,y)) \Rightarrow \log P_w(y|x)=\sum_{i=1}^{n}w_if_i(x,y)-\log Z_w(x)</script><p>倒数第二步到最后一步的推导用到了 $\sum_{y}P(y|x)=1$.</p><p>比较公式公式28和公式29，可得:</p><script type="math/tex; mode=display">\psi (w) = L_{\tilde P}(P_w)</script><p>因此，可以证明，最大熵模型学习中的对偶函数极大化等价于最大熵模型的极大似然估计。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2019/03/23/ML-MaxEntropy/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://www.jiansh
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>逻辑(斯谛)回归(Logistic Regression)</title>
    <link href="http://yoursite.com/2019/03/18/ML-ligistic-regression/"/>
    <id>http://yoursite.com/2019/03/18/ML-ligistic-regression/</id>
    <published>2019-03-18T06:14:13.000Z</published>
    <updated>2019-03-21T14:21:47.653Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2019/03/18/ML-ligistic-regression/" target="_blank" rel="noopener">GitHub</a><br><a href="https://www.jianshu.com/p/9f723c2ac52e" target="_blank" rel="noopener">简书</a><br><a href="https://blog.csdn.net/u014630987/article/details/88687924" target="_blank" rel="noopener">CSDN</a></p><p>在我们学习机器学习的过程中，我们所需解决的问题，大致可以分为两部分：分类和回归.其中,分类是指模型用来预测一个有限的离散值集合中的一个，比如猫狗分类，肿瘤的恶性或良性; 回归是指模型的输出是一个连续变量，比如预测房价、身高等.本篇内容讲解的是机器学习中经典的逻辑(斯谛)回归（Logistic Regression)，从名字上看，大家误以为该方法是一种回归方法，其实不然，它是分类方法的一种，常用于二元分类，但是为什么会取名回归，我个人理解大致有如下几点原因：</p><pre><code>1. 利用回归的思想来解决分类问题;2. 它的输出也是一个连续值，通过设定阈值来实现分类 </code></pre><h1 id="1-逻辑斯谛分布"><a href="#1-逻辑斯谛分布" class="headerlink" title="1. 逻辑斯谛分布"></a>1. 逻辑斯谛分布</h1><p><strong>定义</strong>：设X是连续随机变量，X服从逻辑斯谛分布是指X具有下列分布函数和密度函数：</p><script type="math/tex; mode=display">    F(x)=P(X \leq x)=\frac{1}{1+e^{-(x-u)/\gamma}} \tag{1}</script><script type="math/tex; mode=display">    f(x)=F^{'}(x)=\frac{e^{-(x-\mu)\gamma}}{\gamma(1+e^{-(x-u)/\gamma})^2} \tag{2}</script><p>其中,$\mu$为位置参数,$\gamma &gt; 0$为形状参数.</p><p>该函数以点$(\mu, \frac{1}{2})$为中对称，既有如下关系：</p><script type="math/tex; mode=display">\begin{aligned}F(-x+\mu) &= 1 - F(x+\mu)\\F(-x+\mu)-\frac{1}{2} &=  F(x + \mu) + \frac{1}{2}\end{aligned}\tag{3}</script><p>形状参数$\gamma$的值越小，曲线在中心附近增长的越快.该函数的图形如下图所示：</p><p><img src="/images/logistic_regression_sigmoid.png" alt="Sigmoid"><br>图一 逻辑斯谛分布的分布函数和密度函数</p><h1 id="2-二元逻辑斯谛回归"><a href="#2-二元逻辑斯谛回归" class="headerlink" title="2 二元逻辑斯谛回归"></a>2 二元逻辑斯谛回归</h1><p>二元逻辑斯谛回归模型是一种分类模型，有条件概率分布$P(Y|X)$表示，X取值为实数，随机变量 Y 取值为 1或0；</p><p><strong>逻辑斯谛回归模型</strong>的条件概率如下：</p><script type="math/tex; mode=display">\begin{aligned}    p(Y=1|x)&=\frac{exp(w\cdot x+b)}{1+exp(w\cdot x+b)}=\frac{1}{1+exp(-(w\cdot x+b))} \\    P(Y=0|x)&=\frac{1}{1+exp((w\cdot x+b))}\end{aligned}    \tag{4}</script><p>这里, $ x \in R^n $表示样本的特征向量，$Y \in {0, 1}$是输出表示样本的类别, $w \in R^n$ 和 $ b \in R$是模型的参数，其中，$w$ 表示权重向量,$b$表示偏置。$w \cdot x$表示$w$和$x$的内积.通常为了方便，我们将样本和权重向量进行扩充，仍记作$w$和$b$：</p><script type="math/tex; mode=display">w = (w^1, w^2... w^n, b)</script><script type="math/tex; mode=display">x = (x^1, x^2...x^n, 1)</script><p>此时逻辑斯蒂回归模型记作:</p><script type="math/tex; mode=display">\begin{aligned}    p(Y=1|x)&=\frac{exp(w\cdot x)}{1+exp(w\cdot x)} \\    P(Y=0|x)&=\frac{1}{1+exp((w\cdot x))}\end{aligned}\tag{5}</script><p><strong>几率</strong>是指一个事件发生与不发生的概率比值,即<br>$<br>\frac{p}{1-p}<br>$<br>则它的对数几率为$lnit(p)=log \frac{p}{1-p}$,对于逻辑斯蒂回归回归而言，其对数几率为</p><script type="math/tex; mode=display">logit(\frac{P(Y=1|x)}{1-P(Y=1|x)})=w \cdot x \tag{6}</script><h1 id="3-模型参数估计"><a href="#3-模型参数估计" class="headerlink" title="3 模型参数估计"></a>3 模型参数估计</h1><p>对于给定的训练数据集$T=\{(x_1, y_1), (x_2, y_2)…(x_n, y_x)\}$,可以应用极大似然估计(使模型预测的标签为真是标签的值最大化)模型参数，从而得到最优的逻辑斯蒂回归模型。</p><p>首先，设$P(Y=1|x)=\pi(x), P(Y=0|x)=1-\pi(x)$,则似然函数为:</p><script type="math/tex; mode=display">\begin{aligned}\prod_{i=1}^n[\pi(x)]^{y_i}[1-\pi(x)]^{1-y_i}=\prod_{i=1}^n{y_i\pi(x_i)+(1-y_i)(i-\pi(x_i))}\end{aligned}\tag{7}</script><p>极大似然函数和交叉熵的树学公式形式时一摸一样的，但是他们背后的数学原理略有不同。<br>通常在处理优化问题时，我们都利用对数函数来把连乘变成求和来简化问题，因此公式七的对数似然函数为：</p><script type="math/tex; mode=display">\begin{aligned}L(w) &= \sum_{i=1}^{n}[y_i ln \pi(x_i)+(1-y_i)ln(1-\pi(x_i))]\\    &=\sum_{i=1}^{n}[y_i ln \pi(x_i) - y_i ln(1-\pi(x_i)) + ln(1-\pi(x_i))] \\    &=\sum_{i=1}^{n}[y_iln\frac{\pi(x_i)}{1-\pi(x_i)}(注：这就是对数几率值）+ln(1-\pi(x_i))] \\    &=\sum_{i=1}^{n}[y_i(w*x_i)+ln(\frac{1}{1+exp((w\cdot x))})] \\    &=\sum_{i=1}^{n}[y_i(w*x_i)-ln(1+exp(w\cdot x_1))]\end{aligned}\tag{8}</script><p>通过梯度下降和拟牛顿法即可求的该函数，我们求$L(w)$对$w$的倒数:</p><script type="math/tex; mode=display">\begin{aligned}    \frac{\partial L(w)}{\partial w}&=\sum_{i=1}^{n}[y_ix_i-(\frac{1}{1+exp(w\cdot x_1)}*exp(w \cdot x_i)) * x_i]\\        &=\sum_{i=1}^{n}[y_ix_i-\pi(x_i)*x_i]\end{aligned}\tag{9}</script><p>通常我们在实际优化的时候，都是求取最小值，因此通常使用$-L(w)$作为损失函数.</p><p><strong>问题</strong>:在机器学习或深度学习中,我们通常以$L_2$作为损失函数，但是为什么这里是用了极大似然估计？</p><p>我们先观察一下使用$L_2$范数作为损失函数时，对$w$的求导公式:</p><script type="math/tex; mode=display">\begin{aligned}    L_2(w)&=\frac{1}{2}\sum_{i=1}^{n}(\pi(x_i) - y_i)^2 \\    \frac{\partial l_2(w)}{\partial w} &=\sum_{i=1}^{n}[(\pi(x_i) - y_i)*\frac{\partial\pi (x_i)}{\partial z} * \frac{\partial z}{\partial w}] \\    &=\sum_{i=1}^{n}[(\pi(x_i) - y_i)\pi(x_i) (1-\pi(x_i))  x_i]\end{aligned}\tag{10}</script><p>其中, $z=w \cdot x$, 则$\pi (x) = \frac{exp(x)}{1+exp(x)}$,其导数为$\pi^{‘} (x)=\pi(x)(1-\pi(x))$</p><p>这里主要考虑的是优化问题,极大似然估计函数是一个凸函数,这是优化问题再最容易优化的模型，我们可以得到全局最优解，而对于$L_2$，由于Sigmoig函数导数的特性，当$\pi (x)$接近0或者1时，此时的倒数就接近0，从而容易使函数陷入局部最优.</p><p>下图是两个损失函数以w为参数的简化图<br><img src="/images/logistic_regression_l2.png" alt="L2"><br><img src="/images/logistic_regression_cross_entropy.png" alt="likelihood"></p><h1 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h1><p><a href="https://github.com/RanMaosong/Machine-Learning-LiHang/blob/master/code/%E7%AC%AC%E5%85%AD%E7%AB%A0%20%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/plot.ipynb" target="_blank" rel="noopener">绘图源码</a></p><p>逻辑斯蒂回归实现源码: </p><p><a href="https://github.com/RanMaosong/Machine-Learning-LiHang/blob/master/code/%E7%AC%AC%E5%85%AD%E7%AB%A0%20%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/LogisticRegression.py" target="_blank" rel="noopener">Python</a></p><p><a href="https://github.com/RanMaosong/Machine-Learning-LiHang/blob/master/code/%E7%AC%AC%E5%85%AD%E7%AB%A0%20%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/LogisticRegression.ipynb" target="_blank" rel="noopener">Jupyter</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2019/03/18/ML-ligistic-regression/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://w
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>GloVe:Global Vectors for Word Representation</title>
    <link href="http://yoursite.com/2018/11/21/nlp-glove/"/>
    <id>http://yoursite.com/2018/11/21/nlp-glove/</id>
    <published>2018-11-21T14:33:10.000Z</published>
    <updated>2018-11-22T07:14:58.614Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2018/11/21/nlp-glove/" target="_blank" rel="noopener">Github</a><br><a href="https://www.jianshu.com/p/a6fb7e3530a0" target="_blank" rel="noopener">简书</a><br><a href="https://blog.csdn.net/u014630987/article/details/84339695" target="_blank" rel="noopener">CSDN</a></p><p>词作为自然语言处理中的一个基本单元，如何表示一个词对于后续的处理任务至关重要，最简单的表示方式是 One-hot，但是该方法表示的词之间是相互独立的，因此局限性很大，需要采用一种更合理的表示方法。</p><h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h1><p>目前，学习词向量的方法主流的有两种：</p><ol><li><p>全局矩阵分解的方法，比如LSA，HAL，这类方法首先统计语料库中的“词-文档”或者“词-词”共现矩阵，然后通过矩阵分解的方法来获得一个低维词向量。“词-文档”矩阵是指矩阵的行表示词，列表示文档，矩阵的元素是改词在文档中出现的次数。“词-词”共现矩阵的行和列都表示一个矩阵，</p></li><li><p>局部上下文窗口的方法， 比如skip-gram，通过神经网络的方法使语料库中给定上下文中共同出现的单词对的概率更大</p></li></ol><p>但是这两种方法都有一个缺点全局矩阵分解的方法虽然利用了全局统计信息，但是他会过度重视共现词频高的单词对，然而这些词并没有多大的语义联系，局部上下文窗口的方法虽然在词类比方面的任务表现的很好，但是没有充分利用全局统计信息。</p><p>该篇文章的主要思想就是两者取其长，结合各自的优点进行词向量学习。</p><h1 id="2-Glove"><a href="#2-Glove" class="headerlink" title="2.Glove"></a>2.Glove</h1><p>首先我们先定义一些符号</p><p>$X$ : 表示“词-词”共现矩阵，是一个对阵矩阵</p><p>$X_{ij}$: 表示词 j 出现在中心词 i 的上下文（基于窗口）的次数。</p><p>$X_i$: 表示任何词出现在词 i 上下文的总的次数。</p><p>$P_{ij}=\frac{X_{ij}}{X_i}$: 表示单词 j 出现在 单词 i 的上下文的概率。</p><p>假设我们有一个检测词 k，则存在如下关系</p><div class="table-container"><table><thead><tr><th>$ratio=\frac{P_{ik}}{P_{jk}}$</th><th style="text-align:right">j,k 相关</th><th style="text-align:center">j,k不相</th></tr></thead><tbody><tr><td>i,k相关</td><td style="text-align:right">1</td><td style="text-align:center">非常大</td></tr><tr><td>i,k不相关</td><td style="text-align:right">非常小</td><td style="text-align:center">1</td></tr></tbody></table></div><p>上面表格的意思就是当词 i，k相关时，如果j，k相关，则$ratio=\frac{P_{ik}}{P_{jk}}$接近于1，如果j，k不想管，此时$P_{jk}$非常小，则$ratio=\frac{P_{ik}}{P_{jk}}$的值会很大，如果i，k不相关，我们可以依次类推出上面的结果。</p><p>如果我们单纯地预测$P_{ij}$，此时就变回skip-gram，我们如何利用全局统计信息呢，此时我们可以利用上面表格思想，我们预测概率的比率，即：</p><script type="math/tex; mode=display">F(w_i, w_j, \hat w_k)=\frac{P_{ik}}{P_{ij}} \tag{1}</script><p>其中，$w$表示一个d维的词向量，$\hat w$ 表示上下文检测词。这里的F存在很多解，但是我们可以一步步地添加约束来得到最后的一个解</p><p>因为向量空间具有线性结构，因此我们只考虑函数 $F$ 在目标词的差异上进行运算（这个原因个人觉得理由有点牵强）</p><script type="math/tex; mode=display">F(w_i-w_j, \hat w_k)=\frac{P_{ik}}{P_{ij}} \tag{2}</script><p><strong>这句话是个人的理解</strong>：个人感觉这一部还有个原因是为了减少计算量。</p><p>从上面公式我们可以看到公式左边是个向量，右边是一个标量，因此我们可以使用向量的点乘来解决这个问题。</p><script type="math/tex; mode=display">F((w_i-w_j)^T\hat w_k)=\frac{P_{ik}}{P_{ij}}  \tag{3}</script><p>即</p><script type="math/tex; mode=display">F(w_i^T\hat w_k-w_j^T\hat w_k)=\frac{P_{ik}}{P_{ij}}  \tag{4}</script><p>在这里我们需要寻找一个在 + 和 × 之间的同态函数，比如：</p><script type="math/tex; mode=display">F(w_i^T\hat w_k-w_j^T\hat w_k)=\frac{F(w_i^T\hat w_k)}{F(w_j^T\hat w_k)} \tag{5}</script><p>该式在结合上面地公式(4)：即</p><script type="math/tex; mode=display">\frac{F(w_i^T\hat w_k)}{F(w_i^T\hat w_k)}=\frac{P_{ik}}{P_{ij}} \tag{6}</script><p>因此：</p><script type="math/tex; mode=display">F(w_i^T\hat w_k)=P_{ik}=\frac{X_{ik}}{X_i} \tag{7}</script><p>对于满足公式(5)的函数，我们可以想到有指数函数 exp，即：</p><script type="math/tex; mode=display">\exp(w_i^T\hat w_k-w_j^T\hat w_k)=\frac{\exp(w_i^T\hat w_k)}{\exp(w_j^T\hat w_k)} \tag{8}</script><p>结合公式(7)和(8)我们可以得到</p><script type="math/tex; mode=display">exp(w_i^T\hat w_k)=P_{ik}=\frac{X_{ik}}{X_i} \tag{9}</script><p>我们对其取对数可以得到：</p><script type="math/tex; mode=display">w_i^T\hat w_k=\log(P_{ik})=\log(X_{ik}) - \log(X_i) \tag{10}</script><p>由于上面左边具有交换性，而右边不具有，同时$X_i$和k无关因此可以作为一个偏织项，因此公式(10)可以演变为</p><script type="math/tex; mode=display">w_i^T\hat w_k + b_i + \hat b_k=log(X_{ik}) \tag{11}</script><p>其中$\hat b_k$ 作为 k 的偏置项来保持公式的对称性。</p><p>同事为了防止$X_{ik}$接近0时，log 趋向于负无穷大，因此我们对右式加上一个平滑项，即$\log(X_{ik}) \to log(1+X_{ik})$</p><p>因此该模型的损失函数为：</p><script type="math/tex; mode=display">J = \sum _{i,j}^{V} (w_i^T\hat w_k + b_i + \hat b_k-log(X_{ik}) )^2 \tag{12}</script><p>在这里有个问题就是他对每个单词对平等对待，但是这是不合理的，应该对 $X_{ij}$值较大的，权重应该更大，同时，我们又不应该过度重视高频词，该论文选取的权重函数如下：</p><script type="math/tex; mode=display">f(x)=\begin{cases}-(x/x_{max})^\alpha,\quad if x<x_{max} \\1, otherwise\end{cases}\tag{13}</script><p>最终该损失函数如下：</p><script type="math/tex; mode=display">J = \sum _{i,j}^{V} f(X_{ij})(w_i^T\hat w_k + b_i + \hat b_k-log(X_{ik}) )^2 \tag{14}</script><p>论文中$\alpha$ 和 $x_{max}$分别取值$3/4$ 和 100.</p><h1 id="3-和Skip-gram的联系"><a href="#3-和Skip-gram的联系" class="headerlink" title="3. 和Skip-gram的联系"></a>3. 和Skip-gram的联系</h1><p>前面提过，Glove 结合了之前两种方法的优点，但是他是如何结合或者如何从Skip-gram演变而来的呢？</p><p>总所周知，Skip-gram模型可以通过如下一个公式来表达，即Softmax：</p><script type="math/tex; mode=display">Q_{ij}=\frac{exp(w_i^T\hat w_j)}{\sum_{k=1}^V exp(w_i^T\hat w_k)} \tag{15}</script><p>则该模型的损失函数为：</p><script type="math/tex; mode=display">J =-\sum_{i \in corpus, j \in context(i)} \log Q_{ij} \tag{16}</script><p>由于我们需要利用全局统计信息，因此一个单词对可能出现很多次，因此，我们首先把所有相同的单词对先进行计算，即</p><script type="math/tex; mode=display">\begin{align}J &= -\sum_{i=1}^V\sum_{j=1}^V X_{ij} \log Q_{ij}\\&=-\sum_{i=1}^V X_i \sum_{j=1}^V P_{ij} \log Q_{ij}\\&=\sum_{i=1}^V X_iH(P_i, Q_i)\end{align}\tag {17}</script><p>由于交叉熵具有某些缺点，交叉熵具有长尾效应，当过分重视不太可能的事件时建模效果不是很好，而且交叉熵需要Q归一化，这样计算很大，因此使用这样一个未归一化的最小二成损失来替代 </p><script type="math/tex; mode=display">\hat J = \sum_{i,j} X_i (X_{ij} - \exp(w_i^T \hat w _j))^2\tag{18}</script><p>这里是因为18式中当$X_{ij}$变得很大时，训练变得复杂，于是使用对数可以缓解或补交这个问题,即：</p><script type="math/tex; mode=display">\hat J = \sum_{i,j} X_i (w_i^T \hat w _j- \log X_{ij} )^2\tag{19}</script><p>这里$X_i$可以视为一个权重项，我们通过调整这个权重和添加权重项公式19就演变为公式14.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/11/21/nlp-glove/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://www.jianshu.co
      
    
    </summary>
    
      <category term="NLP(自然语言处理)" scheme="http://yoursite.com/categories/NLP-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="NLP(自然语言处理)" scheme="http://yoursite.com/tags/NLP-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>util.md</title>
    <link href="http://yoursite.com/2018/11/05/util-md/"/>
    <id>http://yoursite.com/2018/11/05/util-md/</id>
    <published>2018-11-05T12:43:15.000Z</published>
    <updated>2018-11-05T12:47:06.151Z</updated>
    
    <content type="html"><![CDATA[<p>收集一些常用的工具包教程链接：</p><ol><li>[Visdom]:      <a href="https://zhuanlan.zhihu.com/p/32025746(知乎" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32025746(知乎</a>)</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;收集一些常用的工具包教程链接：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;[Visdom]:      &lt;a href=&quot;https://zhuanlan.zhihu.com/p/32025746(知乎&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://zhu
      
    
    </summary>
    
      <category term="Util" scheme="http://yoursite.com/categories/Util/"/>
    
    
      <category term="Util" scheme="http://yoursite.com/tags/Util/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode 172.FactorialTrailingZeroes(阶乘后的零)</title>
    <link href="http://yoursite.com/2018/10/07/LeetCode-172-FactorialTrailingZeroes/"/>
    <id>http://yoursite.com/2018/10/07/LeetCode-172-FactorialTrailingZeroes/</id>
    <published>2018-10-07T09:14:00.000Z</published>
    <updated>2018-10-07T11:00:04.527Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2018/10/07/LeetCode-172-FactorialTrailingZeroes/" target="_blank" rel="noopener">GitHub链接</a><br><a href="https://www.jianshu.com/p/2d441cf3f049" target="_blank" rel="noopener">简书链接</a><br><a href="https://blog.csdn.net/u014630987/article/details/82960232" target="_blank" rel="noopener">CSDN链接</a></p><h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><p>给定一个整数 n， 返回 $n!$ 结果尾数中零的个数。<br><strong>示例1</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入: 3</span><br><span class="line">输出: 0</span><br><span class="line">解释: 3! = 6, 尾数中没有零。</span><br></pre></td></tr></table></figure></p><p><strong>示例2</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入: 5</span><br><span class="line">输出: 1</span><br><span class="line">解释: 5! = 120, 尾数中有 1 个零.</span><br></pre></td></tr></table></figure></p><p><strong>说明:</strong> 算法的时间复杂度应为 $O(log n)$</p><h1 id="方法一（错误的）"><a href="#方法一（错误的）" class="headerlink" title="方法一（错误的）"></a>方法一（错误的）</h1><p>求出 $n!$ 的值或者在求的过程中，遇到末尾有0，先除以10来减小数，虽然该算法原理上可以行，但是有两个问题，一、时间复杂度为 $O(n)$，二、结果不对，出现这个问题的原因是编程语言每种类型有自己的数组范围，因此会溢出。所以该方法实际不可行</p><h1 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h1><p>考虑一个问题，位数为零是由2*5产生的，而吧每个数分解，出现5的次数必出现2的次数要少很多，因此，我们通过统计有多少个 5 即可判断尾数有多少个 0.先上代码在将原理<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LeetCode_172</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">trailingZeroes2</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (n &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            count += n / <span class="number">5</span>;</span><br><span class="line">            n /= <span class="number">5</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        LeetCode_172 leetCode = <span class="keyword">new</span> LeetCode_172();</span><br><span class="line">        System.out.println(leetCode.trailingZeroes(<span class="number">3</span>));</span><br><span class="line">        System.out.println(leetCode.trailingZeroes(<span class="number">5</span>));</span><br><span class="line">        System.out.println(leetCode.trailingZeroes(<span class="number">16</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>首先我们先统计n 包含一个5的个数，然后统计包含两个5的个数，依次类推。比如：35！，包含一个5的个数为5,10， 15， 20， 25， 30， 35，即35/5 = 7个，然而当我们遇到25的元素时，里面包含两个5，所以通过 $n/5^2$ ，更大的数一次类推，当 $5^k &gt; n$时即停止，在上面代码中表现为n&gt;0；</p><h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p><a href="https://github.com/RanMaosong/RanMaosong.github.io" target="_blank" rel="noopener">感谢大家的阅读和支持, 欢迎大家上星.</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/10/07/LeetCode-172-FactorialTrailingZeroes/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub链接&lt;/a&gt;&lt;br&gt;&lt;a
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>169. Majority Element(求众数)</title>
    <link href="http://yoursite.com/2018/10/06/LeetCode-169-MajorityElement/"/>
    <id>http://yoursite.com/2018/10/06/LeetCode-169-MajorityElement/</id>
    <published>2018-10-06T14:03:52.000Z</published>
    <updated>2018-10-07T08:11:02.871Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2018/10/06/LeetCode-136-SingleNumber/" target="_blank" rel="noopener">GIthub</a><br><a href="https://www.jianshu.com/p/cf75842c46c0" target="_blank" rel="noopener">简书</a><br><a href="https://blog.csdn.net/u014630987/article/details/82958884" target="_blank" rel="noopener">CSDN</a></p><h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><p>给定一个大小为 n 的数组，找到其中的众数。众数是指在数组中出现次数大于 ⌊ n/2 ⌋ 的元素。<br>你可以假设数组是非空的，并且给定的数组总是存在众数。<br><strong>示例1：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入: [3,2,3]</span><br><span class="line">输出: 3</span><br></pre></td></tr></table></figure></p><p><strong>示例2：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入: [2,2,1,1,1,2,2]</span><br><span class="line">输出: 2</span><br></pre></td></tr></table></figure></p><h1 id="方法一-暴力破解"><a href="#方法一-暴力破解" class="headerlink" title="方法一: 暴力破解"></a>方法一: 暴力破解</h1><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>最简单最直接的方法是统计每个数出现的次数，如果它出现的次数 大于 $\lfloor n/2 \rfloor$, 则这个数就为这个数组的众数。因此实现此算法，需要两个嵌套的 for 循环，外层循环遍历数组来确定当前值，内层循环同样遍历数组，它则用来统计当前值出现的次数。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LeetCode_169</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">majorityElement</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * O(n^2)</span></span><br><span class="line"><span class="comment">         *  Time Limit Exceeded</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">int</span> max = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.length; ++i) &#123;</span><br><span class="line">            <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; nums.length; ++j) &#123;</span><br><span class="line">                <span class="keyword">if</span> (nums[i] == nums[j])</span><br><span class="line">                    ++count;</span><br><span class="line">            &#125;</span><br><span class="line">            max = Math.max(max, count);</span><br><span class="line">            <span class="keyword">if</span> (max &gt; nums.length / <span class="number">2</span>)</span><br><span class="line">                <span class="keyword">return</span> nums[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h2><ul><li><p>时间复杂度： $O(n^2)$</p><p>由于该方法包含两个嵌套的 for 循环，每个循环迭代n次，因此该算法的时间复杂度为 $O(n^2)$</p></li><li><p>空间复杂度： $O(1)$</p></li></ul><h1 id="方法二：-HashMap"><a href="#方法二：-HashMap" class="headerlink" title="方法二： HashMap"></a>方法二： HashMap</h1><h2 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h2><p>在统计元素出现次数的时候， 我们可以使用一个 HashMap 来保存当前已出现的元素出现的次数，这样可以避免重复的统计，从而在一个 for 循环里完成任务。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Maosong Ran</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2018/10/06</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@email</span> maosongran@gmail.com</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LeetCode_169</span> </span>&#123; </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">majorityElement</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * O(n) O(n)</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        HashMap&lt;Integer, Integer&gt; count = <span class="keyword">new</span> HashMap&lt;Integer, Integer&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i &lt; nums.length; ++i) &#123;</span><br><span class="line">            Integer num = count.get(nums[i]);</span><br><span class="line">            <span class="keyword">if</span> (num == <span class="keyword">null</span>)</span><br><span class="line">                num = <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                ++num;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (num &gt; nums.length / <span class="number">2</span>)</span><br><span class="line">                <span class="keyword">return</span> nums[i];</span><br><span class="line">            count.put(nums[i], num);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nums[<span class="number">0</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="复杂度分析-1"><a href="#复杂度分析-1" class="headerlink" title="复杂度分析"></a>复杂度分析</h2><ul><li><p>时间复杂度： $O(n)$</p><p>由于我们只需要遍历一次数组，即可完成统计任务，因此，时间复杂度为$O(n)$</p></li><li><p>空间复杂度： $O(n)$<br>由于数组总存在众数，而众数的条件是其出现次数大于 $\lfloor n/2 \rfloor$,因此最坏情况，HashMap包含 $n - (\lfloor n/2 \rfloor + 1)$个元素，最好情况包含 1 个元素，因此空间复杂度为O(n)</p></li></ul><h1 id="方法三：-排序"><a href="#方法三：-排序" class="headerlink" title="方法三： 排序"></a>方法三： 排序</h1><p>由于众数的是其出现次数大于 $\lfloor n/2 \rfloor$ 的值，因此无论该值的大小是多少，该值总会出现在中心位置：对于数组长度为偶数时，为最中间两个数，为基数时，为最中间一个数。<br>！<a href="/images/leetcode169.png">Sorting</a></p><p>上图中，数组下面的线表示当众数出现在排序数组的最左侧，数组上面的线表示当总数出现在排序数组的最右侧时，测试是众数出现的两个极端情况，其余情况在这两种情况之间，因此总数总是出现在排序数组的最中心位置。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LeetCode_169</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">majorityElement</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        Arrays.sort(nums);</span><br><span class="line">        <span class="keyword">return</span> nums[nums.length/<span class="number">2</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="复杂度分析-2"><a href="#复杂度分析-2" class="headerlink" title="复杂度分析"></a>复杂度分析</h2><ul><li><p>时间复杂度： $O(nlog n)$</p><p>在Java和Python中，数组排序的时间复杂度为$O(nlog n)$</p></li><li><p>空间复杂度： $O(n)$ 或 $O(1)$</p><p>如果允许在原地进行排序时，我们不需要额外的空间，因此，空间复杂度为$O(1)$,如果不允许，则需要额外的等大的数组来存放该有序数组，因此空间复杂度为 $O(n)$</p></li></ul><h1 id="方法三：-分治法"><a href="#方法三：-分治法" class="headerlink" title="方法三： 分治法"></a>方法三： 分治法</h1><h2 id="算法-2"><a href="#算法-2" class="headerlink" title="算法"></a>算法</h2><p>分治法是算法中经常遇到的一种求解问题的方法，它将大问题小化，复杂问题简单化，因此可以很容易得出问题的解。</p><p>在本题中，我们将数组递归地从中间将大数组分成左右两个小数组，然后求左右两个小数组的的众数，如果这两个众数相等，则这个数也是大数组的众数，如果两个数不相等，则这二者之一必有一个是大数组的众数，为什么呢？</p><p>若假设这两个数之一不是大数组的众数，由于他们是小数组的众数，因此，他们出现次数各占小数组的一半以上，因此这两个数在大数组中的出现次数必大于 $\lfloor n/2 \rfloor$,因此剩下位置的值即便是相同，他们出现的次数也不大于 $\lfloor n/2 \rfloor$</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LeetCode_169</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">majorityElement</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> divideAndConquer(nums, <span class="number">0</span>, nums.length-<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">divideAndConquer</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (left == right)</span><br><span class="line">            <span class="keyword">return</span> nums[left];</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">int</span> mid = (left + right)/<span class="number">2</span>;</span><br><span class="line">            <span class="keyword">int</span> leftMajority = divideAndConquer(nums, left, mid);</span><br><span class="line">            <span class="keyword">int</span> rightMajority = divideAndConquer(nums, mid + <span class="number">1</span>, right);</span><br><span class="line"><span class="comment">//            System.out.println(leftMajority + "-&gt;" + rightMajority);</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (leftMajority == rightMajority)</span><br><span class="line">                <span class="keyword">return</span> leftMajority;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">int</span> leftCount = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">int</span> rightCount = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=left; i &lt;= right; ++i) &#123;</span><br><span class="line">                <span class="keyword">if</span> (nums[i] == leftMajority)</span><br><span class="line">                    ++leftCount;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (nums[i] == rightMajority) &#123;</span><br><span class="line">                    ++rightCount;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"><span class="comment">//            System.out.println(leftMajority + ":" + leftCount + ", " + rightMajority + ": " + rightCount);</span></span><br><span class="line">            <span class="keyword">return</span> leftCount &gt; rightCount ? leftMajority : rightMajority;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="复杂度分析-3"><a href="#复杂度分析-3" class="headerlink" title="复杂度分析"></a>复杂度分析</h2><ul><li><p>时间复杂度： $O(nlog n)$</p></li><li><p>空间复杂度： $O(ll0g n)$</p></li></ul><h1 id="方法五：-摩尔投票法"><a href="#方法五：-摩尔投票法" class="headerlink" title="方法五： 摩尔投票法"></a>方法五： 摩尔投票法</h1><h2 id="算法-3"><a href="#算法-3" class="headerlink" title="算法"></a>算法</h2><p>大致思路，首先有一个统计当前投票数的变量 count， 当 count == 0 时，我们以当前变量作为候选众数，然后从当前变量位置开始，若值等于候选众数的值，则count加1，若不相等，则减一，依次遍历玩数组，当遍历完数组，若 count == 0，则该数组不存在众数，不等于0时，此时的候选众数即位真正的众数。</p><p>原理： 由于众数票数大于 $\lfloor n/2 \rfloor$,因此即便它的票数减去其他所有的票数，它的票数也大于零。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LeetCode_169</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">majorityElement</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>  count = <span class="number">0</span>;</span><br><span class="line">        Integer candiate = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> num : nums) &#123;</span><br><span class="line">            <span class="keyword">if</span> (count == <span class="number">0</span>) &#123;</span><br><span class="line">                candiate = num;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            count += (candiate == num) ? <span class="number">1</span> : -<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> candiate;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="复杂度分析-4"><a href="#复杂度分析-4" class="headerlink" title="复杂度分析"></a>复杂度分析</h2><ul><li><p>时间复杂度： $O(n)$</p></li><li><p>空间复杂度： $O(1)$</p></li></ul><h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p><a href="https://github.com/RanMaosong/RanMaosong.github.io" target="_blank" rel="noopener">感谢大家的阅读和支持, 欢迎大家上星.</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/10/06/LeetCode-136-SingleNumber/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GIthub&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https:
      
    
    </summary>
    
      <category term="Weekly Algorithm" scheme="http://yoursite.com/categories/Weekly-Algorithm/"/>
    
    
      <category term="Weekly Algorithm" scheme="http://yoursite.com/tags/Weekly-Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode 136.Single Number(只出现一次的数字)</title>
    <link href="http://yoursite.com/2018/10/06/LeetCode-136-SingleNumber/"/>
    <id>http://yoursite.com/2018/10/06/LeetCode-136-SingleNumber/</id>
    <published>2018-10-06T05:49:39.000Z</published>
    <updated>2018-10-07T09:14:11.348Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2018/10/06/LeetCode-136-SingleNumber/" target="_blank" rel="noopener">GitHub</a><br><a href="https://www.jianshu.com/p/df7d256e2bea" target="_blank" rel="noopener">简书</a><br><a href="https://blog.csdn.net/u014630987/article/details/82951316" target="_blank" rel="noopener">CSDN</a></p><h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><p>给定一个非空整数数组，除了某个元素只出现一次以外，其余每个元素均出现两次。找出那个只出现了一次的元素。</p><p><strong>说明</strong><br>你的算法应该具有线性时间复杂度。 你可以不使用额外空间来实现吗？</p><p><strong>示例1</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入: [2,2,1]</span><br><span class="line">输出: 1</span><br></pre></td></tr></table></figure></p><p><strong>示例2</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入: [4,1,2,1,2]</span><br><span class="line">输出: 4</span><br></pre></td></tr></table></figure></p><p><strong>难度系数:</strong> 简单</p><h1 id="解题思路一"><a href="#解题思路一" class="headerlink" title="解题思路一"></a>解题思路一</h1><p>常规方法是,遍历数组,然后统计每个值出现的次数,最后在选择出现次数为1的那个值.该算法的时间复杂度为O(N),首先是统计数组,此时要遍历整个数组,然后是要遍历我们的统计数组,此时有事一个O(N),由于我们使用了一个统计数组来保存每个值出现的次数,此时需要的空间复杂度为O(n),因此不符合要求.</p><h1 id="解题思路二"><a href="#解题思路二" class="headerlink" title="解题思路二"></a>解题思路二</h1><p>为了解决不需要额外的空间这个要求,我们可以使用位操作中的异或规则来进行处理.异或运算法则如下</p><ol><li>a $\oplus$ a = 0, a $\oplus$ 0=a</li><li>a $\oplus$ b = b $\oplus$ a</li><li>a $\oplus$ b $\oplus$ c = a $\oplus$ (b $\oplus$ c) = a $\oplus$ (c $\oplus$ b) = (a $\oplus$ b) $\oplus$ c</li></ol><p>其中,第一条规则说明,当某个数出现两次时,通过 $\oplus$ 变为0,出现一次时依然保持原来的数,第二、三条的交换律和分配律说明通过多次 $\oplus$ 操作最终解决本题。</p><p><strong>注意</strong>: 本体题目中指出除了某个元素值出现一次外其余的均出现两次，根据法则一可以看出本算法只适合除了某个元素出现一次外，其余元素出现偶数次的情况。</p><p>比如在示例二中的 $\oplus$ 操作：</p><script type="math/tex; mode=display">\begin{aligned}    4 \oplus 1 \oplus 2 \oplus 1 \oplus 2 &=  4 \oplus 1 \oplus 1 \oplus 2 \oplus 2 \\    &= 4 \oplus (1 \oplus 1) \oplus (2 \oplus 2)\\    &=4\end{aligned}</script><h1 id="Java实现代码"><a href="#Java实现代码" class="headerlink" title="Java实现代码"></a>Java实现代码</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Maosong Ran</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2018/10/06</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@email</span> maosongran@gmail.com</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LeetCode_136</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">singleNumber</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> len = nums.length;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i &lt; len; ++i)</span><br><span class="line">            result ^= nums[i];</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        LeetCode_136 leetCode = <span class="keyword">new</span> LeetCode_136();</span><br><span class="line">        System.out.println(leetCode.singleNumber(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>&#125;));</span><br><span class="line">        System.out.println(leetCode.singleNumber(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>&#125;));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>输出</strong>:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">4</span><br></pre></td></tr></table></figure></p><h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p><a href="https://github.com/RanMaosong/RanMaosong.github.io" target="_blank" rel="noopener">感谢大家的阅读和支持, 欢迎大家上星.</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/10/06/LeetCode-136-SingleNumber/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https:
      
    
    </summary>
    
      <category term="Weekly Algorithm" scheme="http://yoursite.com/categories/Weekly-Algorithm/"/>
    
    
      <category term="Weekly Algorithm" scheme="http://yoursite.com/tags/Weekly-Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>数据结构第二次作业</title>
    <link href="http://yoursite.com/2018/09/17/ta-work-02/"/>
    <id>http://yoursite.com/2018/09/17/ta-work-02/</id>
    <published>2018-09-17T13:09:48.000Z</published>
    <updated>2018-09-22T04:25:57.776Z</updated>
    
    <content type="html"><![CDATA[<h1 id="实现在单向链表中-返回某个节点的前驱"><a href="#实现在单向链表中-返回某个节点的前驱" class="headerlink" title="实现在单向链表中,返回某个节点的前驱."></a>实现在单向链表中,返回某个节点的前驱.</h1><h2 id="原理说明"><a href="#原理说明" class="headerlink" title="原理说明"></a>原理说明</h2><p>单向链表的特点是链表只有一个方向,即只有从前驱结点指向后继结点的指针,而没有后继节点指向前驱结点的指针,结构图大致如下:<br><img src="https://upload-images.jianshu.io/upload_images/5208761-03db3a5a57fd70ed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="linklist1.png"><br>从图可以看出,如果我们要访问第三个结点,我们只能从头指针依次便利每个后继结点,直至访问到第三个结点,因此在单向链表中,我们查找某个元素的时间复杂度 $O(n)$.</p><p>基于链表的单向特点,因此我们必须从头开始遍历才能找到某个节点的前驱.以上图为例进行讲解,假如我们要寻找第三个结点的前驱,我们如何实现呢?</p><ol><li>首先我们需要申明两个临时变量pre 和 cur,分别指向Head 头结点和Head-&gt;next(为null或为第一个结点),这样pre和cur就构成一个相关的对,pre表示cur的前驱,当cur到达第三个结点时,此时pre就是我们要找的前驱.<br><img src="https://upload-images.jianshu.io/upload_images/5208761-bb412a44ed651d75.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="linklist2.png"></li><li>再循环中,我们依次把pre和cur向前同时推进,直至cur到达事先定义的结点.<ul><li>在上图中,cur=1,不是我们要找的结点3,因此pre和cur向前移动,即pre=cur, cur=next;<br><img src="https://upload-images.jianshu.io/upload_images/5208761-ba2086541433a47a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="linklist3.png"></li></ul></li></ol><ul><li>此时,cur=2,不符合我们的条件,我们继续执行上面的操作;<br><img src="https://upload-images.jianshu.io/upload_images/5208761-54cc3f96b5132267.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="linklist4.png"><br>现在cur=3,满足我们的条件,此时pre就是他的前驱,返回该节点即可.<h2 id="下面是实现代码-仅供参考"><a href="#下面是实现代码-仅供参考" class="headerlink" title="下面是实现代码,仅供参考"></a>下面是实现代码,仅供参考</h2><h3 id="c语言"><a href="#c语言" class="headerlink" title="　c语言"></a>　<strong>c语言</strong></h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;malloc.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 数据类型</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">int</span> ElementType;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 链表数据结构</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    ElementType data;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span>* <span class="title">next</span>;</span></span><br><span class="line">&#125;Node, *Linklist;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建链表</span></span><br><span class="line"><span class="function">Linklist <span class="title">createList</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> len;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"请输入链表长度:"</span>);</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;len);</span><br><span class="line">    Linklist head = (Linklist)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(Node));</span><br><span class="line">    Linklist tail = head;</span><br><span class="line">    tail-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;len; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> val;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"第%d个节点内容:"</span>, (i+<span class="number">1</span>));</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;val);</span><br><span class="line">        Linklist node = (Linklist)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(Node));</span><br><span class="line">        node-&gt;data = val;</span><br><span class="line">        node-&gt;next = tail-&gt;next;</span><br><span class="line">        tail-&gt;next = node;</span><br><span class="line">        tail = node;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> head;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 查找给定节点的前驱</span></span><br><span class="line"><span class="function">Node* <span class="title">preNodeOf</span><span class="params">(Linklist <span class="built_in">list</span>, Node* node)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Node* cur = <span class="built_in">list</span>-&gt;next;</span><br><span class="line">    Node* pre = <span class="built_in">list</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> (cur != <span class="literal">NULL</span> &amp;&amp; cur-&gt;data != node-&gt;data)</span><br><span class="line">    &#123;</span><br><span class="line">        </span><br><span class="line">        pre = cur;</span><br><span class="line">        cur = cur-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (pre != <span class="built_in">list</span>)</span><br><span class="line">        <span class="keyword">return</span> pre;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 根据索引查找指定节点,index 从1开始</span></span><br><span class="line"><span class="function">Node* <span class="title">getNode</span><span class="params">(Linklist <span class="built_in">list</span>, <span class="keyword">int</span> index)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Node* node = <span class="built_in">list</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;index; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        node = node-&gt;next;</span><br><span class="line">        <span class="keyword">if</span> (node == <span class="literal">NULL</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> node;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 显示链表内容</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">showLinklist</span><span class="params">(Linklist <span class="built_in">list</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Node* node = <span class="built_in">list</span>-&gt;next;</span><br><span class="line">    <span class="keyword">while</span> (node != <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d  "</span>, node-&gt;data);</span><br><span class="line">        node = node-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">length</span><span class="params">(Linklist <span class="built_in">list</span>)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> len = <span class="number">0</span>;</span><br><span class="line">    Node* node = <span class="built_in">list</span>-&gt;next;</span><br><span class="line">    <span class="keyword">while</span> (node != <span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        ++len;</span><br><span class="line">        node = node-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> len;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Linklist <span class="built_in">list</span> = createList();</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"链表内容:"</span>);</span><br><span class="line">    showLinklist(<span class="built_in">list</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> len = length(<span class="built_in">list</span>);</span><br><span class="line">    <span class="keyword">int</span> index;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"\n输入需要查找前驱的节点的索引(0&lt; n &lt;=%d):"</span>, len);</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;index);</span><br><span class="line">    Node* node = getNode(<span class="built_in">list</span>, index);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"第%d个节点的内容:%d\n"</span>, index, node-&gt;data);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    node = preNodeOf(<span class="built_in">list</span>, node);</span><br><span class="line">    <span class="keyword">if</span> (node != <span class="literal">NULL</span>)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, node-&gt;data);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"该节点为第一个节点,无前驱节点!"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h3 id="C-代码-这里使用了模板类"><a href="#C-代码-这里使用了模板类" class="headerlink" title="　C++代码:这里使用了模板类"></a>　<strong>C++代码</strong>:这里使用了模板类</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 节点类, 使用了模板类,方便数据类型</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">Node</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    T data;</span><br><span class="line">    Node&lt;T&gt;* next;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    Node()&#123;&#125;</span><br><span class="line">    Node(T v, Node&lt;T&gt;* node)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">this</span>-&gt;data = v;</span><br><span class="line">        <span class="keyword">this</span>-&gt;next = node;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">LinkList</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    Node&lt;T&gt;* head;</span><br><span class="line">    Node&lt;T&gt;* tail;</span><br><span class="line">    <span class="keyword">int</span> count;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    LinkList();</span><br><span class="line">    ~LinkList();</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">size</span><span class="params">()</span></span>;</span><br><span class="line">    Node&lt;T&gt;* getNode(<span class="keyword">int</span>);</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">append</span><span class="params">(T data)</span></span>;</span><br><span class="line">    Node&lt;T&gt;* preNodeOf(Node&lt;T&gt;*);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 初始化链表</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">LinkList</span>&lt;T&gt;:</span>:LinkList(): count(<span class="number">0</span>)</span><br><span class="line">&#123;</span><br><span class="line">    head = <span class="keyword">new</span> Node&lt;T&gt;;</span><br><span class="line">    head-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">    tail = head;</span><br><span class="line">    <span class="keyword">int</span> length;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"请输入链表初始长度:"</span>;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; length;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; length; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        T data;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"第"</span> &lt;&lt; (i+<span class="number">1</span>) &lt;&lt; <span class="string">"个节点的内容:"</span>;</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; data;</span><br><span class="line">        append(data);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">LinkList</span>&lt;T&gt;:</span>:~LinkList()</span><br><span class="line">&#123;</span><br><span class="line">    Node&lt;T&gt;* node = head-&gt;next;</span><br><span class="line">    Node&lt;T&gt;* tmp;</span><br><span class="line">    <span class="keyword">while</span> (node != <span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        tmp = node;</span><br><span class="line">        node = node-&gt;next;</span><br><span class="line">        <span class="keyword">delete</span> tmp;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">delete</span> head;</span><br><span class="line">    head = <span class="literal">NULL</span>;</span><br><span class="line">    tail = <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 返回链表的长度</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">int</span> <span class="title">LinkList</span>&lt;T&gt;:</span>:size()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> count;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">Node</span>&lt;T&gt;* <span class="title">LinkList</span>&lt;T&gt;:</span>:getNode(<span class="keyword">int</span> index)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (index &gt; count || index &lt;= <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt; <span class="string">"Index Error!"</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    Node&lt;T&gt;* node = head;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;index &amp;&amp; node-&gt;next; ++i) </span><br><span class="line">    &#123;</span><br><span class="line">        node = node-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> node;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">LinkList</span>&lt;T&gt;:</span>:print()</span><br><span class="line">&#123;</span><br><span class="line">    Node&lt;T&gt;* node = head-&gt;next;</span><br><span class="line">    <span class="keyword">while</span> (node)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; node-&gt;data &lt;&lt; <span class="string">"  "</span>;</span><br><span class="line">        node = node-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">LinkList</span>&lt;T&gt;:</span>:append(T data)</span><br><span class="line">&#123;</span><br><span class="line">    Node&lt;T&gt;* node = <span class="keyword">new</span> Node&lt;T&gt;();</span><br><span class="line">    node-&gt;data = data;</span><br><span class="line">    node-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">    tail-&gt;next = node;</span><br><span class="line">    tail = node;</span><br><span class="line">    ++count;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">Node</span>&lt;T&gt;* <span class="title">LinkList</span>&lt;T&gt;:</span>:preNodeOf(Node&lt;T&gt;* node)</span><br><span class="line">&#123;</span><br><span class="line">    Node&lt;T&gt;* pre = head;</span><br><span class="line">    Node&lt;T&gt;* cur = head-&gt;next;</span><br><span class="line">    <span class="keyword">while</span> (cur-&gt;data != node-&gt;data)</span><br><span class="line">    &#123;</span><br><span class="line">        pre = cur;</span><br><span class="line">        cur = cur-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (pre == head) </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> pre;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    LinkList&lt;<span class="keyword">int</span>&gt; <span class="built_in">list</span> = LinkList&lt;<span class="keyword">int</span>&gt;();</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"链表内容:"</span>;</span><br><span class="line">    <span class="built_in">list</span>.print();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> size = <span class="built_in">list</span>.size();</span><br><span class="line">    <span class="keyword">int</span> index;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"输入需要查找前驱的节点的索引(1 &lt;= n &lt;="</span> &lt;&lt; size &lt;&lt; <span class="string">"):"</span>;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; index;</span><br><span class="line">    Node&lt;<span class="keyword">int</span>&gt;* node = <span class="built_in">list</span>.getNode(index);</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"第"</span> &lt;&lt; index &lt;&lt; <span class="string">"个节点的内容:"</span> &lt;&lt; node-&gt;data &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    Node&lt;<span class="keyword">int</span>&gt;* pre = <span class="built_in">list</span>.preNodeOf(node);</span><br><span class="line">    <span class="keyword">if</span> (pre)</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"第"</span> &lt;&lt; index &lt;&lt; <span class="string">"个节点的前驱:"</span> &lt;&lt; pre-&gt;data &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"该节点为第一个节点,无前驱"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面的代码仅供学习使用,课程要求为C/C++变成语言.</p><h3 id="Python3"><a href="#Python3" class="headerlink" title="　Python3"></a>　<strong>Python3</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data, next=None)</span>:</span></span><br><span class="line">        self.data = data</span><br><span class="line">        self.next = next</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinkList</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self._head = Node(<span class="number">0</span>)</span><br><span class="line">        self._tail = self._head</span><br><span class="line">        self._count = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">append</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        node = Node(data, <span class="keyword">None</span>)</span><br><span class="line">        self._tail.next = node</span><br><span class="line">        self._tail = node</span><br><span class="line">        self._count += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._count</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">show</span><span class="params">(self)</span>:</span></span><br><span class="line">        node = self._head.next</span><br><span class="line">        <span class="keyword">while</span> (node):</span><br><span class="line">            print(node.data, end=<span class="string">"  "</span>)</span><br><span class="line">            node = node.next</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_n_elems</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            data = int(input(<span class="string">"请输入第%d个节点的内容:"</span> % (i+<span class="number">1</span>)))</span><br><span class="line">            self.append(data)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pre_node_of</span><span class="params">(self, node)</span>:</span></span><br><span class="line">        pre = self._head</span><br><span class="line">        cur = pre.next</span><br><span class="line">        <span class="keyword">while</span>(cur.data != node.data):</span><br><span class="line">            pre = cur</span><br><span class="line">            cur = cur.next</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (pre == self._head):</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> pre</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_node</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> (<span class="number">1</span> &lt;= index &lt;= self.size() ):</span><br><span class="line">            <span class="keyword">raise</span> Exception(<span class="string">"Index Error!"</span>)</span><br><span class="line">        node = self._head</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(index):</span><br><span class="line">            node = node.next</span><br><span class="line">        <span class="keyword">return</span> node</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    linklist = LinkList()</span><br><span class="line">    length = int(input(<span class="string">"请输入链表长度:"</span>))</span><br><span class="line">    linklist.add_n_elems(length)</span><br><span class="line">    print(<span class="string">"链表内容:"</span>, end=<span class="string">""</span>)</span><br><span class="line">    linklist.show()</span><br><span class="line"></span><br><span class="line">    index = int(input(<span class="string">"\n输入需要查找前驱的节点的索引(0&lt; n &lt;=%d):"</span> % linklist.size()))</span><br><span class="line">    node = linklist.get_node(index)</span><br><span class="line">    pre_node = linklist.pre_node_of(node)</span><br><span class="line">    print(<span class="string">"第%d个节点的内容:%d"</span> % (index, node.data))</span><br><span class="line">    <span class="keyword">if</span> pre_node <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        print(<span class="string">"第%d个节点的前驱的内容内容:%d"</span> % (index, pre_node.data))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"该节点无前驱节点"</span>)</span><br></pre></td></tr></table></figure><h3 id="Java"><a href="#Java" class="headerlink" title="　Java"></a>　<strong>Java</strong></h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Scanner;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> T data;</span><br><span class="line">    <span class="keyword">public</span> Node&lt;T&gt; next;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Node</span><span class="params">()</span></span>&#123;&#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Node</span><span class="params">(T data)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.data = data;</span><br><span class="line">        <span class="keyword">this</span>.next = <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linklist</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Node&lt;T&gt; head;</span><br><span class="line">    <span class="keyword">private</span> Node&lt;T&gt; tail;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> count;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Linklist</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        head = <span class="keyword">new</span> Node&lt;T&gt;();</span><br><span class="line">        tail = head;</span><br><span class="line">        count = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">append</span><span class="params">(T data)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        Node&lt;T&gt; node = <span class="keyword">new</span> Node(data);</span><br><span class="line">        tail.next = node;</span><br><span class="line">        tail = node;</span><br><span class="line">        count++;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">size</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Node&lt;T&gt; <span class="title">getNode</span><span class="params">(<span class="keyword">int</span> index)</span>  <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (index &lt; <span class="number">1</span> || index &gt; count)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> Exception(<span class="string">"Index Error!"</span>);</span><br><span class="line">        Node&lt;T&gt; node = head;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;index; ++i) &#123;</span><br><span class="line">            node = node.next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> node;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Node&lt;T&gt; <span class="title">preNodeOf</span><span class="params">(Node&lt;T&gt; node)</span> </span>&#123;</span><br><span class="line">        Node&lt;T&gt; pre = head;</span><br><span class="line">        Node&lt;T&gt; cur = pre.next;</span><br><span class="line">        <span class="keyword">while</span> (cur.data != node.data) &#123;</span><br><span class="line">            pre = cur;</span><br><span class="line">            cur = cur.next;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (pre == head)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> pre;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">show</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Node&lt;T&gt; node = head.next;</span><br><span class="line">        <span class="keyword">while</span> (node != <span class="keyword">null</span>) &#123;</span><br><span class="line">            System.out.print(node.data + <span class="string">"  "</span>);</span><br><span class="line">            node = node.next;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Demo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Scanner scan = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line"></span><br><span class="line">        Linklist&lt;Integer&gt; list = <span class="keyword">new</span> Linklist&lt;Integer&gt;();</span><br><span class="line">        <span class="keyword">int</span> length;</span><br><span class="line">        System.out.print(<span class="string">"请输入链表长度:"</span>);</span><br><span class="line">        length = scan.nextInt();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;length; ++i) &#123;</span><br><span class="line">            System.out.print(<span class="string">"请输入第"</span> + (i+<span class="number">1</span>) + <span class="string">"个节点的内容:"</span>);</span><br><span class="line">            <span class="keyword">int</span> data = scan.nextInt();</span><br><span class="line">            list.append(data);</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.print(<span class="string">"链表内容:"</span>);</span><br><span class="line">        list.show();</span><br><span class="line"></span><br><span class="line">        System.out.print(<span class="string">"\n输入需要查找前驱的节点的索引(1&lt;= n &lt;="</span> + list.size() + <span class="string">")"</span>);</span><br><span class="line">        <span class="keyword">int</span> index = scan.nextInt();</span><br><span class="line">        Node&lt;Integer&gt; node;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            node = list.getNode(index);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            <span class="comment">// e.printStack();</span></span><br><span class="line">            node = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">            </span><br><span class="line">        Node&lt;Integer&gt; pre_node = list.preNodeOf(node);</span><br><span class="line">        System.out.println(<span class="string">"第"</span> + index + <span class="string">"个节点的内容:"</span> + node.data);</span><br><span class="line">        <span class="keyword">if</span> (pre_node != <span class="keyword">null</span>)</span><br><span class="line">            System.out.println(<span class="string">"第"</span> + index + <span class="string">"个节点的前驱的内容内容:"</span>  + pre_node.data);</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            System.out.println(<span class="string">"该节点无前驱节点"</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;实现在单向链表中-返回某个节点的前驱&quot;&gt;&lt;a href=&quot;#实现在单向链表中-返回某个节点的前驱&quot; class=&quot;headerlink&quot; title=&quot;实现在单向链表中,返回某个节点的前驱.&quot;&gt;&lt;/a&gt;实现在单向链表中,返回某个节点的前驱.&lt;/h1&gt;&lt;h2 id=
      
    
    </summary>
    
      <category term="Teaching Assistant" scheme="http://yoursite.com/categories/Teaching-Assistant/"/>
    
    
      <category term="Teaching Assistant" scheme="http://yoursite.com/tags/Teaching-Assistant/"/>
    
  </entry>
  
  <entry>
    <title>数据结构第一次作业</title>
    <link href="http://yoursite.com/2018/09/17/ta-work-01/"/>
    <id>http://yoursite.com/2018/09/17/ta-work-01/</id>
    <published>2018-09-17T13:09:42.000Z</published>
    <updated>2018-09-22T04:25:47.127Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-逻辑结构和物理结构有什么不同"><a href="#1-逻辑结构和物理结构有什么不同" class="headerlink" title="1.    逻辑结构和物理结构有什么不同?"></a>1.    逻辑结构和物理结构有什么不同?</h1><p>逻辑结构是指从操作对象抽象出的数学模型，其结构定义中的关系用于描述数据元素之间的逻辑关系。<br>逻辑结构在计算机中的表示称为物理结构或存储结构，根据数据元素在计算机中的表示方法，可分为顺序存储结构与链式存储结构。</p><h1 id="2-算法和程序有什么不同？"><a href="#2-算法和程序有什么不同？" class="headerlink" title="2.    算法和程序有什么不同？"></a>2.    算法和程序有什么不同？</h1><p>算法是指解决问题的一种方法或一个过程。<br>算法是若干指令的有穷序列，满足<strong>性质</strong>：</p><blockquote><p>(1)输入：由外部提供的量作为算法的输入.<br>(2)输出：算法产生至少一个量作为输出.<br>(3)确定性：算法的每一步骤必须有确切的定义.<br>(4)有限性：算法的有穷性是指算法必须能在执行有限个步骤之后终止.<br>(5) 可行性，算法需要考虑设计的可能，程序则具体是实现算法上的设计</p></blockquote><p>程序是算法在计算机上用某种程序设计语言的具体实现。<br>程序可以不满足算法的性质(4)。<br>例如操作系统，是一个在无限循环中执行的程序，因而不是一个算法。<br>操作系统的各种任务可看成是单独的问题，每一个问题由操作系统中的一个子程序通过特定的算法来实现。该子程序得到输出结果后便终止。</p><h1 id="3-什么是ADT？"><a href="#3-什么是ADT？" class="headerlink" title="3.    什么是ADT？"></a>3.    什么是ADT？</h1><p>抽象数据类型（ADT）是一个实现包括储存数据元素的存储结构以及实现基本操作的算法，是数据结构作为一个软件组件的实现。ADT的接口用一种类型上的一组操作来定义，每一个操作由它的输入和输出定义。ADT并不会指定数据类型如何实现，这些实现细节对于ADT的用户是隐藏的，并且通过封装来阻止外部对它的访问。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-逻辑结构和物理结构有什么不同&quot;&gt;&lt;a href=&quot;#1-逻辑结构和物理结构有什么不同&quot; class=&quot;headerlink&quot; title=&quot;1.    逻辑结构和物理结构有什么不同?&quot;&gt;&lt;/a&gt;1.    逻辑结构和物理结构有什么不同?&lt;/h1&gt;&lt;p&gt;逻辑结构
      
    
    </summary>
    
      <category term="Teaching Assistant" scheme="http://yoursite.com/categories/Teaching-Assistant/"/>
    
    
      <category term="Teaching Assistant" scheme="http://yoursite.com/tags/Teaching-Assistant/"/>
    
  </entry>
  
  <entry>
    <title>多元线性回归——Day3</title>
    <link href="http://yoursite.com/2018/09/13/ML-100-Days-003/"/>
    <id>http://yoursite.com/2018/09/13/ML-100-Days-003/</id>
    <published>2018-09-13T13:43:18.000Z</published>
    <updated>2018-09-13T14:40:14.231Z</updated>
    
    <content type="html"><![CDATA[<p><strong>相关链接:</strong><br><a href="https://ranmaosong.github.io/2018/09/13/ML-100-Days-003/" target="_blank" rel="noopener">Github地址</a><br><a href="https://www.jianshu.com/p/56b31d24823c" target="_blank" rel="noopener">简书地址</a><br><a href="https://blog.csdn.net/u014630987/article/details/82695498" target="_blank" rel="noopener">CSDN地址</a></p><hr><p>上一节我们讲解了简单现行回归,该回归的输入特征只有 1 个.本节我们对多元线性回归进行讲解,其输入具有多个特征.多元线性回归通过对训练数据拟合一个多元线性方程来对2或多个特征和一个响应值之间进行建模.多元线性回归的处理步骤和上一节的简单线性回归类似,只是在评估阶段存在差异.你可以通过多元线性回归来发现那个因素或特征对预测结果具有较大的影响和找到不同特征是如何相互影响的.多元线性回归数学表达如下:</p><script type="math/tex; mode=display">y = b_0 + b_1x_1 + b_2x_2 ...... + b_nx_n</script><p>从公式我们可以发现, 当n=1时,多元线性回归就变成简单线性回归,因此,简单线性回归是多元线性回归的一个特例.</p><h1 id="说在前面的话"><a href="#说在前面的话" class="headerlink" title="说在前面的话"></a>说在前面的话</h1><h2 id="假设"><a href="#假设" class="headerlink" title="假设"></a>假设</h2><p>在我们进行线性回归建模时,</p><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><h2 id="1-数据预处理"><a href="#1-数据预处理" class="headerlink" title="1. 数据预处理"></a>1. 数据预处理</h2><h2 id="2-训练模型"><a href="#2-训练模型" class="headerlink" title="2. 训练模型"></a>2. 训练模型</h2><h2 id="3-预测测试集"><a href="#3-预测测试集" class="headerlink" title="3. 预测测试集"></a>3. 预测测试集</h2><h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p><a href="https://github.com/RanMaosong/RanMaosong.github.io" target="_blank" rel="noopener">感谢大家的阅读和支持, 欢迎大家上星.</a>.该博客的原始Github项目地址<a href="https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day3_Multiple_Linear_Regression.md" target="_blank" rel="noopener">点击这里</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;相关链接:&lt;/strong&gt;&lt;br&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/09/13/ML-100-Days-003/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github地址&lt;/a&gt;&lt;
      
    
    </summary>
    
      <category term="机器学习100天" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    
    
      <category term="机器学习100天" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    
  </entry>
  
  <entry>
    <title>简单线性回归——Day2</title>
    <link href="http://yoursite.com/2018/09/13/ML-100-Days-002/"/>
    <id>http://yoursite.com/2018/09/13/ML-100-Days-002/</id>
    <published>2018-09-13T06:22:04.000Z</published>
    <updated>2018-09-13T13:36:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>相关链接:</strong><br><a href="https://ranmaosong.github.io/2018/09/13/ML-100-Days-002/" target="_blank" rel="noopener">Github地址</a><br><a href="https://www.jianshu.com/p/ff991ff8d4c7" target="_blank" rel="noopener">简书地址</a><br><a href="https://blog.csdn.net/u014630987/article/details/82687507" target="_blank" rel="noopener">CSDN地址</a></p><hr><p>在预测问题中,我们会经常遇到两种常用术语:回归(Regression)和分类(classification),他们的区别是回归算法解决的是预测连续值,而分类问题则是预测的是离散值,因此回归模型的输出是无限的,而分类问题的输出是有限的.</p><p>在统计学中，线性回归是利用称为线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。</p><p>本文首先介绍简单线性回归，下一节介绍多元线性回归。简单线性回归就是根据一个特征 <code>X</code> 来预测一个与其相关的变量 <code>Y</code>。通常我们我们假设这两种变量之间是现行相关的,即可以通过一条直线把这些变量区分开.因此,简单线性回归就是我们试图寻找一个线性函数,该函数以特征 <code>X</code> 为输入,输出一个变量,且在训练集中,使其预测的值尽可能接近目标值.</p><p>在本文我们以一个学生学习时间(hours),来预测的他该门课程的分数(scores).我们假设这两个变量之间存在某种线性关系,如图所示<br><img src="/images/simple_linear_regression.png" alt="simple_linear_regression"></p><p>我们的目标是找到最有的 $b_0$ 和 $b_1$,使我们训练集中的数据,我们的预测值和真实值之间的误差最小.即</p><script type="math/tex; mode=display">b_0^*,b_1^* = argmax_{b_0, b_1} sum\{(y_i - y_p)^2\}</script><p>其中, $y_p$ 为我们预测的值, $y_i$为真是值.</p><h1 id="1-数据预处理"><a href="#1-数据预处理" class="headerlink" title="1. 数据预处理"></a>1. 数据预处理</h1><p>数据预处理,我们将按照第一天介绍的模型进行处理:</p><ol><li>导入相关库</li><li>导入数据集</li><li>检查缺失值</li><li>划分数据集</li><li>特征标准化</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">dataset = pd.read_csv(&quot;studentscores.csv&quot;)</span><br><span class="line">X = dataset.iloc[:, 0].values</span><br><span class="line">Y = dataset.iloc[:, 1].values</span><br><span class="line"></span><br><span class="line">from sklearn.cross_validation import train_test_split</span><br><span class="line">X_train, Y_train, X_test, Y_test = train_test_split(X, Y, test_size=1/4, random_state=0)</span><br></pre></td></tr></table></figure><h1 id="2-训练线性回归模型"><a href="#2-训练线性回归模型" class="headerlink" title="2. 训练线性回归模型"></a>2. 训练线性回归模型</h1><p>sklearn 机器学习库为我们提供了许多的常用机器学习模型,线性回归模型 LinearRegression 存在于 sklearn.linear_model 文件中, 该文件为我们提供了许多的线性模型.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">regressor = LinearRegression()</span><br><span class="line">regressor.fit(X_train, Y_train)</span><br></pre></td></tr></table></figure><p>我们首先通过 LinearRegression() 初始化一个 regressor 实例来表示线性回归模型.然后通过给 <code>.fit</code> 传入我们的训练集的特征和标签来训练 regressor.</p><p><strong>注意:</strong> 在 sklearn 中对训练数据的格式有一个规定,对于输入 X, 要求其格式是 N*M,其中 N 表示样本数, M 表示每个样本的特征数, 此示例中 M=1.对于标签 Y, 其格式是N*F, N表示样本数, F表示输出值的个数,此处F=1.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(regressor.coef_, regressor.intercept_)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[9.94167834]] [1.93220425]</span><br></pre></td></tr></table></figure></p><p>regressor.coef_表示模型的权重, regressor.intercept_ 表示模型的偏执,分别表示上面模型公式的$b_1$ 和 $b_0$.</p><h1 id="3-预测结果"><a href="#3-预测结果" class="headerlink" title="3. 预测结果"></a>3. 预测结果</h1><p>当我们通过 <code>.fit</code> 函数训练好后模型,我们可以通过 <code>.predict</code> 函数来预测我们未知的数据.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Y_pred = regressor.predict(X_test)</span><br></pre></td></tr></table></figure></p><p>我们通过手动预测来验证上面提到的 regressor.coef_, regressor.intercept_ 表示的意义:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">temp = regressor.intercept_[<span class="number">0</span>] + regressor.coef_[<span class="number">0</span>] * X_test[<span class="number">0</span>]</span><br><span class="line">print(temp, Y_pred[<span class="number">0</span>])</span><br></pre></td></tr></table></figure></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[16.84472176] [16.84472176]</span><br></pre></td></tr></table></figure></p><h1 id="4-可视化"><a href="#4-可视化" class="headerlink" title="4. 可视化"></a>4. 可视化</h1><p>首先我们可视化训练集的结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X_train, Y_train, color=&quot;red&quot;)</span><br><span class="line">plt.plot(X_train, regressor.predict(X_train), color=&apos;blue&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="/images/simple_linear_regression_fig1.png" alt="figure1"><br>对测试集进行可视化<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X_test, Y_test, color=&quot;red&quot;)</span><br><span class="line">plt.plot(X_test, regressor.predict(X_test), color=&quot;blue&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="/images/simple_linear_regression_fig2.png" alt="figure2"></p><p>其中 <code>.scatter</code> 用于画散点图, <code>.plot</code> 用于画直线.</p><h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p><a href="https://github.com/RanMaosong/RanMaosong.github.io" target="_blank" rel="noopener">感谢大家的阅读和支持, 欢迎大家上星.</a>.该博客的原始Github项目地址<a href="https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day2_Simple_Linear_Regression.md" target="_blank" rel="noopener">点击这里</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;相关链接:&lt;/strong&gt;&lt;br&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/09/13/ML-100-Days-002/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github地址&lt;/a&gt;&lt;
      
    
    </summary>
    
      <category term="机器学习100天" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    
    
      <category term="机器学习100天" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    
  </entry>
  
  <entry>
    <title>数据预处理——Day 1</title>
    <link href="http://yoursite.com/2018/09/11/ML-100-Days-001/"/>
    <id>http://yoursite.com/2018/09/11/ML-100-Days-001/</id>
    <published>2018-09-11T02:53:21.000Z</published>
    <updated>2018-09-13T13:36:13.568Z</updated>
    
    <content type="html"><![CDATA[<p><strong>相关链接:</strong><br><a href="https://ranmaosong.github.io/2018/09/11/ML-100-Days-001/" target="_blank" rel="noopener">Github地址</a><br><a href="https://www.jianshu.com/p/b7b5512395f8" target="_blank" rel="noopener">简书地址</a><br><a href="https://blog.csdn.net/u014630987/article/details/82669796" target="_blank" rel="noopener">CSDN地址</a></p><hr><p>总所周知,对于机器学习任务,特别是对于深度学习任务,我们需要创建训练集,因此需要收集大量的数据.然而,在实际中,我们收集的数据极易受噪声、缺失值和数据不一致的影响。通常我们对数据进行如下几种预处理：</p><ol><li>缺失值处理</li><li>异常值处理</li><li>冗余数据处理</li><li>数据标准化</li><li>数据离散化</li><li>数据向量化: 我自己总结的，主要针对文本数据。不知道算不算预处理，但这是文本数据的必须操作。</li></ol><p>本文主要针对缺失值处理和数据向量化两种与处理方法进行讲解，整个处理过程分为6步：</p><ol><li>导入相关库</li><li>导入数据集</li><li>处理缺失值</li><li>编码分类数据</li><li>创建训练集和测试集</li><li>特征缩放</li></ol><p>下面我们将通过实例来一步步详细介绍这些操作。</p><h1 id="1-导入相关库"><a href="#1-导入相关库" class="headerlink" title="1. 导入相关库"></a>1. 导入相关库</h1><p><a href="http://www.numpy.org/" target="_blank" rel="noopener">numpy</a> 和 <a href="https://pandas.pydata.org/" target="_blank" rel="noopener">pandas</a> 是利用 Python 进行科学计算时非常重要两个科学计算库。其中 numpy 负责提供基本的数学计算函数，起运算对象事针对张量或矩阵；而 pandas 库则用来导入和管理数据集。起导入规则如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure><h1 id="2-导入数据集"><a href="#2-导入数据集" class="headerlink" title="2. 导入数据集"></a>2. 导入数据集</h1><p>pandas 是一个管理数据的函数库，我为我们提供了许多读取和操作数据的接口:</p><p><img src="/images/pd_read_data.png" alt="pandas_read_data"></p><p><code>.csv</code> 格式文件是我们保存数据的一种常用格式，其格式简单，其默认格式是以 ,(英文逗号)分割数据集，显示效果和 excel 表格类似。 pandas 为我们提供了访问 <code>.csv</code> 的接口，其 <code>pd.read_csv()</code>用来进行读取 <code>.csv</code>格式数据,该函数常用的参数如下：</p><ul><li>filename: 文件名</li><li>header: 表头，默认不为空（第一行为表头），设为 None， 表示无表头</li><li>sep: 指定分隔符。如果不指定参数，则会尝试使用逗号分隔</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataset = pd.read_csv(<span class="string">'Data.csv'</span>)</span><br><span class="line">X = dataset.iloc[ : , :<span class="number">-1</span>].values</span><br><span class="line">Y = dataset.iloc[ : , <span class="number">3</span>].values</span><br></pre></td></tr></table></figure><p><a href="https://github.com/Avik-Jain/100-Days-Of-ML-Code/tree/master/datasets" target="_blank" rel="noopener">Data.csv</a>是我们访问的数据集,其内容如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Country,Age,Salary,Purchased</span><br><span class="line">France,44,72000,No</span><br><span class="line">Spain,27,48000,Yes</span><br><span class="line">Germany,30,54000,No</span><br><span class="line">Spain,38,61000,No</span><br><span class="line">Germany,40,,Yes</span><br><span class="line">France,35,58000,Yes</span><br><span class="line">Spain,,52000,No</span><br><span class="line">France,48,79000,Yes</span><br><span class="line">Germany,50,83000,No</span><br><span class="line">France,37,67000,Yes</span><br></pre></td></tr></table></figure></p><p><code>pd.read_csv()</code>返回的数据类型为DataFrame（把他就像成一张表），其没有原始的下表操作，我们访问内容通过如下几个函数：</p><ol><li>loc: 通过行标签访问数据</li><li>iloc: 通过行索引进行访问</li><li>ix: loc 和 iloc 的混合</li></ol><p><code>.value</code> 属性则是将 DataFrame 转成 numpy类型，以便后续的数值计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(type(dataset), type(X), type(Y))</span><br><span class="line">print(X.shape, Y.shape)</span><br></pre></td></tr></table></figure><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; &lt;class &apos;numpy.ndarray&apos;&gt; &lt;class &apos;numpy.ndarray&apos;&gt;</span><br><span class="line">(10, 3) (10,)</span><br><span class="line">[[&apos;France&apos; 44.0 72000.0]</span><br><span class="line"> [&apos;Spain&apos; 27.0 48000.0]</span><br><span class="line"> [&apos;Germany&apos; 30.0 54000.0]</span><br><span class="line"> [&apos;Spain&apos; 38.0 61000.0]</span><br><span class="line"> [&apos;Germany&apos; 40.0 nan]</span><br><span class="line"> [&apos;France&apos; 35.0 58000.0]</span><br><span class="line"> [&apos;Spain&apos; nan 52000.0]</span><br><span class="line"> [&apos;France&apos; 48.0 79000.0]</span><br><span class="line"> [&apos;Germany&apos; 50.0 83000.0]</span><br><span class="line"> [&apos;France&apos; 37.0 67000.0]]</span><br></pre></td></tr></table></figure></p><h1 id="3-处理缺失值"><a href="#3-处理缺失值" class="headerlink" title="3. 处理缺失值"></a>3. 处理缺失值</h1><p>从数据集我们可以发现，某些数据为空，这时我们需要对这些缺失值进行处理。我么可以对缺失值进行均值和中值处理，即用整个数据集的该列数据的均值或中值来替换缺失的值。在sklearn的preprocessing包中包含了对数据集中缺失值的处理，主要是应用Imputer类进行处理。代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line">imputer = Imputer(missing_values = <span class="string">"NaN"</span>, strategy = <span class="string">"mean"</span>, axis = <span class="number">0</span>)</span><br><span class="line">imputer = imputer.fit(X[ : , <span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line">X[ : , <span class="number">1</span>:<span class="number">3</span>] = imputer.transform(X[ : , <span class="number">1</span>:<span class="number">3</span>])</span><br></pre></td></tr></table></figure></p><p>该代码首先导入 Imputer 类，然后通过一些参数来创建 Imputer 实例，参数意思大致如下：</p><ul><li>missing_values: 我们要处理的缺失值类型，由于 <code>pandas.read_csv()</code> 会将确实的值默认设置为 Nan，所以这里其类型为 <code>Nan</code>。</li><li>strategy： 表示采用何种策略，其取值有 mean, median, most_frequent,默认为 mean，即均值。</li><li>axis: 表示我们处理的方向，由于这里列方向是同一类型数据，即主轴方向，因此其值为0.</li></ul><p><strong>注意</strong>: sklearn 库预处理操作有个特征是，其处理分为两步： fit 和 transform，fit 大致可以理解为我们对那些数据进行操作或统计，transform 表示实际进行操作。这一点和 sklearn 中的机器学习算法类似，两个固定接口: fit 和 predict。</p><p>由于我们数据库中的数据只有第2、3 列才是数值数据，我们也只能对这两列数据进行预处理，所欲第三行我们在 <code>.fit</code> 中指出要操作的数据，在其内部分别计算了这两列的均值，以供后面处理使用。</p><p>第四行代码执行缺失值处理，通过第三行已经计算出 相应的均值，所以这行代码实际执行缺失值处理</p><p>第三行和第四行代码可以通过<code>.fit_transform</code> 函数来一步执行。这也是 sklearn 的一个特点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(X)</span><br></pre></td></tr></table></figure><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[[&apos;France&apos; 44.0 72000.0]</span><br><span class="line"> [&apos;Spain&apos; 27.0 48000.0]</span><br><span class="line"> [&apos;Germany&apos; 30.0 54000.0]</span><br><span class="line"> [&apos;Spain&apos; 38.0 61000.0]</span><br><span class="line"> [&apos;Germany&apos; 40.0 63777.77777777778]</span><br><span class="line"> [&apos;France&apos; 35.0 58000.0]</span><br><span class="line"> [&apos;Spain&apos; 38.77777777777778 52000.0]</span><br><span class="line"> [&apos;France&apos; 48.0 79000.0]</span><br><span class="line"> [&apos;Germany&apos; 50.0 83000.0]</span><br><span class="line"> [&apos;France&apos; 37.0 67000.0]]</span><br></pre></td></tr></table></figure></p><h1 id="4-编码分类数据"><a href="#4-编码分类数据" class="headerlink" title="4. 编码分类数据"></a>4. 编码分类数据</h1><p>对于我们计算机进行科学计算只针对数值，而不能对文本进行数值计算，因此我们需要对文本进行数据向量化。如何用数值表示文本呢? 最常用的的格式就是 one-hot 格式,这种格式将文本表示成一个向量,该向量只有一个元素的值为1,其余全为0.</p><p>比如,我们数据集有”a”, “b”, “c”, “d”四个文本,此时我们的向量长度则为4,则这四个文本表示如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a: [1, 0, 0, 0]</span><br><span class="line">b: [0, 1, 0, 0]</span><br><span class="line">c: [0, 0, 1, 0]</span><br><span class="line">d: [0, 0, 0, 1]</span><br></pre></td></tr></table></figure></p><p>因此我们需要先统计数据库中所有的文本,并对每个文本从 0 开始进行编号.该过程也通过sklearn库来进行实现:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder, OneHotEncoder</span><br><span class="line">labelencoder_X = LabelEncoder()</span><br><span class="line">X[ : , <span class="number">0</span>] = labelencoder_X.fit_transform(X[ : , <span class="number">0</span>])</span><br></pre></td></tr></table></figure></p><p>由于对于特征(数据集中的最后一列是标签),只有第一列是文本,因此我们需要对第一列进行文本统计.上面代码就是将第一列的文本用自己的编号来进行表示.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(X)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[[0 44.0 72000.0]</span><br><span class="line"> [2 27.0 48000.0]</span><br><span class="line"> [1 30.0 54000.0]</span><br><span class="line"> [2 38.0 61000.0]</span><br><span class="line"> [1 40.0 63777.77777777778]</span><br><span class="line"> [0 35.0 58000.0]</span><br><span class="line"> [2 38.77777777777778 52000.0]</span><br><span class="line"> [0 48.0 79000.0]</span><br><span class="line"> [1 50.0 83000.0]</span><br><span class="line"> [0 37.0 67000.0]]</span><br></pre></td></tr></table></figure></p><p>如何将这些编号表示的文本转换成one-hot形式呢?这就需要使用 sklearn.preprocessing 中的 OneHotEncoder 进行编号.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">onehotencoder = OneHotEncoder(categorical_features = [<span class="number">0</span>])</span><br><span class="line">X = onehotencoder.fit_transform(X).toarray()</span><br><span class="line">labelencoder_Y = LabelEncoder()</span><br><span class="line">Y =  labelencoder_Y.fit_transform(Y)</span><br></pre></td></tr></table></figure></p><p>categorical_features 参数指定了我们需要对哪些列进行one-hot编码,这里为0,表示对第一列,然后利用’.fit_transform’ 进行编码.由于对于标签我们只需要其标签类型,因此不需要进行 one-hot 编码.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(X[<span class="number">0</span>])</span><br><span class="line">print(Y)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[1.0e+00 0.0e+00 0.0e+00 4.4e+01 7.2e+04]</span><br><span class="line">[0 1 0 0 1 1 0 1 0 1]</span><br></pre></td></tr></table></figure></p><p>因为第一列总共有三种文本,因此其one-hot向量长度为3,且第一个样本的 index 为0, 故其 one-hot 为 [1, 0, 0],总的就如上所示.</p><h1 id="5-创建训练集和测试集"><a href="#5-创建训练集和测试集" class="headerlink" title="5. 创建训练集和测试集"></a>5. 创建训练集和测试集</h1><p>对于机器学习任务,我们需要将数据集按照比例划分成两个部分: 训练集和测试集.训练接用于训练模型,测试集用于测试我们模型的性能.对于数据的划分,我们使用 sklearn.cross_validation 的 train_test_split 对数据集按比例进行划分.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cross_validation import train_test_split</span><br><span class="line">X_train, X_test, Y_train, Y_test = train_test_split( X , Y , test_size = 0.2, random_state = 0)</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(X.shape, X_train.shape, X_test.shape)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(10, 5) (8, 5) (2, 5)</span><br></pre></td></tr></table></figure></p><h1 id="6-特征缩放"><a href="#6-特征缩放" class="headerlink" title="6. 特征缩放"></a>6. 特征缩放</h1><p>通过我们需要对数据进行标准化处理,这样可以减小噪声的影响.通常将其标准化为:均值为0, 方差为1的数据范围.<br>标准化之前的方差和均值:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(np.mean(X_train), np.std(X_train))</span><br><span class="line">print(np.mean(X_test), np.std(X_test))</span><br></pre></td></tr></table></figure></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">12527.338888888888 25395.495797239782</span><br><span class="line">13708.2 28152.896706378193</span><br></pre></td></tr></table></figure></p><p>进行标准化:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">sc_X = StandardScaler()</span><br><span class="line">X_train = sc_X.fit_transform(X_train)</span><br><span class="line">X_test = sc_X.fit_transform(X_test)</span><br></pre></td></tr></table></figure></p><p>标准化之后的均值和方法:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(np.mean(X_train), np.std(X_train))</span><br><span class="line">print(np.mean(X_test), np.std(X_test))</span><br></pre></td></tr></table></figure></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">7.771561172376095e-17 1.0</span><br><span class="line">0.0 0.6324555320336759</span><br></pre></td></tr></table></figure></p><h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p><a href="https://github.com/RanMaosong/RanMaosong.github.io" target="_blank" rel="noopener">感谢大家的阅读和支持, 欢迎大家上星.</a>.该博客的原始Github项目地址[点击这里]</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;相关链接:&lt;/strong&gt;&lt;br&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/09/11/ML-100-Days-001/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github地址&lt;/a&gt;&lt;
      
    
    </summary>
    
      <category term="机器学习100天" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    
    
      <category term="机器学习100天" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    
  </entry>
  
  <entry>
    <title>序言</title>
    <link href="http://yoursite.com/2018/09/10/ML-100-Days/"/>
    <id>http://yoursite.com/2018/09/10/ML-100-Days/</id>
    <published>2018-09-10T03:16:16.000Z</published>
    <updated>2018-09-13T13:34:39.533Z</updated>
    
    <content type="html"><![CDATA[<p><strong>相关链接:</strong><br><a href="https://ranmaosong.github.io/2018/09/10/ML-100-Days/" target="_blank" rel="noopener">Github地址</a><br><a href="https://www.jianshu.com/p/ee8fc3881c57" target="_blank" rel="noopener">简书地址</a><br><a href="https://blog.csdn.net/u014630987/article/details/82669779" target="_blank" rel="noopener">CSDN地址</a></p><p>该系列博客是基于由<a href="https://github.com/llSourcell" target="_blank" rel="noopener">Siraj Raval</a>提出的<a href="https://github.com/Avik-Jain/100-Days-Of-ML-Code" target="_blank" rel="noopener">100 Days of Machine Learning Coding</a>.而该系列博客的目的是在学习该项目的同事丰富其内容，加入自己查阅的资料和自己的理解。该项目全部基于 Python 语言进行讲解</p><p><strong>内容安排</strong></p><ul><li><a href="https://ranmaosong.github.io/2018/09/11/ML-100-Days-001/" target="_blank" rel="noopener">第1天 数据集预处理</a></li><li><a href="/2018/09/13/ML-100-Days-002/">第2天 简单线性回归</a></li><li><a href="*">第3天 多元线性回归</a></li><li><a href="*">第4天 Logistic回归</a></li><li><a href="*">第5天 Logistic回归</a></li><li><a href="*">第6天 实现 Logistic回归</a></li><li><a href="*">第7天 K 均值</a></li><li><a href="*">第8天 Logistic回归数学原理</a></li><li><a href="*">第9天 支持向量机</a></li><li><a href="*">第10天 实现 K 均值</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;相关链接:&lt;/strong&gt;&lt;br&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/09/10/ML-100-Days/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github地址&lt;/a&gt;&lt;br&gt;&lt;
      
    
    </summary>
    
      <category term="机器学习100天" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    
    
      <category term="机器学习100天" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    
  </entry>
  
  <entry>
    <title>排序算法</title>
    <link href="http://yoursite.com/2018/08/14/Algorithm-sorting/"/>
    <id>http://yoursite.com/2018/08/14/Algorithm-sorting/</id>
    <published>2018-08-14T14:22:27.000Z</published>
    <updated>2018-08-14T14:57:29.883Z</updated>
    
    <content type="html"><![CDATA[<p><strong>排序</strong>就是将一组无序的元素通过某种规则将他们按照某种主键来进行升序或者降序排列，比如，我们考试成绩，按照总分进行降序排列，按照语文成绩进行降序排列，按照数学成绩进行降序排列，此时的<em>总分</em>, <em>语文</em>, <em>数学</em> 就是我们排序的主键,又或者我们打开电脑文件夹,按照名字排序,按照时间排序等,由此可见,排序操作在我们的日常生活中的作用之大,下面我们将讲解一些常见的排序算法,参考书籍为<strong>&lt;&lt;算法 第四版&gt;&gt;</strong>.</p><p><strong>声明</strong>: 本节所有内容均默认以升序排列,同时所有代码遵从以下模板.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Template</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">sort</span><span class="params">(Comparable[] a)</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">less</span><span class="params">(Comparable v, Comparable w)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> v.compareTo(w) &lt; <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">exch</span><span class="params">(Comparable[] a, <span class="keyword">int</span> i, <span class="keyword">int</span> j)</span> </span>&#123;</span><br><span class="line">        Comparable t = a[i];</span><br><span class="line">        a[i] = a[j];</span><br><span class="line">        a[j] = t;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">show</span><span class="params">(Comparable[] a)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; a.length; ++i) &#123;</span><br><span class="line">            System.out.print(a[i] + <span class="string">", "</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isSorted</span><span class="params">(Comparable[] a)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; a.length; ++i) &#123;</span><br><span class="line">            <span class="keyword">if</span> (less(a[i], a[i-<span class="number">1</span>]))</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"><span class="comment">//        String[] a = new String[n];</span></span><br><span class="line">        sort(a);</span><br><span class="line">        <span class="keyword">assert</span> Template.isSorted(a);</span><br><span class="line">        Template.show(a);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="选择排序"><a href="#选择排序" class="headerlink" title="选择排序"></a>选择排序</h1><p><strong>选择排序</strong> 顾名思义就是从乱序的数组中选择最值,其思路如下: 首先,从 N 个数中选择最小的数,放在数组第一个位置,然后从剩下的 N-1 个数中选择最小的数放在数组的第二个位置,迭代此过程直至完成排序.该算法大约进行 $N^2/2$ 次比较和 N 次交换,详细计算如下:</p><p>寻找第一个最小值时,我们需要在余下的 N-1(为什么是 N-1 不是 N?因为数组坐标为0的数是我们目前最小的值,此时我们从剩下的 N-1 个数比较依次找最小值) 个数中进行比较,且交换 1 次,此时进行了 N-1 次比较,寻找第二个最小值时,需要在余下的 N-2 个数中进行比较,此时进行了 N-2 次比较,所以我们总共需要 $(N-1)+(N-2)+\dots +1=N(N-1)/2$ 比较和 N-1 次交换.</p><p>代码如下:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> sort;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> song</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 18-8-14 下午9:38</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Selection</span> <span class="keyword">extends</span> <span class="title">Template</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span>  <span class="title">sort</span><span class="params">(Comparable[] a)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 升序排列</span></span><br><span class="line">        <span class="keyword">int</span> N = a.length;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; ++i) &#123;</span><br><span class="line">            <span class="keyword">int</span> min = i;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = i+<span class="number">1</span>; j &lt; N; ++j) &#123;</span><br><span class="line">                <span class="keyword">if</span> (Template.less(a[j], a[min]))</span><br><span class="line">                    min = j;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            Template.exch(a, i, min);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        String[] a = <span class="keyword">new</span> String[]&#123;<span class="string">"s"</span>, <span class="string">"o"</span>, <span class="string">"r"</span>, <span class="string">"t"</span>, <span class="string">"e"</span>, <span class="string">"x"</span>, <span class="string">"a"</span>, <span class="string">"m"</span>, <span class="string">"p"</span>, <span class="string">"l"</span>, <span class="string">"e"</span>&#125;;</span><br><span class="line">        sort(a);</span><br><span class="line">        <span class="keyword">assert</span> Template.isSorted(a);</span><br><span class="line">        Template.show(a);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;排序&lt;/strong&gt;就是将一组无序的元素通过某种规则将他们按照某种主键来进行升序或者降序排列，比如，我们考试成绩，按照总分进行降序排列，按照语文成绩进行降序排列，按照数学成绩进行降序排列，此时的&lt;em&gt;总分&lt;/em&gt;, &lt;em&gt;语文&lt;/em&gt;, &lt;em&gt;数
      
    
    </summary>
    
      <category term="Algorithm(4th)" scheme="http://yoursite.com/categories/Algorithm-4th/"/>
    
    
      <category term="Algorithm(4th)" scheme="http://yoursite.com/tags/Algorithm-4th/"/>
    
  </entry>
  
  <entry>
    <title>最长回文子串(Longest Palindromic Substring)</title>
    <link href="http://yoursite.com/2018/04/10/LeetCode-001-LongestPalindromicSubstring/"/>
    <id>http://yoursite.com/2018/04/10/LeetCode-001-LongestPalindromicSubstring/</id>
    <published>2018-04-10T13:05:49.000Z</published>
    <updated>2018-10-06T06:27:45.892Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2018/04/10/001-LeetCode-LongestPalindromicSubstring/" target="_blank" rel="noopener">Github地址</a><br><a href="https://www.jianshu.com/p/c0840c4d03be" target="_blank" rel="noopener">简书地址</a><br><a href="https://blog.csdn.net/u014630987/article/details/79948604" target="_blank" rel="noopener">CSDN地址</a></p><p><strong>问题描述</strong></p><p><strong>给定一个字符串 s，找出其中最长的回文子串，假设给定字符串的长度最大维 1000.</strong></p><p>例如:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入: &quot;babad&quot;</span><br><span class="line">输出: &quot;bab&quot;</span><br><span class="line">注意： “aba” 也是正确的解，有多个解返回其中一个即可</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入：&quot;cbbd&quot;</span><br><span class="line">输出：&quot;bb&quot;</span><br></pre></td></tr></table></figure><p>回文串是指一个字符串对称，从最左边和最右边分别往最中间遍历，各个位置的字符都相同。解决这个问题，下面将从四个算法分别进行介绍。</p><p><strong>1、暴力枚举法（不可取）</strong></p><p>暴力枚举法是最简单、最容易想到的方法，其思路是：首先找到字符串 s 的所有子串，然后判断该子串是否是回文字符串，最后返回最长的回文子串。该方法虽然简单明了，但因其计算成本太高，该算法在实际中并<strong>不可取</strong>。</p><p>由于一个长为 n 的字符串，共有 $\frac{n(n+1)}{2}$ 个连续子串，故寻找子串的时间复杂度为 $O(n^2)$, 判断一个字符串是否维回文串的时间复杂度维 $O(n)$,故</p><p><strong>时间复杂度:</strong> $O(n^3)$</p><p><strong>空间复杂度为:</strong> $O(1)$。</p><p>下面是 Java 的实现代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LongestPalindromicSubstring</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">longestPalindrome</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 保存得到的最长回文子串的起始位置</span></span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>, right = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> len = s.length();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;len; ++i) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j=i; j&lt;len; ++j) &#123;</span><br><span class="line">                <span class="comment">// 获取 s 的连续子串</span></span><br><span class="line">                String subStr = s.substring(i, j+<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 判断子串是否是回文字符串</span></span><br><span class="line">                <span class="keyword">if</span> (isPalindrome(subStr)) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (j-i &gt; right-left) &#123;</span><br><span class="line">                        left = i;</span><br><span class="line">                        right = j;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> s.substring(left, right+<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">isPalindrome</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> len = s.length();</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>, right = len-<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (left &lt; right) &#123;</span><br><span class="line">            <span class="keyword">if</span> (s.charAt(left) != s.charAt(right))</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">            ++left;</span><br><span class="line">            --right;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>2、中心扩展法（可取）</strong></p><p>中心扩展法是根据暴力枚举法改进而来，主要是去除了一些不必要的子字符串的判断，<strong>主要思路:</strong>首先从字符串 s 中选择一个字符作为子字符串的中心字符,然后以该字符维中心依次往左右两边扩展,判断该子串的最左边和最右边的字符是否相同,相同则继续向两边扩展,不相同则停止扩展,该子串则是以该字符为中心的最长回文子串,这样就减少了很多在暴力枚举方法中不必要的字符的判断.</p><p>和暴力枚举方法比较,以”abacdfgdcaba”为例,假设我们以第一个字符 ‘c’ 为中心,中心扩展首先比较”acd”,由于 ‘a’ 和 ‘d’不相同,则停止扩展,二暴力枚举还需比较 “bacdf” 和 “abacdfg”.该算法在扩展时需要同时考虑子串是奇数和偶数的情况.</p><p>由于需要依次迭代每个字符串中心,因此该迭代需要 $O(n)$ 时间复杂度,同时从中心向两边扩展的复杂度维 $O(n)$,因此:</p><p><strong>时间复杂度:</strong> $O(n^2)$</p><p><strong>空间复杂度为:</strong> $O(1)$。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LongestPalindromicSubstring</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span>  <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"abacdfgdcaba"</span>));</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"cbbd"</span>));</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"babad"</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">longestPalindrome</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 保存获得的最大回文子串</span></span><br><span class="line">        String maxStr = <span class="string">""</span>;</span><br><span class="line">        <span class="keyword">int</span> len = s.length();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;len; ++i) &#123;</span><br><span class="line">            String subStr1 = isPalindrome(s, i, i);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (subStr1.length() &gt; maxStr.length()) &#123;</span><br><span class="line">                maxStr = subStr1;</span><br><span class="line">            &#125;</span><br><span class="line">            String subStr2 = isPalindrome(s, i, i+<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (subStr2.length() &gt; maxStr.length()) &#123;</span><br><span class="line">                maxStr = subStr2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> maxStr;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> String <span class="title">isPalindrome</span><span class="params">(String s, <span class="keyword">int</span> i, <span class="keyword">int</span> j)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// i 表示中心扩展的左边字符</span></span><br><span class="line">        <span class="comment">// j 表示中心扩展的右边字符</span></span><br><span class="line">        <span class="keyword">int</span> len = s.length();</span><br><span class="line">        <span class="keyword">while</span> (i &gt;= <span class="number">0</span> &amp;&amp; j &lt; len &amp;&amp; s.charAt(i) == s.charAt(j)) &#123;</span><br><span class="line">            --i;</span><br><span class="line">            ++j;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span>  s.substring(i+<span class="number">1</span>, j);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>3、动态规划（可取）</strong></p><p>回文字符串的子串也是回文字符串,我们可以将最长回文子串分解为一些列子问题,使用动态规划.设 f 为状态表,f(i,j)表示字符区间 [i, j](包括j)是否为回文字符串<br>,f(i, j)=false 表示子串 [i, j] 不是回文字符串,f(i, j)=true 表示子串 [i, j] 为回文字符串.当我们判断了字符 [i], [j] 相同时,只需判断 f(i+1, j-1) 是否维 true 即可.</p><p>状态表满足以下关系:</p><script type="math/tex; mode=display">f(i,j)=\begin{cases}true,\quad i=j \\s[i]==s[j], \quad i= j-1 \\s[i] = s[j]\quad and\quad f(i+1, j-1), \quad i< j-1\end{cases}</script><p>由于状态表 f 是一个 n*n 的方阵,且是一个对称方阵,故我们只需判断状态表 f 的右上角的内容,因此:</p><p><strong>时间复杂度:</strong> $O(n^2)$<br><strong>空间复杂度:</strong> $O(n^2)$</p><p>实现代码:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">test</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span>  <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"abacdfgdcaba"</span>));</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"cbbd"</span>));</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"babad"</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">longestPalindrome</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n = s.length();</span><br><span class="line">        <span class="keyword">boolean</span>[][] f = <span class="keyword">new</span> <span class="keyword">boolean</span>[n][n];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> left =<span class="number">0</span>, right=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;n; ++j) &#123;</span><br><span class="line">            f[j][j] = <span class="keyword">true</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;j; ++i) &#123;</span><br><span class="line">                <span class="keyword">if</span> (s.charAt(i) == s.charAt(j) &amp;&amp; (i == j-<span class="number">1</span> || f[i+<span class="number">1</span>][j-<span class="number">1</span>])) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (j-i &gt; right - left) &#123;</span><br><span class="line">                        left = i;</span><br><span class="line">                        right = j;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    f[i][j] = <span class="keyword">true</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> s.substring(left, right+<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>该代码首先将状态表全部初始化为 false, 然后按照从上到下,从左到右的顺序依次判断状态表的值.<br><strong>4、Manacher 算法（马拉车算法）（可取）</strong></p><p>Manacher 算法是一种经典的求取最长回文子串的方法,其基本原理是使用已知回文字符串的左半部分来推导以右半部分的字符为中心的回文字符.</p><p>我们使用 p[i] 表示以第 i 个字符为中心的最长回文半径.可以利用已知p[0],p[1]……p[i-1]的值,来计算 p[i] 的值.我们定义 maxRight 是当前计算 i 位置时所有回文子串所能达到的最右端的位置,且该回文串的中心位置为 k,此时有如下关系: maxRight = k + p[k],此时有两种情况:</p><p><img src="/images/longestPalindromicSubstring.png" alt="pictures001"></p><p><strong>第一种情况</strong>:i &gt; maxRight,此时初始化p[i] = 1, 然后判断s[i+p[i]] == s[i-p[i]],若不相等则停止,若相等,则++p[i]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">if (i &gt; maxRight) &#123;</span><br><span class="line">    p[i] = 1;</span><br><span class="line">    while(s[i+p[i]] == s[i-p[i]]) &#123;</span><br><span class="line">        ++p[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>第二种情况</strong>:i &lt;= maxRight,此时不在给 p[i] 赋值维为1,由回文串的对称性可得, 2k-i 是 i 关于 k 的对称点.此时由两种情况:</p><ol><li>以 2k-i 为中心的回文串的半径(如图蓝色箭头)大于等于 $maxRight - i$(空心箭头),由对称性可知,已知紫色箭头 5 和 6 关于 k 对称,且 2k-i 和 i 关于 k 对称,所以空心箭头 1 和 4 对称,2 和 3 对称,又箭头 7 和 8 对称,且箭头 1 和 2 分别是 7 和 8 的一部分,所以空心箭头 1 和 2 对称,故空心箭头 3 和 4 对称.所以p[i]的对称半径至少为 maxRight - i.所以首先 p[i]=maxRight - i,然后在依次往两边扩展判断是否对称.</li></ol><p><img src="/images/LongestPalindroomicSubstring1.jpg" alt="picture2"></p><ol><li>以 2k-i 为中心的回文半径小于 maxRight-i,根据和上面类似的推导,可以得知 p[i] = p[2k-i],且不在扩展.</li></ol><p><img src="/images/LongestPalindroomicSubstring2.png" alt="picture3"></p><p>复杂度分析:</p><ul><li><strong>时间复杂度:</strong>$O(n)$</li><li><strong>空间复杂度:</strong> $O(n)$ </li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">test</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span>  <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"abacdfgdcaba"</span>));</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"cbbd"</span>));</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"babad"</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">longestPalindrome</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        StringBuilder temp = <span class="keyword">new</span> StringBuilder(<span class="string">"#"</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; s.length(); ++i) &#123;</span><br><span class="line">            temp.append(s.charAt(i));</span><br><span class="line">            temp.append(<span class="string">"#"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        String str = temp.toString();</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        * maxCenter: 保存当前能延伸到最右端的回文字符串的中心位置</span></span><br><span class="line"><span class="comment">        * maxId: 保存当前最长回文子串的中心位置</span></span><br><span class="line"><span class="comment">        * p: 保存以该位置的字符维中心位置的最长回文字符的右边长度</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="keyword">int</span> maxCenter=<span class="number">0</span>, maxId=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> n = str.length();</span><br><span class="line">        <span class="keyword">int</span>[] p = <span class="keyword">new</span> <span class="keyword">int</span>[n];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;n; ++i) &#123;</span><br><span class="line">            <span class="keyword">int</span> syncCenter = <span class="number">2</span> * maxCenter - i;</span><br><span class="line">            p[i] = (i&lt;maxCenter + p[maxCenter) ? Math.min(p[syncCenter], maxCenter + p[maxCenter] - i) : <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span>(i-p[i] &gt;=<span class="number">0</span> &amp;&amp; i+p[i]&lt;n &amp;&amp; str.charAt(i-p[i]) == str.charAt(i+p[i])) &#123;</span><br><span class="line">                ++p[i];</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (i + p[i] &gt;= p[maxCenter] + maxCenter) &#123;</span><br><span class="line">                maxCenter = i;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (p[i] &gt; p[maxId]) &#123;</span><br><span class="line">                maxId = i;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> s.substring((maxId - p[maxId])/<span class="number">2</span>, (maxId + p[maxId]) / <span class="number">2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/04/10/001-LeetCode-LongestPalindromicSubstring/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github地址&lt;/a&gt;&lt;b
      
    
    </summary>
    
      <category term="Weekly Algorithm" scheme="http://yoursite.com/categories/Weekly-Algorithm/"/>
    
    
      <category term="Weekly Algorithm" scheme="http://yoursite.com/tags/Weekly-Algorithm/"/>
    
  </entry>
  
</feed>
