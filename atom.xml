<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>技术闲谈</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-10-07T08:11:02.871Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>茂松</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>169. Majority Element(求众数)</title>
    <link href="http://yoursite.com/2018/10/06/LeetCode-169-MajorityElement/"/>
    <id>http://yoursite.com/2018/10/06/LeetCode-169-MajorityElement/</id>
    <published>2018-10-06T14:03:52.000Z</published>
    <updated>2018-10-07T08:11:02.871Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2018/10/06/LeetCode-136-SingleNumber/" target="_blank" rel="noopener">GIthub</a><br><a href="https://www.jianshu.com/p/cf75842c46c0" target="_blank" rel="noopener">简书</a><br><a href="https://blog.csdn.net/u014630987/article/details/82958884" target="_blank" rel="noopener">CSDN</a></p><h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><p>给定一个大小为 n 的数组，找到其中的众数。众数是指在数组中出现次数大于 ⌊ n/2 ⌋ 的元素。<br>你可以假设数组是非空的，并且给定的数组总是存在众数。<br><strong>示例1：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入: [3,2,3]</span><br><span class="line">输出: 3</span><br></pre></td></tr></table></figure></p><p><strong>示例2：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入: [2,2,1,1,1,2,2]</span><br><span class="line">输出: 2</span><br></pre></td></tr></table></figure></p><h1 id="方法一-暴力破解"><a href="#方法一-暴力破解" class="headerlink" title="方法一: 暴力破解"></a>方法一: 暴力破解</h1><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>最简单最直接的方法是统计每个数出现的次数，如果它出现的次数 大于 $\lfloor n/2 \rfloor$, 则这个数就为这个数组的众数。因此实现此算法，需要两个嵌套的 for 循环，外层循环遍历数组来确定当前值，内层循环同样遍历数组，它则用来统计当前值出现的次数。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LeetCode_169</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">majorityElement</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * O(n^2)</span></span><br><span class="line"><span class="comment">         *  Time Limit Exceeded</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">int</span> max = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.length; ++i) &#123;</span><br><span class="line">            <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; nums.length; ++j) &#123;</span><br><span class="line">                <span class="keyword">if</span> (nums[i] == nums[j])</span><br><span class="line">                    ++count;</span><br><span class="line">            &#125;</span><br><span class="line">            max = Math.max(max, count);</span><br><span class="line">            <span class="keyword">if</span> (max &gt; nums.length / <span class="number">2</span>)</span><br><span class="line">                <span class="keyword">return</span> nums[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h2><ul><li><p>时间复杂度： $O(n^2)$</p><p>由于该方法包含两个嵌套的 for 循环，每个循环迭代n次，因此该算法的时间复杂度为 $O(n^2)$</p></li><li><p>空间复杂度： $O(1)$</p></li></ul><h1 id="方法二：-HashMap"><a href="#方法二：-HashMap" class="headerlink" title="方法二： HashMap"></a>方法二： HashMap</h1><h2 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h2><p>在统计元素出现次数的时候， 我们可以使用一个 HashMap 来保存当前已出现的元素出现的次数，这样可以避免重复的统计，从而在一个 for 循环里完成任务。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Maosong Ran</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2018/10/06</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@email</span> maosongran@gmail.com</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LeetCode_169</span> </span>&#123; </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">majorityElement</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * O(n) O(n)</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        HashMap&lt;Integer, Integer&gt; count = <span class="keyword">new</span> HashMap&lt;Integer, Integer&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i &lt; nums.length; ++i) &#123;</span><br><span class="line">            Integer num = count.get(nums[i]);</span><br><span class="line">            <span class="keyword">if</span> (num == <span class="keyword">null</span>)</span><br><span class="line">                num = <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                ++num;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (num &gt; nums.length / <span class="number">2</span>)</span><br><span class="line">                <span class="keyword">return</span> nums[i];</span><br><span class="line">            count.put(nums[i], num);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nums[<span class="number">0</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="复杂度分析-1"><a href="#复杂度分析-1" class="headerlink" title="复杂度分析"></a>复杂度分析</h2><ul><li><p>时间复杂度： $O(n)$</p><p>由于我们只需要遍历一次数组，即可完成统计任务，因此，时间复杂度为$O(n)$</p></li><li><p>空间复杂度： $O(n)$<br>由于数组总存在众数，而众数的条件是其出现次数大于 $\lfloor n/2 \rfloor$,因此最坏情况，HashMap包含 $n - (\lfloor n/2 \rfloor + 1)$个元素，最好情况包含 1 个元素，因此空间复杂度为O(n)</p></li></ul><h1 id="方法三：-排序"><a href="#方法三：-排序" class="headerlink" title="方法三： 排序"></a>方法三： 排序</h1><p>由于众数的是其出现次数大于 $\lfloor n/2 \rfloor$ 的值，因此无论该值的大小是多少，该值总会出现在中心位置：对于数组长度为偶数时，为最中间两个数，为基数时，为最中间一个数。<br>！<a href="/images/leetcode169.png">Sorting</a></p><p>上图中，数组下面的线表示当众数出现在排序数组的最左侧，数组上面的线表示当总数出现在排序数组的最右侧时，测试是众数出现的两个极端情况，其余情况在这两种情况之间，因此总数总是出现在排序数组的最中心位置。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LeetCode_169</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">majorityElement</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        Arrays.sort(nums);</span><br><span class="line">        <span class="keyword">return</span> nums[nums.length/<span class="number">2</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="复杂度分析-2"><a href="#复杂度分析-2" class="headerlink" title="复杂度分析"></a>复杂度分析</h2><ul><li><p>时间复杂度： $O(nlog n)$</p><p>在Java和Python中，数组排序的时间复杂度为$O(nlog n)$</p></li><li><p>空间复杂度： $O(n)$ 或 $O(1)$</p><p>如果允许在原地进行排序时，我们不需要额外的空间，因此，空间复杂度为$O(1)$,如果不允许，则需要额外的等大的数组来存放该有序数组，因此空间复杂度为 $O(n)$</p></li></ul><h1 id="方法三：-分治法"><a href="#方法三：-分治法" class="headerlink" title="方法三： 分治法"></a>方法三： 分治法</h1><h2 id="算法-2"><a href="#算法-2" class="headerlink" title="算法"></a>算法</h2><p>分治法是算法中经常遇到的一种求解问题的方法，它将大问题小化，复杂问题简单化，因此可以很容易得出问题的解。</p><p>在本题中，我们将数组递归地从中间将大数组分成左右两个小数组，然后求左右两个小数组的的众数，如果这两个众数相等，则这个数也是大数组的众数，如果两个数不相等，则这二者之一必有一个是大数组的众数，为什么呢？</p><p>若假设这两个数之一不是大数组的众数，由于他们是小数组的众数，因此，他们出现次数各占小数组的一半以上，因此这两个数在大数组中的出现次数必大于 $\lfloor n/2 \rfloor$,因此剩下位置的值即便是相同，他们出现的次数也不大于 $\lfloor n/2 \rfloor$</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LeetCode_169</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">majorityElement</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> divideAndConquer(nums, <span class="number">0</span>, nums.length-<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">divideAndConquer</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (left == right)</span><br><span class="line">            <span class="keyword">return</span> nums[left];</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">int</span> mid = (left + right)/<span class="number">2</span>;</span><br><span class="line">            <span class="keyword">int</span> leftMajority = divideAndConquer(nums, left, mid);</span><br><span class="line">            <span class="keyword">int</span> rightMajority = divideAndConquer(nums, mid + <span class="number">1</span>, right);</span><br><span class="line"><span class="comment">//            System.out.println(leftMajority + "-&gt;" + rightMajority);</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (leftMajority == rightMajority)</span><br><span class="line">                <span class="keyword">return</span> leftMajority;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">int</span> leftCount = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">int</span> rightCount = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=left; i &lt;= right; ++i) &#123;</span><br><span class="line">                <span class="keyword">if</span> (nums[i] == leftMajority)</span><br><span class="line">                    ++leftCount;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (nums[i] == rightMajority) &#123;</span><br><span class="line">                    ++rightCount;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"><span class="comment">//            System.out.println(leftMajority + ":" + leftCount + ", " + rightMajority + ": " + rightCount);</span></span><br><span class="line">            <span class="keyword">return</span> leftCount &gt; rightCount ? leftMajority : rightMajority;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="复杂度分析-3"><a href="#复杂度分析-3" class="headerlink" title="复杂度分析"></a>复杂度分析</h2><ul><li><p>时间复杂度： $O(nlog n)$</p></li><li><p>空间复杂度： $O(ll0g n)$</p></li></ul><h1 id="方法五：-摩尔投票法"><a href="#方法五：-摩尔投票法" class="headerlink" title="方法五： 摩尔投票法"></a>方法五： 摩尔投票法</h1><h2 id="算法-3"><a href="#算法-3" class="headerlink" title="算法"></a>算法</h2><p>大致思路，首先有一个统计当前投票数的变量 count， 当 count == 0 时，我们以当前变量作为候选众数，然后从当前变量位置开始，若值等于候选众数的值，则count加1，若不相等，则减一，依次遍历玩数组，当遍历完数组，若 count == 0，则该数组不存在众数，不等于0时，此时的候选众数即位真正的众数。</p><p>原理： 由于众数票数大于 $\lfloor n/2 \rfloor$,因此即便它的票数减去其他所有的票数，它的票数也大于零。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LeetCode_169</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">majorityElement</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>  count = <span class="number">0</span>;</span><br><span class="line">        Integer candiate = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> num : nums) &#123;</span><br><span class="line">            <span class="keyword">if</span> (count == <span class="number">0</span>) &#123;</span><br><span class="line">                candiate = num;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            count += (candiate == num) ? <span class="number">1</span> : -<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> candiate;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="复杂度分析-4"><a href="#复杂度分析-4" class="headerlink" title="复杂度分析"></a>复杂度分析</h2><ul><li><p>时间复杂度： $O(n)$</p></li><li><p>空间复杂度： $O(1)$</p></li></ul><h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p><a href="https://github.com/RanMaosong/RanMaosong.github.io" target="_blank" rel="noopener">感谢大家的阅读和支持, 欢迎大家上星.</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/10/06/LeetCode-136-SingleNumber/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GIthub&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https:
      
    
    </summary>
    
      <category term="Weekly Algorithm" scheme="http://yoursite.com/categories/Weekly-Algorithm/"/>
    
    
      <category term="Weekly Algorithm" scheme="http://yoursite.com/tags/Weekly-Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode 136.Single Number(只出现一次的数字)</title>
    <link href="http://yoursite.com/2018/10/06/LeetCode-136-SingleNumber/"/>
    <id>http://yoursite.com/2018/10/06/LeetCode-136-SingleNumber/</id>
    <published>2018-10-06T05:49:39.000Z</published>
    <updated>2018-10-06T07:27:45.223Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2018/10/06/LeetCode-136-SingleNumber/" target="_blank" rel="noopener">GitHub</a><br><a href="https://www.jianshu.com/p/df7d256e2bea" target="_blank" rel="noopener">简书</a><br><a href="https://blog.csdn.net/u014630987/article/details/82951316" target="_blank" rel="noopener">CSDN</a></p><h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><p>给定一个非空整数数组，除了某个元素只出现一次以外，其余每个元素均出现两次。找出那个只出现了一次的元素。</p><p><strong>说明</strong><br>你的算法应该具有线性时间复杂度。 你可以不使用额外空间来实现吗？</p><p><strong>示例1</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入: [2,2,1]</span><br><span class="line">输出: 1</span><br></pre></td></tr></table></figure></p><p><strong>示例2</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入: [4,1,2,1,2]</span><br><span class="line">输出: 4</span><br></pre></td></tr></table></figure></p><p><strong>难度系数:</strong> 简单</p><h1 id="解题思路一"><a href="#解题思路一" class="headerlink" title="解题思路一"></a>解题思路一</h1><p>常规方法是,遍历数组,然后统计每个值出现的次数,最后在选择出现次数为1的那个值.该算法的时间复杂度为O(N),首先是统计数组,此时要遍历整个数组,然后是要遍历我们的统计数组,此时有事一个O(N),由于我们使用了一个统计数组来保存每个值出现的次数,此时需要的空间复杂度为O(n),因此不符合要求.</p><h1 id="解题思路二"><a href="#解题思路二" class="headerlink" title="解题思路二"></a>解题思路二</h1><p>为了解决不需要额外的空间这个要求,我们可以使用位操作中的异或规则来进行处理.异或运算法则如下</p><ol><li>a $\oplus$ a = 0, a $\oplus$ 0=a</li><li>a $\oplus$ b = b $\oplus$ a</li><li>a $\oplus$ b $\oplus$ c = a $\oplus$ (b $\oplus$ c) = a $\oplus$ (c $\oplus$ b) = (a $\oplus$ b) $\oplus$ c</li></ol><p>其中,第一条规则说明,当某个数出现两次时,通过 $\oplus$ 变为0,出现一次时依然保持原来的数,第二、三条的交换律和分配律说明通过多次 $\oplus$ 操作最终解决本题。</p><p><strong>注意</strong>: 本体题目中指出除了某个元素值出现一次外其余的均出现两次，根据法则一可以看出本算法只适合除了某个元素出现一次外，其余元素出现偶数次的情况。</p><p>比如在示例二中的 $\oplus$ 操作：</p><script type="math/tex; mode=display">\begin{aligned}    4 \oplus 1 \oplus 2 \oplus 1 \oplus 2 &=  4 \oplus 1 \oplus 1 \oplus 2 \oplus 2 \\    &= 4 \oplus (1 \oplus 1) \oplus (2 \oplus 2)\\    &=4\end{aligned}</script><h1 id="Java实现代码"><a href="#Java实现代码" class="headerlink" title="Java实现代码"></a>Java实现代码</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Maosong Ran</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2018/10/06</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@email</span> maosongran@gmail.com</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LeetCode_136</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">singleNumber</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> len = nums.length;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i &lt; len; ++i)</span><br><span class="line">            result ^= nums[i];</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        LeetCode_136 leetCode = <span class="keyword">new</span> LeetCode_136();</span><br><span class="line">        System.out.println(leetCode.singleNumber(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>&#125;));</span><br><span class="line">        System.out.println(leetCode.singleNumber(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>&#125;));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>输出</strong>:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">4</span><br></pre></td></tr></table></figure></p><h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p><a href="https://github.com/RanMaosong/RanMaosong.github.io" target="_blank" rel="noopener">感谢大家的阅读和支持, 欢迎大家上星.</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/10/06/LeetCode-136-SingleNumber/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https:
      
    
    </summary>
    
      <category term="Weekly Algorithm" scheme="http://yoursite.com/categories/Weekly-Algorithm/"/>
    
    
      <category term="Weekly Algorithm" scheme="http://yoursite.com/tags/Weekly-Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>数据结构第二次作业</title>
    <link href="http://yoursite.com/2018/09/17/ta-work-02/"/>
    <id>http://yoursite.com/2018/09/17/ta-work-02/</id>
    <published>2018-09-17T13:09:48.000Z</published>
    <updated>2018-09-22T04:25:57.776Z</updated>
    
    <content type="html"><![CDATA[<h1 id="实现在单向链表中-返回某个节点的前驱"><a href="#实现在单向链表中-返回某个节点的前驱" class="headerlink" title="实现在单向链表中,返回某个节点的前驱."></a>实现在单向链表中,返回某个节点的前驱.</h1><h2 id="原理说明"><a href="#原理说明" class="headerlink" title="原理说明"></a>原理说明</h2><p>单向链表的特点是链表只有一个方向,即只有从前驱结点指向后继结点的指针,而没有后继节点指向前驱结点的指针,结构图大致如下:<br><img src="https://upload-images.jianshu.io/upload_images/5208761-03db3a5a57fd70ed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="linklist1.png"><br>从图可以看出,如果我们要访问第三个结点,我们只能从头指针依次便利每个后继结点,直至访问到第三个结点,因此在单向链表中,我们查找某个元素的时间复杂度 $O(n)$.</p><p>基于链表的单向特点,因此我们必须从头开始遍历才能找到某个节点的前驱.以上图为例进行讲解,假如我们要寻找第三个结点的前驱,我们如何实现呢?</p><ol><li>首先我们需要申明两个临时变量pre 和 cur,分别指向Head 头结点和Head-&gt;next(为null或为第一个结点),这样pre和cur就构成一个相关的对,pre表示cur的前驱,当cur到达第三个结点时,此时pre就是我们要找的前驱.<br><img src="https://upload-images.jianshu.io/upload_images/5208761-bb412a44ed651d75.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="linklist2.png"></li><li>再循环中,我们依次把pre和cur向前同时推进,直至cur到达事先定义的结点.<ul><li>在上图中,cur=1,不是我们要找的结点3,因此pre和cur向前移动,即pre=cur, cur=next;<br><img src="https://upload-images.jianshu.io/upload_images/5208761-ba2086541433a47a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="linklist3.png"></li></ul></li></ol><ul><li>此时,cur=2,不符合我们的条件,我们继续执行上面的操作;<br><img src="https://upload-images.jianshu.io/upload_images/5208761-54cc3f96b5132267.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="linklist4.png"><br>现在cur=3,满足我们的条件,此时pre就是他的前驱,返回该节点即可.<h2 id="下面是实现代码-仅供参考"><a href="#下面是实现代码-仅供参考" class="headerlink" title="下面是实现代码,仅供参考"></a>下面是实现代码,仅供参考</h2><h3 id="c语言"><a href="#c语言" class="headerlink" title="　c语言"></a>　<strong>c语言</strong></h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;malloc.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 数据类型</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">int</span> ElementType;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 链表数据结构</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    ElementType data;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span>* <span class="title">next</span>;</span></span><br><span class="line">&#125;Node, *Linklist;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建链表</span></span><br><span class="line"><span class="function">Linklist <span class="title">createList</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> len;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"请输入链表长度:"</span>);</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;len);</span><br><span class="line">    Linklist head = (Linklist)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(Node));</span><br><span class="line">    Linklist tail = head;</span><br><span class="line">    tail-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;len; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> val;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"第%d个节点内容:"</span>, (i+<span class="number">1</span>));</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;val);</span><br><span class="line">        Linklist node = (Linklist)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(Node));</span><br><span class="line">        node-&gt;data = val;</span><br><span class="line">        node-&gt;next = tail-&gt;next;</span><br><span class="line">        tail-&gt;next = node;</span><br><span class="line">        tail = node;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> head;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 查找给定节点的前驱</span></span><br><span class="line"><span class="function">Node* <span class="title">preNodeOf</span><span class="params">(Linklist <span class="built_in">list</span>, Node* node)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Node* cur = <span class="built_in">list</span>-&gt;next;</span><br><span class="line">    Node* pre = <span class="built_in">list</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> (cur != <span class="literal">NULL</span> &amp;&amp; cur-&gt;data != node-&gt;data)</span><br><span class="line">    &#123;</span><br><span class="line">        </span><br><span class="line">        pre = cur;</span><br><span class="line">        cur = cur-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (pre != <span class="built_in">list</span>)</span><br><span class="line">        <span class="keyword">return</span> pre;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 根据索引查找指定节点,index 从1开始</span></span><br><span class="line"><span class="function">Node* <span class="title">getNode</span><span class="params">(Linklist <span class="built_in">list</span>, <span class="keyword">int</span> index)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Node* node = <span class="built_in">list</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;index; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        node = node-&gt;next;</span><br><span class="line">        <span class="keyword">if</span> (node == <span class="literal">NULL</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> node;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 显示链表内容</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">showLinklist</span><span class="params">(Linklist <span class="built_in">list</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Node* node = <span class="built_in">list</span>-&gt;next;</span><br><span class="line">    <span class="keyword">while</span> (node != <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d  "</span>, node-&gt;data);</span><br><span class="line">        node = node-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">length</span><span class="params">(Linklist <span class="built_in">list</span>)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> len = <span class="number">0</span>;</span><br><span class="line">    Node* node = <span class="built_in">list</span>-&gt;next;</span><br><span class="line">    <span class="keyword">while</span> (node != <span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        ++len;</span><br><span class="line">        node = node-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> len;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Linklist <span class="built_in">list</span> = createList();</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"链表内容:"</span>);</span><br><span class="line">    showLinklist(<span class="built_in">list</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> len = length(<span class="built_in">list</span>);</span><br><span class="line">    <span class="keyword">int</span> index;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"\n输入需要查找前驱的节点的索引(0&lt; n &lt;=%d):"</span>, len);</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;index);</span><br><span class="line">    Node* node = getNode(<span class="built_in">list</span>, index);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"第%d个节点的内容:%d\n"</span>, index, node-&gt;data);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    node = preNodeOf(<span class="built_in">list</span>, node);</span><br><span class="line">    <span class="keyword">if</span> (node != <span class="literal">NULL</span>)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, node-&gt;data);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"该节点为第一个节点,无前驱节点!"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h3 id="C-代码-这里使用了模板类"><a href="#C-代码-这里使用了模板类" class="headerlink" title="　C++代码:这里使用了模板类"></a>　<strong>C++代码</strong>:这里使用了模板类</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 节点类, 使用了模板类,方便数据类型</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">Node</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    T data;</span><br><span class="line">    Node&lt;T&gt;* next;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    Node()&#123;&#125;</span><br><span class="line">    Node(T v, Node&lt;T&gt;* node)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">this</span>-&gt;data = v;</span><br><span class="line">        <span class="keyword">this</span>-&gt;next = node;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">LinkList</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    Node&lt;T&gt;* head;</span><br><span class="line">    Node&lt;T&gt;* tail;</span><br><span class="line">    <span class="keyword">int</span> count;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    LinkList();</span><br><span class="line">    ~LinkList();</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">size</span><span class="params">()</span></span>;</span><br><span class="line">    Node&lt;T&gt;* getNode(<span class="keyword">int</span>);</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">append</span><span class="params">(T data)</span></span>;</span><br><span class="line">    Node&lt;T&gt;* preNodeOf(Node&lt;T&gt;*);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 初始化链表</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">LinkList</span>&lt;T&gt;:</span>:LinkList(): count(<span class="number">0</span>)</span><br><span class="line">&#123;</span><br><span class="line">    head = <span class="keyword">new</span> Node&lt;T&gt;;</span><br><span class="line">    head-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">    tail = head;</span><br><span class="line">    <span class="keyword">int</span> length;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"请输入链表初始长度:"</span>;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; length;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; length; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        T data;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"第"</span> &lt;&lt; (i+<span class="number">1</span>) &lt;&lt; <span class="string">"个节点的内容:"</span>;</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; data;</span><br><span class="line">        append(data);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">LinkList</span>&lt;T&gt;:</span>:~LinkList()</span><br><span class="line">&#123;</span><br><span class="line">    Node&lt;T&gt;* node = head-&gt;next;</span><br><span class="line">    Node&lt;T&gt;* tmp;</span><br><span class="line">    <span class="keyword">while</span> (node != <span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        tmp = node;</span><br><span class="line">        node = node-&gt;next;</span><br><span class="line">        <span class="keyword">delete</span> tmp;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">delete</span> head;</span><br><span class="line">    head = <span class="literal">NULL</span>;</span><br><span class="line">    tail = <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 返回链表的长度</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">int</span> <span class="title">LinkList</span>&lt;T&gt;:</span>:size()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> count;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">Node</span>&lt;T&gt;* <span class="title">LinkList</span>&lt;T&gt;:</span>:getNode(<span class="keyword">int</span> index)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (index &gt; count || index &lt;= <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt; <span class="string">"Index Error!"</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    Node&lt;T&gt;* node = head;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;index &amp;&amp; node-&gt;next; ++i) </span><br><span class="line">    &#123;</span><br><span class="line">        node = node-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> node;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">LinkList</span>&lt;T&gt;:</span>:print()</span><br><span class="line">&#123;</span><br><span class="line">    Node&lt;T&gt;* node = head-&gt;next;</span><br><span class="line">    <span class="keyword">while</span> (node)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; node-&gt;data &lt;&lt; <span class="string">"  "</span>;</span><br><span class="line">        node = node-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">LinkList</span>&lt;T&gt;:</span>:append(T data)</span><br><span class="line">&#123;</span><br><span class="line">    Node&lt;T&gt;* node = <span class="keyword">new</span> Node&lt;T&gt;();</span><br><span class="line">    node-&gt;data = data;</span><br><span class="line">    node-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">    tail-&gt;next = node;</span><br><span class="line">    tail = node;</span><br><span class="line">    ++count;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">Node</span>&lt;T&gt;* <span class="title">LinkList</span>&lt;T&gt;:</span>:preNodeOf(Node&lt;T&gt;* node)</span><br><span class="line">&#123;</span><br><span class="line">    Node&lt;T&gt;* pre = head;</span><br><span class="line">    Node&lt;T&gt;* cur = head-&gt;next;</span><br><span class="line">    <span class="keyword">while</span> (cur-&gt;data != node-&gt;data)</span><br><span class="line">    &#123;</span><br><span class="line">        pre = cur;</span><br><span class="line">        cur = cur-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (pre == head) </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> pre;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    LinkList&lt;<span class="keyword">int</span>&gt; <span class="built_in">list</span> = LinkList&lt;<span class="keyword">int</span>&gt;();</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"链表内容:"</span>;</span><br><span class="line">    <span class="built_in">list</span>.print();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> size = <span class="built_in">list</span>.size();</span><br><span class="line">    <span class="keyword">int</span> index;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"输入需要查找前驱的节点的索引(1 &lt;= n &lt;="</span> &lt;&lt; size &lt;&lt; <span class="string">"):"</span>;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; index;</span><br><span class="line">    Node&lt;<span class="keyword">int</span>&gt;* node = <span class="built_in">list</span>.getNode(index);</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"第"</span> &lt;&lt; index &lt;&lt; <span class="string">"个节点的内容:"</span> &lt;&lt; node-&gt;data &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    Node&lt;<span class="keyword">int</span>&gt;* pre = <span class="built_in">list</span>.preNodeOf(node);</span><br><span class="line">    <span class="keyword">if</span> (pre)</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"第"</span> &lt;&lt; index &lt;&lt; <span class="string">"个节点的前驱:"</span> &lt;&lt; pre-&gt;data &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"该节点为第一个节点,无前驱"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面的代码仅供学习使用,课程要求为C/C++变成语言.</p><h3 id="Python3"><a href="#Python3" class="headerlink" title="　Python3"></a>　<strong>Python3</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data, next=None)</span>:</span></span><br><span class="line">        self.data = data</span><br><span class="line">        self.next = next</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinkList</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self._head = Node(<span class="number">0</span>)</span><br><span class="line">        self._tail = self._head</span><br><span class="line">        self._count = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">append</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        node = Node(data, <span class="keyword">None</span>)</span><br><span class="line">        self._tail.next = node</span><br><span class="line">        self._tail = node</span><br><span class="line">        self._count += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._count</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">show</span><span class="params">(self)</span>:</span></span><br><span class="line">        node = self._head.next</span><br><span class="line">        <span class="keyword">while</span> (node):</span><br><span class="line">            print(node.data, end=<span class="string">"  "</span>)</span><br><span class="line">            node = node.next</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_n_elems</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            data = int(input(<span class="string">"请输入第%d个节点的内容:"</span> % (i+<span class="number">1</span>)))</span><br><span class="line">            self.append(data)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pre_node_of</span><span class="params">(self, node)</span>:</span></span><br><span class="line">        pre = self._head</span><br><span class="line">        cur = pre.next</span><br><span class="line">        <span class="keyword">while</span>(cur.data != node.data):</span><br><span class="line">            pre = cur</span><br><span class="line">            cur = cur.next</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (pre == self._head):</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> pre</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_node</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> (<span class="number">1</span> &lt;= index &lt;= self.size() ):</span><br><span class="line">            <span class="keyword">raise</span> Exception(<span class="string">"Index Error!"</span>)</span><br><span class="line">        node = self._head</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(index):</span><br><span class="line">            node = node.next</span><br><span class="line">        <span class="keyword">return</span> node</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    linklist = LinkList()</span><br><span class="line">    length = int(input(<span class="string">"请输入链表长度:"</span>))</span><br><span class="line">    linklist.add_n_elems(length)</span><br><span class="line">    print(<span class="string">"链表内容:"</span>, end=<span class="string">""</span>)</span><br><span class="line">    linklist.show()</span><br><span class="line"></span><br><span class="line">    index = int(input(<span class="string">"\n输入需要查找前驱的节点的索引(0&lt; n &lt;=%d):"</span> % linklist.size()))</span><br><span class="line">    node = linklist.get_node(index)</span><br><span class="line">    pre_node = linklist.pre_node_of(node)</span><br><span class="line">    print(<span class="string">"第%d个节点的内容:%d"</span> % (index, node.data))</span><br><span class="line">    <span class="keyword">if</span> pre_node <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        print(<span class="string">"第%d个节点的前驱的内容内容:%d"</span> % (index, pre_node.data))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"该节点无前驱节点"</span>)</span><br></pre></td></tr></table></figure><h3 id="Java"><a href="#Java" class="headerlink" title="　Java"></a>　<strong>Java</strong></h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Scanner;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> T data;</span><br><span class="line">    <span class="keyword">public</span> Node&lt;T&gt; next;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Node</span><span class="params">()</span></span>&#123;&#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Node</span><span class="params">(T data)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.data = data;</span><br><span class="line">        <span class="keyword">this</span>.next = <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linklist</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Node&lt;T&gt; head;</span><br><span class="line">    <span class="keyword">private</span> Node&lt;T&gt; tail;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> count;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Linklist</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        head = <span class="keyword">new</span> Node&lt;T&gt;();</span><br><span class="line">        tail = head;</span><br><span class="line">        count = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">append</span><span class="params">(T data)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        Node&lt;T&gt; node = <span class="keyword">new</span> Node(data);</span><br><span class="line">        tail.next = node;</span><br><span class="line">        tail = node;</span><br><span class="line">        count++;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">size</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Node&lt;T&gt; <span class="title">getNode</span><span class="params">(<span class="keyword">int</span> index)</span>  <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (index &lt; <span class="number">1</span> || index &gt; count)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> Exception(<span class="string">"Index Error!"</span>);</span><br><span class="line">        Node&lt;T&gt; node = head;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;index; ++i) &#123;</span><br><span class="line">            node = node.next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> node;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Node&lt;T&gt; <span class="title">preNodeOf</span><span class="params">(Node&lt;T&gt; node)</span> </span>&#123;</span><br><span class="line">        Node&lt;T&gt; pre = head;</span><br><span class="line">        Node&lt;T&gt; cur = pre.next;</span><br><span class="line">        <span class="keyword">while</span> (cur.data != node.data) &#123;</span><br><span class="line">            pre = cur;</span><br><span class="line">            cur = cur.next;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (pre == head)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> pre;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">show</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Node&lt;T&gt; node = head.next;</span><br><span class="line">        <span class="keyword">while</span> (node != <span class="keyword">null</span>) &#123;</span><br><span class="line">            System.out.print(node.data + <span class="string">"  "</span>);</span><br><span class="line">            node = node.next;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Demo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Scanner scan = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line"></span><br><span class="line">        Linklist&lt;Integer&gt; list = <span class="keyword">new</span> Linklist&lt;Integer&gt;();</span><br><span class="line">        <span class="keyword">int</span> length;</span><br><span class="line">        System.out.print(<span class="string">"请输入链表长度:"</span>);</span><br><span class="line">        length = scan.nextInt();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;length; ++i) &#123;</span><br><span class="line">            System.out.print(<span class="string">"请输入第"</span> + (i+<span class="number">1</span>) + <span class="string">"个节点的内容:"</span>);</span><br><span class="line">            <span class="keyword">int</span> data = scan.nextInt();</span><br><span class="line">            list.append(data);</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.print(<span class="string">"链表内容:"</span>);</span><br><span class="line">        list.show();</span><br><span class="line"></span><br><span class="line">        System.out.print(<span class="string">"\n输入需要查找前驱的节点的索引(1&lt;= n &lt;="</span> + list.size() + <span class="string">")"</span>);</span><br><span class="line">        <span class="keyword">int</span> index = scan.nextInt();</span><br><span class="line">        Node&lt;Integer&gt; node;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            node = list.getNode(index);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            <span class="comment">// e.printStack();</span></span><br><span class="line">            node = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">            </span><br><span class="line">        Node&lt;Integer&gt; pre_node = list.preNodeOf(node);</span><br><span class="line">        System.out.println(<span class="string">"第"</span> + index + <span class="string">"个节点的内容:"</span> + node.data);</span><br><span class="line">        <span class="keyword">if</span> (pre_node != <span class="keyword">null</span>)</span><br><span class="line">            System.out.println(<span class="string">"第"</span> + index + <span class="string">"个节点的前驱的内容内容:"</span>  + pre_node.data);</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            System.out.println(<span class="string">"该节点无前驱节点"</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;实现在单向链表中-返回某个节点的前驱&quot;&gt;&lt;a href=&quot;#实现在单向链表中-返回某个节点的前驱&quot; class=&quot;headerlink&quot; title=&quot;实现在单向链表中,返回某个节点的前驱.&quot;&gt;&lt;/a&gt;实现在单向链表中,返回某个节点的前驱.&lt;/h1&gt;&lt;h2 id=
      
    
    </summary>
    
      <category term="Teaching Assistant" scheme="http://yoursite.com/categories/Teaching-Assistant/"/>
    
    
      <category term="Teaching Assistant" scheme="http://yoursite.com/tags/Teaching-Assistant/"/>
    
  </entry>
  
  <entry>
    <title>数据结构第一次作业</title>
    <link href="http://yoursite.com/2018/09/17/ta-work-01/"/>
    <id>http://yoursite.com/2018/09/17/ta-work-01/</id>
    <published>2018-09-17T13:09:42.000Z</published>
    <updated>2018-09-22T04:25:47.127Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-逻辑结构和物理结构有什么不同"><a href="#1-逻辑结构和物理结构有什么不同" class="headerlink" title="1.    逻辑结构和物理结构有什么不同?"></a>1.    逻辑结构和物理结构有什么不同?</h1><p>逻辑结构是指从操作对象抽象出的数学模型，其结构定义中的关系用于描述数据元素之间的逻辑关系。<br>逻辑结构在计算机中的表示称为物理结构或存储结构，根据数据元素在计算机中的表示方法，可分为顺序存储结构与链式存储结构。</p><h1 id="2-算法和程序有什么不同？"><a href="#2-算法和程序有什么不同？" class="headerlink" title="2.    算法和程序有什么不同？"></a>2.    算法和程序有什么不同？</h1><p>算法是指解决问题的一种方法或一个过程。<br>算法是若干指令的有穷序列，满足<strong>性质</strong>：</p><blockquote><p>(1)输入：由外部提供的量作为算法的输入.<br>(2)输出：算法产生至少一个量作为输出.<br>(3)确定性：算法的每一步骤必须有确切的定义.<br>(4)有限性：算法的有穷性是指算法必须能在执行有限个步骤之后终止.<br>(5) 可行性，算法需要考虑设计的可能，程序则具体是实现算法上的设计</p></blockquote><p>程序是算法在计算机上用某种程序设计语言的具体实现。<br>程序可以不满足算法的性质(4)。<br>例如操作系统，是一个在无限循环中执行的程序，因而不是一个算法。<br>操作系统的各种任务可看成是单独的问题，每一个问题由操作系统中的一个子程序通过特定的算法来实现。该子程序得到输出结果后便终止。</p><h1 id="3-什么是ADT？"><a href="#3-什么是ADT？" class="headerlink" title="3.    什么是ADT？"></a>3.    什么是ADT？</h1><p>抽象数据类型（ADT）是一个实现包括储存数据元素的存储结构以及实现基本操作的算法，是数据结构作为一个软件组件的实现。ADT的接口用一种类型上的一组操作来定义，每一个操作由它的输入和输出定义。ADT并不会指定数据类型如何实现，这些实现细节对于ADT的用户是隐藏的，并且通过封装来阻止外部对它的访问。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-逻辑结构和物理结构有什么不同&quot;&gt;&lt;a href=&quot;#1-逻辑结构和物理结构有什么不同&quot; class=&quot;headerlink&quot; title=&quot;1.    逻辑结构和物理结构有什么不同?&quot;&gt;&lt;/a&gt;1.    逻辑结构和物理结构有什么不同?&lt;/h1&gt;&lt;p&gt;逻辑结构
      
    
    </summary>
    
      <category term="Teaching Assistant" scheme="http://yoursite.com/categories/Teaching-Assistant/"/>
    
    
      <category term="Teaching Assistant" scheme="http://yoursite.com/tags/Teaching-Assistant/"/>
    
  </entry>
  
  <entry>
    <title>多元线性回归——Day3</title>
    <link href="http://yoursite.com/2018/09/13/ML-100-Days-003/"/>
    <id>http://yoursite.com/2018/09/13/ML-100-Days-003/</id>
    <published>2018-09-13T13:43:18.000Z</published>
    <updated>2018-09-13T14:40:14.231Z</updated>
    
    <content type="html"><![CDATA[<p><strong>相关链接:</strong><br><a href="https://ranmaosong.github.io/2018/09/13/ML-100-Days-003/" target="_blank" rel="noopener">Github地址</a><br><a href="https://www.jianshu.com/p/56b31d24823c" target="_blank" rel="noopener">简书地址</a><br><a href="https://blog.csdn.net/u014630987/article/details/82695498" target="_blank" rel="noopener">CSDN地址</a></p><hr><p>上一节我们讲解了简单现行回归,该回归的输入特征只有 1 个.本节我们对多元线性回归进行讲解,其输入具有多个特征.多元线性回归通过对训练数据拟合一个多元线性方程来对2或多个特征和一个响应值之间进行建模.多元线性回归的处理步骤和上一节的简单线性回归类似,只是在评估阶段存在差异.你可以通过多元线性回归来发现那个因素或特征对预测结果具有较大的影响和找到不同特征是如何相互影响的.多元线性回归数学表达如下:</p><script type="math/tex; mode=display">y = b_0 + b_1x_1 + b_2x_2 ...... + b_nx_n</script><p>从公式我们可以发现, 当n=1时,多元线性回归就变成简单线性回归,因此,简单线性回归是多元线性回归的一个特例.</p><h1 id="说在前面的话"><a href="#说在前面的话" class="headerlink" title="说在前面的话"></a>说在前面的话</h1><h2 id="假设"><a href="#假设" class="headerlink" title="假设"></a>假设</h2><p>在我们进行线性回归建模时,</p><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><h2 id="1-数据预处理"><a href="#1-数据预处理" class="headerlink" title="1. 数据预处理"></a>1. 数据预处理</h2><h2 id="2-训练模型"><a href="#2-训练模型" class="headerlink" title="2. 训练模型"></a>2. 训练模型</h2><h2 id="3-预测测试集"><a href="#3-预测测试集" class="headerlink" title="3. 预测测试集"></a>3. 预测测试集</h2><h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p><a href="https://github.com/RanMaosong/RanMaosong.github.io" target="_blank" rel="noopener">感谢大家的阅读和支持, 欢迎大家上星.</a>.该博客的原始Github项目地址<a href="https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day3_Multiple_Linear_Regression.md" target="_blank" rel="noopener">点击这里</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;相关链接:&lt;/strong&gt;&lt;br&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/09/13/ML-100-Days-003/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github地址&lt;/a&gt;&lt;
      
    
    </summary>
    
      <category term="机器学习100天" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    
    
      <category term="机器学习100天" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    
  </entry>
  
  <entry>
    <title>简单线性回归——Day2</title>
    <link href="http://yoursite.com/2018/09/13/ML-100-Days-002/"/>
    <id>http://yoursite.com/2018/09/13/ML-100-Days-002/</id>
    <published>2018-09-13T06:22:04.000Z</published>
    <updated>2018-09-13T13:36:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>相关链接:</strong><br><a href="https://ranmaosong.github.io/2018/09/13/ML-100-Days-002/" target="_blank" rel="noopener">Github地址</a><br><a href="https://www.jianshu.com/p/ff991ff8d4c7" target="_blank" rel="noopener">简书地址</a><br><a href="https://blog.csdn.net/u014630987/article/details/82687507" target="_blank" rel="noopener">CSDN地址</a></p><hr><p>在预测问题中,我们会经常遇到两种常用术语:回归(Regression)和分类(classification),他们的区别是回归算法解决的是预测连续值,而分类问题则是预测的是离散值,因此回归模型的输出是无限的,而分类问题的输出是有限的.</p><p>在统计学中，线性回归是利用称为线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。</p><p>本文首先介绍简单线性回归，下一节介绍多元线性回归。简单线性回归就是根据一个特征 <code>X</code> 来预测一个与其相关的变量 <code>Y</code>。通常我们我们假设这两种变量之间是现行相关的,即可以通过一条直线把这些变量区分开.因此,简单线性回归就是我们试图寻找一个线性函数,该函数以特征 <code>X</code> 为输入,输出一个变量,且在训练集中,使其预测的值尽可能接近目标值.</p><p>在本文我们以一个学生学习时间(hours),来预测的他该门课程的分数(scores).我们假设这两个变量之间存在某种线性关系,如图所示<br><img src="/images/simple_linear_regression.png" alt="simple_linear_regression"></p><p>我们的目标是找到最有的 $b_0$ 和 $b_1$,使我们训练集中的数据,我们的预测值和真实值之间的误差最小.即</p><script type="math/tex; mode=display">b_0^*,b_1^* = argmax_{b_0, b_1} sum\{(y_i - y_p)^2\}</script><p>其中, $y_p$ 为我们预测的值, $y_i$为真是值.</p><h1 id="1-数据预处理"><a href="#1-数据预处理" class="headerlink" title="1. 数据预处理"></a>1. 数据预处理</h1><p>数据预处理,我们将按照第一天介绍的模型进行处理:</p><ol><li>导入相关库</li><li>导入数据集</li><li>检查缺失值</li><li>划分数据集</li><li>特征标准化</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">dataset = pd.read_csv(&quot;studentscores.csv&quot;)</span><br><span class="line">X = dataset.iloc[:, 0].values</span><br><span class="line">Y = dataset.iloc[:, 1].values</span><br><span class="line"></span><br><span class="line">from sklearn.cross_validation import train_test_split</span><br><span class="line">X_train, Y_train, X_test, Y_test = train_test_split(X, Y, test_size=1/4, random_state=0)</span><br></pre></td></tr></table></figure><h1 id="2-训练线性回归模型"><a href="#2-训练线性回归模型" class="headerlink" title="2. 训练线性回归模型"></a>2. 训练线性回归模型</h1><p>sklearn 机器学习库为我们提供了许多的常用机器学习模型,线性回归模型 LinearRegression 存在于 sklearn.linear_model 文件中, 该文件为我们提供了许多的线性模型.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">regressor = LinearRegression()</span><br><span class="line">regressor.fit(X_train, Y_train)</span><br></pre></td></tr></table></figure><p>我们首先通过 LinearRegression() 初始化一个 regressor 实例来表示线性回归模型.然后通过给 <code>.fit</code> 传入我们的训练集的特征和标签来训练 regressor.</p><p><strong>注意:</strong> 在 sklearn 中对训练数据的格式有一个规定,对于输入 X, 要求其格式是 N*M,其中 N 表示样本数, M 表示每个样本的特征数, 此示例中 M=1.对于标签 Y, 其格式是N*F, N表示样本数, F表示输出值的个数,此处F=1.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(regressor.coef_, regressor.intercept_)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[9.94167834]] [1.93220425]</span><br></pre></td></tr></table></figure></p><p>regressor.coef_表示模型的权重, regressor.intercept_ 表示模型的偏执,分别表示上面模型公式的$b_1$ 和 $b_0$.</p><h1 id="3-预测结果"><a href="#3-预测结果" class="headerlink" title="3. 预测结果"></a>3. 预测结果</h1><p>当我们通过 <code>.fit</code> 函数训练好后模型,我们可以通过 <code>.predict</code> 函数来预测我们未知的数据.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Y_pred = regressor.predict(X_test)</span><br></pre></td></tr></table></figure></p><p>我们通过手动预测来验证上面提到的 regressor.coef_, regressor.intercept_ 表示的意义:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">temp = regressor.intercept_[<span class="number">0</span>] + regressor.coef_[<span class="number">0</span>] * X_test[<span class="number">0</span>]</span><br><span class="line">print(temp, Y_pred[<span class="number">0</span>])</span><br></pre></td></tr></table></figure></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[16.84472176] [16.84472176]</span><br></pre></td></tr></table></figure></p><h1 id="4-可视化"><a href="#4-可视化" class="headerlink" title="4. 可视化"></a>4. 可视化</h1><p>首先我们可视化训练集的结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X_train, Y_train, color=&quot;red&quot;)</span><br><span class="line">plt.plot(X_train, regressor.predict(X_train), color=&apos;blue&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="/images/simple_linear_regression_fig1.png" alt="figure1"><br>对测试集进行可视化<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X_test, Y_test, color=&quot;red&quot;)</span><br><span class="line">plt.plot(X_test, regressor.predict(X_test), color=&quot;blue&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="/images/simple_linear_regression_fig2.png" alt="figure2"></p><p>其中 <code>.scatter</code> 用于画散点图, <code>.plot</code> 用于画直线.</p><h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p><a href="https://github.com/RanMaosong/RanMaosong.github.io" target="_blank" rel="noopener">感谢大家的阅读和支持, 欢迎大家上星.</a>.该博客的原始Github项目地址<a href="https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day2_Simple_Linear_Regression.md" target="_blank" rel="noopener">点击这里</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;相关链接:&lt;/strong&gt;&lt;br&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/09/13/ML-100-Days-002/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github地址&lt;/a&gt;&lt;
      
    
    </summary>
    
      <category term="机器学习100天" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    
    
      <category term="机器学习100天" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    
  </entry>
  
  <entry>
    <title>数据预处理——Day 1</title>
    <link href="http://yoursite.com/2018/09/11/ML-100-Days-001/"/>
    <id>http://yoursite.com/2018/09/11/ML-100-Days-001/</id>
    <published>2018-09-11T02:53:21.000Z</published>
    <updated>2018-09-13T13:36:13.568Z</updated>
    
    <content type="html"><![CDATA[<p><strong>相关链接:</strong><br><a href="https://ranmaosong.github.io/2018/09/11/ML-100-Days-001/" target="_blank" rel="noopener">Github地址</a><br><a href="https://www.jianshu.com/p/b7b5512395f8" target="_blank" rel="noopener">简书地址</a><br><a href="https://blog.csdn.net/u014630987/article/details/82669796" target="_blank" rel="noopener">CSDN地址</a></p><hr><p>总所周知,对于机器学习任务,特别是对于深度学习任务,我们需要创建训练集,因此需要收集大量的数据.然而,在实际中,我们收集的数据极易受噪声、缺失值和数据不一致的影响。通常我们对数据进行如下几种预处理：</p><ol><li>缺失值处理</li><li>异常值处理</li><li>冗余数据处理</li><li>数据标准化</li><li>数据离散化</li><li>数据向量化: 我自己总结的，主要针对文本数据。不知道算不算预处理，但这是文本数据的必须操作。</li></ol><p>本文主要针对缺失值处理和数据向量化两种与处理方法进行讲解，整个处理过程分为6步：</p><ol><li>导入相关库</li><li>导入数据集</li><li>处理缺失值</li><li>编码分类数据</li><li>创建训练集和测试集</li><li>特征缩放</li></ol><p>下面我们将通过实例来一步步详细介绍这些操作。</p><h1 id="1-导入相关库"><a href="#1-导入相关库" class="headerlink" title="1. 导入相关库"></a>1. 导入相关库</h1><p><a href="http://www.numpy.org/" target="_blank" rel="noopener">numpy</a> 和 <a href="https://pandas.pydata.org/" target="_blank" rel="noopener">pandas</a> 是利用 Python 进行科学计算时非常重要两个科学计算库。其中 numpy 负责提供基本的数学计算函数，起运算对象事针对张量或矩阵；而 pandas 库则用来导入和管理数据集。起导入规则如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure><h1 id="2-导入数据集"><a href="#2-导入数据集" class="headerlink" title="2. 导入数据集"></a>2. 导入数据集</h1><p>pandas 是一个管理数据的函数库，我为我们提供了许多读取和操作数据的接口:</p><p><img src="/images/pd_read_data.png" alt="pandas_read_data"></p><p><code>.csv</code> 格式文件是我们保存数据的一种常用格式，其格式简单，其默认格式是以 ,(英文逗号)分割数据集，显示效果和 excel 表格类似。 pandas 为我们提供了访问 <code>.csv</code> 的接口，其 <code>pd.read_csv()</code>用来进行读取 <code>.csv</code>格式数据,该函数常用的参数如下：</p><ul><li>filename: 文件名</li><li>header: 表头，默认不为空（第一行为表头），设为 None， 表示无表头</li><li>sep: 指定分隔符。如果不指定参数，则会尝试使用逗号分隔</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataset = pd.read_csv(<span class="string">'Data.csv'</span>)</span><br><span class="line">X = dataset.iloc[ : , :<span class="number">-1</span>].values</span><br><span class="line">Y = dataset.iloc[ : , <span class="number">3</span>].values</span><br></pre></td></tr></table></figure><p><a href="https://github.com/Avik-Jain/100-Days-Of-ML-Code/tree/master/datasets" target="_blank" rel="noopener">Data.csv</a>是我们访问的数据集,其内容如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Country,Age,Salary,Purchased</span><br><span class="line">France,44,72000,No</span><br><span class="line">Spain,27,48000,Yes</span><br><span class="line">Germany,30,54000,No</span><br><span class="line">Spain,38,61000,No</span><br><span class="line">Germany,40,,Yes</span><br><span class="line">France,35,58000,Yes</span><br><span class="line">Spain,,52000,No</span><br><span class="line">France,48,79000,Yes</span><br><span class="line">Germany,50,83000,No</span><br><span class="line">France,37,67000,Yes</span><br></pre></td></tr></table></figure></p><p><code>pd.read_csv()</code>返回的数据类型为DataFrame（把他就像成一张表），其没有原始的下表操作，我们访问内容通过如下几个函数：</p><ol><li>loc: 通过行标签访问数据</li><li>iloc: 通过行索引进行访问</li><li>ix: loc 和 iloc 的混合</li></ol><p><code>.value</code> 属性则是将 DataFrame 转成 numpy类型，以便后续的数值计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(type(dataset), type(X), type(Y))</span><br><span class="line">print(X.shape, Y.shape)</span><br></pre></td></tr></table></figure><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; &lt;class &apos;numpy.ndarray&apos;&gt; &lt;class &apos;numpy.ndarray&apos;&gt;</span><br><span class="line">(10, 3) (10,)</span><br><span class="line">[[&apos;France&apos; 44.0 72000.0]</span><br><span class="line"> [&apos;Spain&apos; 27.0 48000.0]</span><br><span class="line"> [&apos;Germany&apos; 30.0 54000.0]</span><br><span class="line"> [&apos;Spain&apos; 38.0 61000.0]</span><br><span class="line"> [&apos;Germany&apos; 40.0 nan]</span><br><span class="line"> [&apos;France&apos; 35.0 58000.0]</span><br><span class="line"> [&apos;Spain&apos; nan 52000.0]</span><br><span class="line"> [&apos;France&apos; 48.0 79000.0]</span><br><span class="line"> [&apos;Germany&apos; 50.0 83000.0]</span><br><span class="line"> [&apos;France&apos; 37.0 67000.0]]</span><br></pre></td></tr></table></figure></p><h1 id="3-处理缺失值"><a href="#3-处理缺失值" class="headerlink" title="3. 处理缺失值"></a>3. 处理缺失值</h1><p>从数据集我们可以发现，某些数据为空，这时我们需要对这些缺失值进行处理。我么可以对缺失值进行均值和中值处理，即用整个数据集的该列数据的均值或中值来替换缺失的值。在sklearn的preprocessing包中包含了对数据集中缺失值的处理，主要是应用Imputer类进行处理。代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line">imputer = Imputer(missing_values = <span class="string">"NaN"</span>, strategy = <span class="string">"mean"</span>, axis = <span class="number">0</span>)</span><br><span class="line">imputer = imputer.fit(X[ : , <span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line">X[ : , <span class="number">1</span>:<span class="number">3</span>] = imputer.transform(X[ : , <span class="number">1</span>:<span class="number">3</span>])</span><br></pre></td></tr></table></figure></p><p>该代码首先导入 Imputer 类，然后通过一些参数来创建 Imputer 实例，参数意思大致如下：</p><ul><li>missing_values: 我们要处理的缺失值类型，由于 <code>pandas.read_csv()</code> 会将确实的值默认设置为 Nan，所以这里其类型为 <code>Nan</code>。</li><li>strategy： 表示采用何种策略，其取值有 mean, median, most_frequent,默认为 mean，即均值。</li><li>axis: 表示我们处理的方向，由于这里列方向是同一类型数据，即主轴方向，因此其值为0.</li></ul><p><strong>注意</strong>: sklearn 库预处理操作有个特征是，其处理分为两步： fit 和 transform，fit 大致可以理解为我们对那些数据进行操作或统计，transform 表示实际进行操作。这一点和 sklearn 中的机器学习算法类似，两个固定接口: fit 和 predict。</p><p>由于我们数据库中的数据只有第2、3 列才是数值数据，我们也只能对这两列数据进行预处理，所欲第三行我们在 <code>.fit</code> 中指出要操作的数据，在其内部分别计算了这两列的均值，以供后面处理使用。</p><p>第四行代码执行缺失值处理，通过第三行已经计算出 相应的均值，所以这行代码实际执行缺失值处理</p><p>第三行和第四行代码可以通过<code>.fit_transform</code> 函数来一步执行。这也是 sklearn 的一个特点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(X)</span><br></pre></td></tr></table></figure><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[[&apos;France&apos; 44.0 72000.0]</span><br><span class="line"> [&apos;Spain&apos; 27.0 48000.0]</span><br><span class="line"> [&apos;Germany&apos; 30.0 54000.0]</span><br><span class="line"> [&apos;Spain&apos; 38.0 61000.0]</span><br><span class="line"> [&apos;Germany&apos; 40.0 63777.77777777778]</span><br><span class="line"> [&apos;France&apos; 35.0 58000.0]</span><br><span class="line"> [&apos;Spain&apos; 38.77777777777778 52000.0]</span><br><span class="line"> [&apos;France&apos; 48.0 79000.0]</span><br><span class="line"> [&apos;Germany&apos; 50.0 83000.0]</span><br><span class="line"> [&apos;France&apos; 37.0 67000.0]]</span><br></pre></td></tr></table></figure></p><h1 id="4-编码分类数据"><a href="#4-编码分类数据" class="headerlink" title="4. 编码分类数据"></a>4. 编码分类数据</h1><p>对于我们计算机进行科学计算只针对数值，而不能对文本进行数值计算，因此我们需要对文本进行数据向量化。如何用数值表示文本呢? 最常用的的格式就是 one-hot 格式,这种格式将文本表示成一个向量,该向量只有一个元素的值为1,其余全为0.</p><p>比如,我们数据集有”a”, “b”, “c”, “d”四个文本,此时我们的向量长度则为4,则这四个文本表示如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a: [1, 0, 0, 0]</span><br><span class="line">b: [0, 1, 0, 0]</span><br><span class="line">c: [0, 0, 1, 0]</span><br><span class="line">d: [0, 0, 0, 1]</span><br></pre></td></tr></table></figure></p><p>因此我们需要先统计数据库中所有的文本,并对每个文本从 0 开始进行编号.该过程也通过sklearn库来进行实现:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder, OneHotEncoder</span><br><span class="line">labelencoder_X = LabelEncoder()</span><br><span class="line">X[ : , <span class="number">0</span>] = labelencoder_X.fit_transform(X[ : , <span class="number">0</span>])</span><br></pre></td></tr></table></figure></p><p>由于对于特征(数据集中的最后一列是标签),只有第一列是文本,因此我们需要对第一列进行文本统计.上面代码就是将第一列的文本用自己的编号来进行表示.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(X)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[[0 44.0 72000.0]</span><br><span class="line"> [2 27.0 48000.0]</span><br><span class="line"> [1 30.0 54000.0]</span><br><span class="line"> [2 38.0 61000.0]</span><br><span class="line"> [1 40.0 63777.77777777778]</span><br><span class="line"> [0 35.0 58000.0]</span><br><span class="line"> [2 38.77777777777778 52000.0]</span><br><span class="line"> [0 48.0 79000.0]</span><br><span class="line"> [1 50.0 83000.0]</span><br><span class="line"> [0 37.0 67000.0]]</span><br></pre></td></tr></table></figure></p><p>如何将这些编号表示的文本转换成one-hot形式呢?这就需要使用 sklearn.preprocessing 中的 OneHotEncoder 进行编号.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">onehotencoder = OneHotEncoder(categorical_features = [<span class="number">0</span>])</span><br><span class="line">X = onehotencoder.fit_transform(X).toarray()</span><br><span class="line">labelencoder_Y = LabelEncoder()</span><br><span class="line">Y =  labelencoder_Y.fit_transform(Y)</span><br></pre></td></tr></table></figure></p><p>categorical_features 参数指定了我们需要对哪些列进行one-hot编码,这里为0,表示对第一列,然后利用’.fit_transform’ 进行编码.由于对于标签我们只需要其标签类型,因此不需要进行 one-hot 编码.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(X[<span class="number">0</span>])</span><br><span class="line">print(Y)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[1.0e+00 0.0e+00 0.0e+00 4.4e+01 7.2e+04]</span><br><span class="line">[0 1 0 0 1 1 0 1 0 1]</span><br></pre></td></tr></table></figure></p><p>因为第一列总共有三种文本,因此其one-hot向量长度为3,且第一个样本的 index 为0, 故其 one-hot 为 [1, 0, 0],总的就如上所示.</p><h1 id="5-创建训练集和测试集"><a href="#5-创建训练集和测试集" class="headerlink" title="5. 创建训练集和测试集"></a>5. 创建训练集和测试集</h1><p>对于机器学习任务,我们需要将数据集按照比例划分成两个部分: 训练集和测试集.训练接用于训练模型,测试集用于测试我们模型的性能.对于数据的划分,我们使用 sklearn.cross_validation 的 train_test_split 对数据集按比例进行划分.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cross_validation import train_test_split</span><br><span class="line">X_train, X_test, Y_train, Y_test = train_test_split( X , Y , test_size = 0.2, random_state = 0)</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(X.shape, X_train.shape, X_test.shape)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(10, 5) (8, 5) (2, 5)</span><br></pre></td></tr></table></figure></p><h1 id="6-特征缩放"><a href="#6-特征缩放" class="headerlink" title="6. 特征缩放"></a>6. 特征缩放</h1><p>通过我们需要对数据进行标准化处理,这样可以减小噪声的影响.通常将其标准化为:均值为0, 方差为1的数据范围.<br>标准化之前的方差和均值:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(np.mean(X_train), np.std(X_train))</span><br><span class="line">print(np.mean(X_test), np.std(X_test))</span><br></pre></td></tr></table></figure></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">12527.338888888888 25395.495797239782</span><br><span class="line">13708.2 28152.896706378193</span><br></pre></td></tr></table></figure></p><p>进行标准化:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">sc_X = StandardScaler()</span><br><span class="line">X_train = sc_X.fit_transform(X_train)</span><br><span class="line">X_test = sc_X.fit_transform(X_test)</span><br></pre></td></tr></table></figure></p><p>标准化之后的均值和方法:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(np.mean(X_train), np.std(X_train))</span><br><span class="line">print(np.mean(X_test), np.std(X_test))</span><br></pre></td></tr></table></figure></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">7.771561172376095e-17 1.0</span><br><span class="line">0.0 0.6324555320336759</span><br></pre></td></tr></table></figure></p><h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p><a href="https://github.com/RanMaosong/RanMaosong.github.io" target="_blank" rel="noopener">感谢大家的阅读和支持, 欢迎大家上星.</a>.该博客的原始Github项目地址[点击这里]</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;相关链接:&lt;/strong&gt;&lt;br&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/09/11/ML-100-Days-001/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github地址&lt;/a&gt;&lt;
      
    
    </summary>
    
      <category term="机器学习100天" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    
    
      <category term="机器学习100天" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    
  </entry>
  
  <entry>
    <title>序言</title>
    <link href="http://yoursite.com/2018/09/10/ML-100-Days/"/>
    <id>http://yoursite.com/2018/09/10/ML-100-Days/</id>
    <published>2018-09-10T03:16:16.000Z</published>
    <updated>2018-09-13T13:34:39.533Z</updated>
    
    <content type="html"><![CDATA[<p><strong>相关链接:</strong><br><a href="https://ranmaosong.github.io/2018/09/10/ML-100-Days/" target="_blank" rel="noopener">Github地址</a><br><a href="https://www.jianshu.com/p/ee8fc3881c57" target="_blank" rel="noopener">简书地址</a><br><a href="https://blog.csdn.net/u014630987/article/details/82669779" target="_blank" rel="noopener">CSDN地址</a></p><p>该系列博客是基于由<a href="https://github.com/llSourcell" target="_blank" rel="noopener">Siraj Raval</a>提出的<a href="https://github.com/Avik-Jain/100-Days-Of-ML-Code" target="_blank" rel="noopener">100 Days of Machine Learning Coding</a>.而该系列博客的目的是在学习该项目的同事丰富其内容，加入自己查阅的资料和自己的理解。该项目全部基于 Python 语言进行讲解</p><p><strong>内容安排</strong></p><ul><li><a href="https://ranmaosong.github.io/2018/09/11/ML-100-Days-001/" target="_blank" rel="noopener">第1天 数据集预处理</a></li><li><a href="/2018/09/13/ML-100-Days-002/">第2天 简单线性回归</a></li><li><a href="*">第3天 多元线性回归</a></li><li><a href="*">第4天 Logistic回归</a></li><li><a href="*">第5天 Logistic回归</a></li><li><a href="*">第6天 实现 Logistic回归</a></li><li><a href="*">第7天 K 均值</a></li><li><a href="*">第8天 Logistic回归数学原理</a></li><li><a href="*">第9天 支持向量机</a></li><li><a href="*">第10天 实现 K 均值</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;相关链接:&lt;/strong&gt;&lt;br&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/09/10/ML-100-Days/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github地址&lt;/a&gt;&lt;br&gt;&lt;
      
    
    </summary>
    
      <category term="机器学习100天" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    
    
      <category term="机器学习100天" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    
  </entry>
  
  <entry>
    <title>排序算法</title>
    <link href="http://yoursite.com/2018/08/14/Algorithm-sorting/"/>
    <id>http://yoursite.com/2018/08/14/Algorithm-sorting/</id>
    <published>2018-08-14T14:22:27.000Z</published>
    <updated>2018-08-14T14:57:29.883Z</updated>
    
    <content type="html"><![CDATA[<p><strong>排序</strong>就是将一组无序的元素通过某种规则将他们按照某种主键来进行升序或者降序排列，比如，我们考试成绩，按照总分进行降序排列，按照语文成绩进行降序排列，按照数学成绩进行降序排列，此时的<em>总分</em>, <em>语文</em>, <em>数学</em> 就是我们排序的主键,又或者我们打开电脑文件夹,按照名字排序,按照时间排序等,由此可见,排序操作在我们的日常生活中的作用之大,下面我们将讲解一些常见的排序算法,参考书籍为<strong>&lt;&lt;算法 第四版&gt;&gt;</strong>.</p><p><strong>声明</strong>: 本节所有内容均默认以升序排列,同时所有代码遵从以下模板.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Template</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">sort</span><span class="params">(Comparable[] a)</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">less</span><span class="params">(Comparable v, Comparable w)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> v.compareTo(w) &lt; <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">exch</span><span class="params">(Comparable[] a, <span class="keyword">int</span> i, <span class="keyword">int</span> j)</span> </span>&#123;</span><br><span class="line">        Comparable t = a[i];</span><br><span class="line">        a[i] = a[j];</span><br><span class="line">        a[j] = t;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">show</span><span class="params">(Comparable[] a)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; a.length; ++i) &#123;</span><br><span class="line">            System.out.print(a[i] + <span class="string">", "</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isSorted</span><span class="params">(Comparable[] a)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; a.length; ++i) &#123;</span><br><span class="line">            <span class="keyword">if</span> (less(a[i], a[i-<span class="number">1</span>]))</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"><span class="comment">//        String[] a = new String[n];</span></span><br><span class="line">        sort(a);</span><br><span class="line">        <span class="keyword">assert</span> Template.isSorted(a);</span><br><span class="line">        Template.show(a);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="选择排序"><a href="#选择排序" class="headerlink" title="选择排序"></a>选择排序</h1><p><strong>选择排序</strong> 顾名思义就是从乱序的数组中选择最值,其思路如下: 首先,从 N 个数中选择最小的数,放在数组第一个位置,然后从剩下的 N-1 个数中选择最小的数放在数组的第二个位置,迭代此过程直至完成排序.该算法大约进行 $N^2/2$ 次比较和 N 次交换,详细计算如下:</p><p>寻找第一个最小值时,我们需要在余下的 N-1(为什么是 N-1 不是 N?因为数组坐标为0的数是我们目前最小的值,此时我们从剩下的 N-1 个数比较依次找最小值) 个数中进行比较,且交换 1 次,此时进行了 N-1 次比较,寻找第二个最小值时,需要在余下的 N-2 个数中进行比较,此时进行了 N-2 次比较,所以我们总共需要 $(N-1)+(N-2)+\dots +1=N(N-1)/2$ 比较和 N-1 次交换.</p><p>代码如下:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> sort;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> song</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 18-8-14 下午9:38</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Selection</span> <span class="keyword">extends</span> <span class="title">Template</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span>  <span class="title">sort</span><span class="params">(Comparable[] a)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 升序排列</span></span><br><span class="line">        <span class="keyword">int</span> N = a.length;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; ++i) &#123;</span><br><span class="line">            <span class="keyword">int</span> min = i;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = i+<span class="number">1</span>; j &lt; N; ++j) &#123;</span><br><span class="line">                <span class="keyword">if</span> (Template.less(a[j], a[min]))</span><br><span class="line">                    min = j;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            Template.exch(a, i, min);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        String[] a = <span class="keyword">new</span> String[]&#123;<span class="string">"s"</span>, <span class="string">"o"</span>, <span class="string">"r"</span>, <span class="string">"t"</span>, <span class="string">"e"</span>, <span class="string">"x"</span>, <span class="string">"a"</span>, <span class="string">"m"</span>, <span class="string">"p"</span>, <span class="string">"l"</span>, <span class="string">"e"</span>&#125;;</span><br><span class="line">        sort(a);</span><br><span class="line">        <span class="keyword">assert</span> Template.isSorted(a);</span><br><span class="line">        Template.show(a);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;排序&lt;/strong&gt;就是将一组无序的元素通过某种规则将他们按照某种主键来进行升序或者降序排列，比如，我们考试成绩，按照总分进行降序排列，按照语文成绩进行降序排列，按照数学成绩进行降序排列，此时的&lt;em&gt;总分&lt;/em&gt;, &lt;em&gt;语文&lt;/em&gt;, &lt;em&gt;数
      
    
    </summary>
    
      <category term="Algorithm(4th)" scheme="http://yoursite.com/categories/Algorithm-4th/"/>
    
    
      <category term="Algorithm(4th)" scheme="http://yoursite.com/tags/Algorithm-4th/"/>
    
  </entry>
  
  <entry>
    <title>最长回文子串(Longest Palindromic Substring)</title>
    <link href="http://yoursite.com/2018/04/10/LeetCode-001-LongestPalindromicSubstring/"/>
    <id>http://yoursite.com/2018/04/10/LeetCode-001-LongestPalindromicSubstring/</id>
    <published>2018-04-10T13:05:49.000Z</published>
    <updated>2018-10-06T06:27:45.892Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2018/04/10/001-LeetCode-LongestPalindromicSubstring/" target="_blank" rel="noopener">Github地址</a><br><a href="https://www.jianshu.com/p/c0840c4d03be" target="_blank" rel="noopener">简书地址</a><br><a href="https://blog.csdn.net/u014630987/article/details/79948604" target="_blank" rel="noopener">CSDN地址</a></p><p><strong>问题描述</strong></p><p><strong>给定一个字符串 s，找出其中最长的回文子串，假设给定字符串的长度最大维 1000.</strong></p><p>例如:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入: &quot;babad&quot;</span><br><span class="line">输出: &quot;bab&quot;</span><br><span class="line">注意： “aba” 也是正确的解，有多个解返回其中一个即可</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入：&quot;cbbd&quot;</span><br><span class="line">输出：&quot;bb&quot;</span><br></pre></td></tr></table></figure><p>回文串是指一个字符串对称，从最左边和最右边分别往最中间遍历，各个位置的字符都相同。解决这个问题，下面将从四个算法分别进行介绍。</p><p><strong>1、暴力枚举法（不可取）</strong></p><p>暴力枚举法是最简单、最容易想到的方法，其思路是：首先找到字符串 s 的所有子串，然后判断该子串是否是回文字符串，最后返回最长的回文子串。该方法虽然简单明了，但因其计算成本太高，该算法在实际中并<strong>不可取</strong>。</p><p>由于一个长为 n 的字符串，共有 $\frac{n(n+1)}{2}$ 个连续子串，故寻找子串的时间复杂度为 $O(n^2)$, 判断一个字符串是否维回文串的时间复杂度维 $O(n)$,故</p><p><strong>时间复杂度:</strong> $O(n^3)$</p><p><strong>空间复杂度为:</strong> $O(1)$。</p><p>下面是 Java 的实现代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LongestPalindromicSubstring</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">longestPalindrome</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 保存得到的最长回文子串的起始位置</span></span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>, right = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> len = s.length();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;len; ++i) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j=i; j&lt;len; ++j) &#123;</span><br><span class="line">                <span class="comment">// 获取 s 的连续子串</span></span><br><span class="line">                String subStr = s.substring(i, j+<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 判断子串是否是回文字符串</span></span><br><span class="line">                <span class="keyword">if</span> (isPalindrome(subStr)) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (j-i &gt; right-left) &#123;</span><br><span class="line">                        left = i;</span><br><span class="line">                        right = j;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> s.substring(left, right+<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">isPalindrome</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> len = s.length();</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>, right = len-<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (left &lt; right) &#123;</span><br><span class="line">            <span class="keyword">if</span> (s.charAt(left) != s.charAt(right))</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">            ++left;</span><br><span class="line">            --right;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>2、中心扩展法（可取）</strong></p><p>中心扩展法是根据暴力枚举法改进而来，主要是去除了一些不必要的子字符串的判断，<strong>主要思路:</strong>首先从字符串 s 中选择一个字符作为子字符串的中心字符,然后以该字符维中心依次往左右两边扩展,判断该子串的最左边和最右边的字符是否相同,相同则继续向两边扩展,不相同则停止扩展,该子串则是以该字符为中心的最长回文子串,这样就减少了很多在暴力枚举方法中不必要的字符的判断.</p><p>和暴力枚举方法比较,以”abacdfgdcaba”为例,假设我们以第一个字符 ‘c’ 为中心,中心扩展首先比较”acd”,由于 ‘a’ 和 ‘d’不相同,则停止扩展,二暴力枚举还需比较 “bacdf” 和 “abacdfg”.该算法在扩展时需要同时考虑子串是奇数和偶数的情况.</p><p>由于需要依次迭代每个字符串中心,因此该迭代需要 $O(n)$ 时间复杂度,同时从中心向两边扩展的复杂度维 $O(n)$,因此:</p><p><strong>时间复杂度:</strong> $O(n^2)$</p><p><strong>空间复杂度为:</strong> $O(1)$。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LongestPalindromicSubstring</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span>  <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"abacdfgdcaba"</span>));</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"cbbd"</span>));</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"babad"</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">longestPalindrome</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 保存获得的最大回文子串</span></span><br><span class="line">        String maxStr = <span class="string">""</span>;</span><br><span class="line">        <span class="keyword">int</span> len = s.length();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;len; ++i) &#123;</span><br><span class="line">            String subStr1 = isPalindrome(s, i, i);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (subStr1.length() &gt; maxStr.length()) &#123;</span><br><span class="line">                maxStr = subStr1;</span><br><span class="line">            &#125;</span><br><span class="line">            String subStr2 = isPalindrome(s, i, i+<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (subStr2.length() &gt; maxStr.length()) &#123;</span><br><span class="line">                maxStr = subStr2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> maxStr;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> String <span class="title">isPalindrome</span><span class="params">(String s, <span class="keyword">int</span> i, <span class="keyword">int</span> j)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// i 表示中心扩展的左边字符</span></span><br><span class="line">        <span class="comment">// j 表示中心扩展的右边字符</span></span><br><span class="line">        <span class="keyword">int</span> len = s.length();</span><br><span class="line">        <span class="keyword">while</span> (i &gt;= <span class="number">0</span> &amp;&amp; j &lt; len &amp;&amp; s.charAt(i) == s.charAt(j)) &#123;</span><br><span class="line">            --i;</span><br><span class="line">            ++j;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span>  s.substring(i+<span class="number">1</span>, j);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>3、动态规划（可取）</strong></p><p>回文字符串的子串也是回文字符串,我们可以将最长回文子串分解为一些列子问题,使用动态规划.设 f 为状态表,f(i,j)表示字符区间 [i, j](包括j)是否为回文字符串<br>,f(i, j)=false 表示子串 [i, j] 不是回文字符串,f(i, j)=true 表示子串 [i, j] 为回文字符串.当我们判断了字符 [i], [j] 相同时,只需判断 f(i+1, j-1) 是否维 true 即可.</p><p>状态表满足以下关系:</p><script type="math/tex; mode=display">f(i,j)=\begin{cases}true,\quad i=j \\s[i]==s[j], \quad i= j-1 \\s[i] = s[j]\quad and\quad f(i+1, j-1), \quad i< j-1\end{cases}</script><p>由于状态表 f 是一个 n*n 的方阵,且是一个对称方阵,故我们只需判断状态表 f 的右上角的内容,因此:</p><p><strong>时间复杂度:</strong> $O(n^2)$<br><strong>空间复杂度:</strong> $O(n^2)$</p><p>实现代码:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">test</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span>  <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"abacdfgdcaba"</span>));</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"cbbd"</span>));</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"babad"</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">longestPalindrome</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n = s.length();</span><br><span class="line">        <span class="keyword">boolean</span>[][] f = <span class="keyword">new</span> <span class="keyword">boolean</span>[n][n];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> left =<span class="number">0</span>, right=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;n; ++j) &#123;</span><br><span class="line">            f[j][j] = <span class="keyword">true</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;j; ++i) &#123;</span><br><span class="line">                <span class="keyword">if</span> (s.charAt(i) == s.charAt(j) &amp;&amp; (i == j-<span class="number">1</span> || f[i+<span class="number">1</span>][j-<span class="number">1</span>])) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (j-i &gt; right - left) &#123;</span><br><span class="line">                        left = i;</span><br><span class="line">                        right = j;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    f[i][j] = <span class="keyword">true</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> s.substring(left, right+<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>该代码首先将状态表全部初始化为 false, 然后按照从上到下,从左到右的顺序依次判断状态表的值.<br><strong>4、Manacher 算法（马拉车算法）（可取）</strong></p><p>Manacher 算法是一种经典的求取最长回文子串的方法,其基本原理是使用已知回文字符串的左半部分来推导以右半部分的字符为中心的回文字符.</p><p>我们使用 p[i] 表示以第 i 个字符为中心的最长回文半径.可以利用已知p[0],p[1]……p[i-1]的值,来计算 p[i] 的值.我们定义 maxRight 是当前计算 i 位置时所有回文子串所能达到的最右端的位置,且该回文串的中心位置为 k,此时有如下关系: maxRight = k + p[k],此时有两种情况:</p><p><img src="/images/longestPalindromicSubstring.png" alt="pictures001"></p><p><strong>第一种情况</strong>:i &gt; maxRight,此时初始化p[i] = 1, 然后判断s[i+p[i]] == s[i-p[i]],若不相等则停止,若相等,则++p[i]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">if (i &gt; maxRight) &#123;</span><br><span class="line">    p[i] = 1;</span><br><span class="line">    while(s[i+p[i]] == s[i-p[i]]) &#123;</span><br><span class="line">        ++p[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>第二种情况</strong>:i &lt;= maxRight,此时不在给 p[i] 赋值维为1,由回文串的对称性可得, 2k-i 是 i 关于 k 的对称点.此时由两种情况:</p><ol><li>以 2k-i 为中心的回文串的半径(如图蓝色箭头)大于等于 $maxRight - i$(空心箭头),由对称性可知,已知紫色箭头 5 和 6 关于 k 对称,且 2k-i 和 i 关于 k 对称,所以空心箭头 1 和 4 对称,2 和 3 对称,又箭头 7 和 8 对称,且箭头 1 和 2 分别是 7 和 8 的一部分,所以空心箭头 1 和 2 对称,故空心箭头 3 和 4 对称.所以p[i]的对称半径至少为 maxRight - i.所以首先 p[i]=maxRight - i,然后在依次往两边扩展判断是否对称.</li></ol><p><img src="/images/LongestPalindroomicSubstring1.jpg" alt="picture2"></p><ol><li>以 2k-i 为中心的回文半径小于 maxRight-i,根据和上面类似的推导,可以得知 p[i] = p[2k-i],且不在扩展.</li></ol><p><img src="/images/LongestPalindroomicSubstring2.png" alt="picture3"></p><p>复杂度分析:</p><ul><li><strong>时间复杂度:</strong>$O(n)$</li><li><strong>空间复杂度:</strong> $O(n)$ </li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">test</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span>  <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"abacdfgdcaba"</span>));</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"cbbd"</span>));</span><br><span class="line">        System.out.println(<span class="keyword">new</span> test().longestPalindrome(<span class="string">"babad"</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">longestPalindrome</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        StringBuilder temp = <span class="keyword">new</span> StringBuilder(<span class="string">"#"</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; s.length(); ++i) &#123;</span><br><span class="line">            temp.append(s.charAt(i));</span><br><span class="line">            temp.append(<span class="string">"#"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        String str = temp.toString();</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        * maxCenter: 保存当前能延伸到最右端的回文字符串的中心位置</span></span><br><span class="line"><span class="comment">        * maxId: 保存当前最长回文子串的中心位置</span></span><br><span class="line"><span class="comment">        * p: 保存以该位置的字符维中心位置的最长回文字符的右边长度</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="keyword">int</span> maxCenter=<span class="number">0</span>, maxId=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> n = str.length();</span><br><span class="line">        <span class="keyword">int</span>[] p = <span class="keyword">new</span> <span class="keyword">int</span>[n];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;n; ++i) &#123;</span><br><span class="line">            <span class="keyword">int</span> syncCenter = <span class="number">2</span> * maxCenter - i;</span><br><span class="line">            p[i] = (i&lt;maxCenter + p[maxCenter) ? Math.min(p[syncCenter], maxCenter + p[maxCenter] - i) : <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span>(i-p[i] &gt;=<span class="number">0</span> &amp;&amp; i+p[i]&lt;n &amp;&amp; str.charAt(i-p[i]) == str.charAt(i+p[i])) &#123;</span><br><span class="line">                ++p[i];</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (i + p[i] &gt;= p[maxCenter] + maxCenter) &#123;</span><br><span class="line">                maxCenter = i;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (p[i] &gt; p[maxId]) &#123;</span><br><span class="line">                maxId = i;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> s.substring((maxId - p[maxId])/<span class="number">2</span>, (maxId + p[maxId]) / <span class="number">2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/04/10/001-LeetCode-LongestPalindromicSubstring/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github地址&lt;/a&gt;&lt;b
      
    
    </summary>
    
      <category term="Weekly Algorithm" scheme="http://yoursite.com/categories/Weekly-Algorithm/"/>
    
    
      <category term="Weekly Algorithm" scheme="http://yoursite.com/tags/Weekly-Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>issue_mathjax</title>
    <link href="http://yoursite.com/2018/04/08/issue-mathjax/"/>
    <id>http://yoursite.com/2018/04/08/issue-mathjax/</id>
    <published>2018-04-08T13:50:26.000Z</published>
    <updated>2018-04-08T13:52:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>$c^{T}x^{<em>}=y^{T}Ax^{</em>}=y^{*T}b$</p><script type="math/tex; mode=display">c^{T}x^{*}=y^{T}Ax^{*}=y^{*T}b</script><p>$c_x=yAx=y_b$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;$c^{T}x^{&lt;em&gt;}=y^{T}Ax^{&lt;/em&gt;}=y^{*T}b$&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;c^{T}x^{*}=y^{T}Ax^{*}=y^{*T}b&lt;/script&gt;&lt;p&gt;$c_x=yAx=y_b$
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Spring（二） —— 核心： 依赖注入</title>
    <link href="http://yoursite.com/2018/03/20/002-Spring-IoC/"/>
    <id>http://yoursite.com/2018/03/20/002-Spring-IoC/</id>
    <published>2018-03-20T14:05:32.000Z</published>
    <updated>2018-04-08T13:36:44.590Z</updated>
    
    <content type="html"><![CDATA[<p>在所有的 Java 程序中，总存在 A 对象调用 B 对象， B 对象调用 C 对象，这种情形被 Spring 称为依赖，即 A 对象依赖 B 对象，B 对象依赖 C 对象。对于整个 Java 项目而言，它是由一些相互调用的对象所构成， Spring 把这种相互调用的关系称为依赖关系。</p><p>Spring 提供的核心功能有两个：</p><ol><li>Spring容器作为一个超级大工厂，负责创建、管理所有的Bean（Java对象）；</li><li>Spring容器管理容器中Bean之间的依赖关系，它使用一种称为“依赖注入”的方式来管理 Bean 之间的依赖关系。</li></ol><p>依赖注入是一种优秀的解耦方法，使用依赖注入，程序中的各个组件不需要以硬编码的方式耦合在一起，同时不再需要工厂模式，我们只需使用配置文件就可将各个组件组织在一起。使用依赖注入，不仅可以为Bean注入普通的属性值，还可以注入其它 Bean 的引用。</p><h2 id="一、理解依赖注入"><a href="#一、理解依赖注入" class="headerlink" title="一、理解依赖注入"></a>一、理解依赖注入</h2><p>依赖注入(Dependency Injection)最开始的时候被称为控制反转(Inversion of Control,IoC).在传统方法中，我们主要有两种方式来实现依赖注入：</p><pre><code>1. 原始方法：调用者通过被调用对象的构造函数主动创建以来对象，在调用被依赖对象的方法；2. 工厂方法：调用者通过被依赖对象的的工厂方法来调用其方法； </code></pre><p>第一种方法，调用者通过“new”来创建被依赖对象，当被依赖对象做出更改时，必须修改调用者中的代码，这种以硬编码方法的耦合，非常不利于项目的升级维护。</p><p>工厂方法，大致分为三点：1、调用者面向被依赖对象的借口编程，2、被依赖对象的创建交给工厂实现，3、 调用者通过工厂来获得被依赖对象。但是这种方法会将工厂方法和被依赖对象耦合在一起。</p><p>由于Spring容器负责将被依赖对象赋值给调用者的成员变量，这样调用者无需主动获取被依赖对象，，这样对象间的耦合就通过配置文件来实现。</p><p>总之，Spring 框架的优点有：</p><pre><code>1. 程序无需主动使用 new 调用构造器来创建依赖对象，所有的java对象可交给 Spring 容器去创建。2. 调用者需要调用被依赖对象的方法时，调用者无需主动获取被依赖对象，Spring 容器会注入依赖对象。</code></pre><p>依赖注入有两种方式：</p><pre><code>1. 设值构造： IoC 容器使用成员变量的 setter 方法来注入被依赖对象。2. 构造注入： IoC 容器使用构造器来注入被依赖对象。</code></pre><h2 id="二、设值注入"><a href="#二、设值注入" class="headerlink" title="二、设值注入"></a>二、设值注入</h2><p>设置注入是指 IoC 容器通过成员变量的 setter 方法来注入被依赖对象，这种方法简单、直观，且大量使用。Spring 容器推荐面向接口编程，不管是调用者还是被依赖对象，都应该定义为借口，程序面向他们的借口编程而不是实现类编程。面向借口编程，可以更好地让规范和实现分离，从而更好的解耦，对于 Java EE，不管是 DAO组件还是应用业务逻辑组件，都应该先定义一个借口，来规范该组件应该实现的功能。</p><p>下面我们先定义两个接口在分别实现这两个接口。</p><p>1、 Person 接口</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> demo1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 定义一个使用斧头的方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">useAxe</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li><p>Axe 接口</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> demo1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Axe</span> </span>&#123;</span><br><span class="line">    <span class="comment">//Axe 借口中定义一个 chop() 方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">chop</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>Chinese 类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> demo1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Chinese</span> <span class="keyword">implements</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Axe axe;</span><br><span class="line">    <span class="comment">// 设值注入所需的setter方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAxe</span><span class="params">(Axe axe)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.axe = axe;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">useAxe</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 调用 axe 的chop方法</span></span><br><span class="line">        <span class="comment">// 表明 Person 对象依赖于 axe 对象</span></span><br><span class="line">        System.out.println(axe.chop());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>StoneAxe 类 </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> demo1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StoneAxe</span> <span class="keyword">implements</span> <span class="title">Axe</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">chop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"石头砍柴好慢"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><p>此时，我们已创建好所有的 Java 类，但是他们都还不是 Bean， Spring 也不知道哪些类相互耦合和相互依赖，这时我们需要使用 XML 配置文件来指定实例之间的依赖关系。Spring 2.0 开始，采用 XML Schema 来定义配置文件的语义约束。</p><p>XML 配置文件的内容如下：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">beans</span> <span class="attr">xmlns</span>=<span class="string">"http://www.springframework.org/schema/beans"</span></span></span><br><span class="line"><span class="tag">       <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">       <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--配置 chinese 实例，其实现类是 Chinese 类--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"chinese"</span> <span class="attr">class</span>=<span class="string">"demo1.Chinese"</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--驱动调用 chinese 的 setAxe 方法，将容器中的stoneAxe 作为参数传入--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"axe"</span> <span class="attr">ref</span>=<span class="string">"stoneAxe"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"stoneAxe"</span> <span class="attr">class</span>=<span class="string">"demo1.StoneAxe"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">beans</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>Spring 配置 Bean 实例时必须指定两个属性：</p><ol><li>id： 该 Bean 的唯一标志，Spring 根据 id 属性来管理 Bean，程序通过 id 属性值来访问该 Bean；</li><li><p>class: 指定该 Bean 的实现类，此处不可一世借口，必须是实现类，因为借口不可以实例化。</p><p>Spring 会自动检测每个 <bean...>定义里面的<property...>元素，Spring 会在自动调用默认构造器创建实例之后，立即调用对应的setter方法维 Bean 的成员变量注入值。</property...></bean...></p><p>下面的代码通过 ApplicationContest 的子类 ClassPathXmlApplicationContext 来获取 Person 实例。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> demo1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.springframework.context.ApplicationContext;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.support.ClassPathXmlApplicationContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BeanTest</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 创建容器</span></span><br><span class="line">        ApplicationContext ctx = <span class="keyword">new</span> ClassPathXmlApplicationContext(<span class="string">"beans1.xml"</span>);</span><br><span class="line">        <span class="comment">//获取 chinese 实例</span></span><br><span class="line">        Person p = ctx.getBean(<span class="string">"chinese"</span>, Chinese.class);</span><br><span class="line">        <span class="comment">// 调用 useAxe() 方法</span></span><br><span class="line">        p.useAxe();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出：</p><blockquote><p>石头砍柴好慢</p></blockquote></li></ol><p>如果某一天，系统需要改变 Axe 的实现，比如 SteelAxe 类，此时只需给出 Axe 的另一个类和更改 XML 配置文件，而Person、Chinese类如需任何改动。</p><p>SteelAxe 类<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> demo1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SteelAxe</span> <span class="keyword">implements</span> <span class="title">Axe</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">chop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"钢斧砍柴真快"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>XML 配置文件更改如下：<br>添加一行<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"steelAxe"</span> <span class="attr">class</span>=<span class="string">"demo1.SteelAxe"</span> /&gt;</span></span><br></pre></td></tr></table></figure></p><p>将<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"axe"</span> <span class="attr">ref</span>=<span class="string">"stoneAxe"</span> /&gt;</span></span><br></pre></td></tr></table></figure></p><p>改成：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"axe"</span> <span class="attr">ref</span>=<span class="string">"steelAxe"</span> /&gt;</span></span><br></pre></td></tr></table></figure></p><p>输出：</p><blockquote><p>钢斧砍柴真快</p></blockquote><p>从上面的变化可以看出，chinese 实例与具体的 Axe 实现类没有任何联系，chinese 实例只与 Axe 借口耦合，这就是 Spring 强调面向借口编程的原因。</p><h2 id="三、构造注入"><a href="#三、构造注入" class="headerlink" title="三、构造注入"></a>三、构造注入</h2><h2 id="四、对比两种方式"><a href="#四、对比两种方式" class="headerlink" title="四、对比两种方式"></a>四、对比两种方式</h2><h2 id="五-总结"><a href="#五-总结" class="headerlink" title="五 总结"></a>五 总结</h2><p>Spring IoC容器的三个基本要点：</p><ol><li><p>应用程序各组件面向借口编程。面向接口编程可以讲组件之间的耦合关系提升到借口层次，从而有利于项目的后期维护。</p></li><li><p>应用程序的各组件不在有程序主动创建，而是由 Spring 容器负责产生并初始化。</p></li><li><p>Spring 采用配置文件或注解来管理 Bean 的实现类、依赖关系， Spring容器则根据配置文件或注解，利用反射机制来创建实例，并为之注入依赖关系</p></li></ol><p>$c^{T}x^{<em>}=y^{T}Ax^{</em>}=y^{*T}b$</p><script type="math/tex; mode=display">c^Tx^*=y^{T}Ax^*=y^{*T}b</script><p>2</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在所有的 Java 程序中，总存在 A 对象调用 B 对象， B 对象调用 C 对象，这种情形被 Spring 称为依赖，即 A 对象依赖 B 对象，B 对象依赖 C 对象。对于整个 Java 项目而言，它是由一些相互调用的对象所构成， Spring 把这种相互调用的关系称
      
    
    </summary>
    
      <category term="Spring" scheme="http://yoursite.com/categories/Spring/"/>
    
    
      <category term="Spring" scheme="http://yoursite.com/tags/Spring/"/>
    
  </entry>
  
  <entry>
    <title>Spring（一） —— 入门</title>
    <link href="http://yoursite.com/2018/03/17/001-Spring-Introduction/"/>
    <id>http://yoursite.com/2018/03/17/001-Spring-Introduction/</id>
    <published>2018-03-17T11:25:00.000Z</published>
    <updated>2018-03-20T14:32:46.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>声明</strong>：本教程均在Spring4.0.4版本中实现</p><p>目前Java EE主流的轻量级开发框架有SSH(Struts+spring+Hibernate)和SSM(Spring+Sprng MVC+Mybatis),从中可以看出，无论是SSH还是SSM，Spring在Java开发中有着重要的作用。</p><p>Spring以IoC、AOP维主要思想，它是一个“一站式”框架，即在 Java EE的三层架构，即表现层（Web层）、业务逻辑层（Service层）、数据访问层（DAO）层中，每一层均提供了不同的解决技术。</p><p>当使用Spring框架时，必须使用Spring Core Container（即Spring容器），它是Spring的核心机制，主要由org.springframework.core、org.springframework.beans、org.springframework.context、prg.springframework.expression四个包及其子包组成，主要提供Spring IoC容器支持。</p><h2 id="一-Maven依赖"><a href="#一-Maven依赖" class="headerlink" title="一 Maven依赖"></a>一 Maven依赖</h2><p>本教程全都基于Maven管理工具来构建基于 Spring 框架的应用，因此基础Maven依赖主要包括三个spring-core、spring-context、spring-beans<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- spring-core --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.0.4.RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- spring-context --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-context<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.0.4.RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Spring-beans --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-beans<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.0.4.RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></p><h2 id="二-Spring-Bean概念"><a href="#二-Spring-Bean概念" class="headerlink" title="二 Spring Bean概念"></a>二 Spring Bean概念</h2><p>Spring 核心容器像是一个超级大工厂，所有的Java对象都会成为 Spring 核心容器的管理对象，Spring把容器中的一切管理对象成为Bean。</p><p>Spring容器中的Bean和Java Bean不同，它不需要像 Java Bean 那样遵守一些规范，Spring对Bean没有任何的要求。<strong>在 Spring 中，一切 Java 对象，都是Bean</strong>。</p><p>下面我们先创建一个简单的 Java 类<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.crazyit.app.service;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Axe</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">chop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"使用斧头劈柴"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>接着我们在创建一个 Person 类，在 Person 类的 useAxe() 方法中使用 Axe 对象的 chop() 方法，这样 Person 对象就依赖于 Axe对象。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.crazyit.app.service;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Axe axe;zhong</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAxe</span><span class="params">(Axe axe)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.axe = axe;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">useAxe</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"我打算去砍点柴火！"</span>);</span><br><span class="line">        <span class="comment">// 调用 axe 的 chop() 方法</span></span><br><span class="line">        <span class="comment">// 表明 Person 对象依赖于 axe 对象</span></span><br><span class="line">        System.out.print(axe.chop());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>我们创建好 Java 类后，如何让它们成为Spring Bean，这就要通过XML配置文件或者注解来让 Spring 管理这些 Bean，该配置文件内容如下：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span><br><span class="line"><span class="comment">&lt;!--  Spring 配置文件的根元素没使用spring-beans-4.0.xsd 语义约束--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">beans</span> <span class="attr">xmlns</span>=<span class="string">"http://www.springframework.org/schema/beans"</span></span></span><br><span class="line"><span class="tag">       <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">       <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--配置名为person的Bean，其实现类为org.crazyit.app.service.Person--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"person"</span> <span class="attr">class</span>=<span class="string">"org.crazyit.app.service.Person"</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--控制调用setAxe()方法，将容器的名为axe的Bean作为参数传入--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"axe"</span> <span class="attr">ref</span>=<span class="string">"axe"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--配置名为axe的Bean，其实现类为org.crazyit.app.service.Axe--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"axe"</span> <span class="attr">class</span>=<span class="string">"org.crazyit.app.service.Axe"</span> /&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--配置名为win的Bean，其实现类为javax.swing.JFrame--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"win"</span> <span class="attr">class</span>=<span class="string">"javax.swing.JFrame"</span> /&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--配置名为date的Bean，其实现类为java.util.Date--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"date"</span> <span class="attr">class</span>=<span class="string">"java.util.Date"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">beans</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>该配置文件的根元素维<bean...>,它包括多个<bean...>元素，每个<bean...>元素定义一个Bean，上面的配置文件共定义了四个Bean，其中两个为我们自定义的 Java 类，后面两个使用了 JDK 标准库中的类，这和前面说的 <strong>Spring中,一切 Java 对象皆可为 Bean</strong>一致。</bean...></bean...></bean...></p><p>我们通过如下元素来将 Java 类变为 Bean，<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--配置名为axe的Bean，其实现类为org.crazyit.app.service.Axe--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"axe"</span> <span class="attr">class</span>=<span class="string">"org.crazyit.app.service.Axe"</span> /&gt;</span></span><br></pre></td></tr></table></figure></p><p>其中 id 属性为该 Bean 指定了唯一标志，class 属性指定该Bean要实现哪个类，这个类可以是任何 Java 类：自定义类和标准库类都可以。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--配置名为person的Bean，其实现类为org.crazyit.app.service.Person--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"person"</span> <span class="attr">class</span>=<span class="string">"org.crazyit.app.service.Person"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--控制调用setAxe()方法，将容器的名为axe的Bean作为参数传入--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"axe"</span> <span class="attr">ref</span>=<span class="string">"axe"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br></pre></td></tr></table></figure><p><bean...> 元素还可以包含多个 <property...> 子元素。它用来设置该 Bean 的属性的值，通过调用setter方法来实现，name 属性指定要设置哪个属性的值，ref或value属性决定使用什么内容来作为参数：</property...></bean...></p><ol><li>ref 表示使用容器中的某个 Bean 来作为 setter 的参数；</li><li>value 指明使用内置对象及其包装类来作为setter参数</li></ol><p><strong>底层浅析：</strong></p><p>1.<bean...> 元素默认以反射方式来调用该类的无参构造函数创建 Bean； 其实现方式类似如下代码，以 id 为 person 的Bean为例：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">String idStr = ...; <span class="comment">// 解析&lt;bean.../&gt;元素的id属性得到该字符串值“person”</span></span><br><span class="line">string classStr = ...; <span class="comment">//解析&lt;bean.../&gt;元素的 class 属性得到该字符串值“org.crazyit.app.service.Person”</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">clazz</span> </span>= Class.forName(classStr);</span><br><span class="line">Object obj = <span class="class"><span class="keyword">class</span>.<span class="title">newInstance</span>()</span>;</span><br><span class="line"><span class="comment">// container 代表 Spring 容器</span></span><br><span class="line">container.put(idStr, obj);</span><br></pre></td></tr></table></figure></bean...></p><ol><li><property...>也是通过反射机制来调用对象的 setter 方法来实现的，实现方式如下，以 id 为 person 的Bean为例：<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">String nameStr = ...; <span class="comment">// 解析&lt;property .../&gt;元素的name属性，得到该属性值“axe”</span></span><br><span class="line">String refStr = ...; <span class="comment">//解析&lt;property.../&gt;元素的ref属性得到字符串“axe”</span></span><br><span class="line">String setterName = <span class="string">"set"</span> + nameStr.substring(<span class="number">0</span>, <span class="number">1</span>).toUpperCase() + nameStr.substring(<span class="number">1</span>); <span class="comment">//生成要调用的setter方法名</span></span><br><span class="line"><span class="comment">//获取容器中名为refStr的Bean，</span></span><br><span class="line">Object paramBean = container.get(refStr);</span><br><span class="line">Method setter = clazz.getMethos(setterName, paramBean,getClass());</span><br><span class="line">setter.incoke(obj, paramBean)</span><br></pre></td></tr></table></figure></property...></li></ol><h2 id="三-访问-Bean"><a href="#三-访问-Bean" class="headerlink" title="三 访问 Bean"></a>三 访问 Bean</h2><p>我们在xml配置文件中配置好Bean和它们的相关依赖后，我们需要通过Spring容器来访问容器中的Bean。<code>ApplicationContext</code>是Spring容器最常用的接口，他有两个实现类：</p><ol><li>ClassPathXmlApplicationContext：该类从类加载路径下搜索配置文件，并根据配置文件来创建Spring容器；</li><li>FileSystemXmlApplicationContext：该类从文件系统的相对路径或者绝对路径下去搜索配置文件；</li></ol><p>对于应用程序，类加载路径总是不变的，因此通常用ClassPathXmlApplicationContext来创建Spring容器 。代码如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> lee;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.crazyit.app.service.Person;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.ApplicationContext;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.support.ClassPathXmlApplicationContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BeanTest</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//创建Spring容器</span></span><br><span class="line">        ApplicationContext ctx = <span class="keyword">new</span> ClassPathXmlApplicationContext(<span class="string">"beans.xml"</span>);</span><br><span class="line">        <span class="comment">//获取id为person的Bean</span></span><br><span class="line">        Person p = ctx.getBean(<span class="string">"person"</span>, Person.class);</span><br><span class="line">        <span class="comment">// 调用useAxe() 方法</span></span><br><span class="line">        p.useAxe();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>通过Spring容器来获取Bean对象主要由两个方法：</p><ol><li>Object getBean(String id): 根据容器中Bean的id来获取Bean，该方法需要强制类型转换。</li><li>T getBean(String id, Class <t> requiredType):根据容器中Bean的id来获取Bean，该方法无需进行强制类型转换。</t></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;声明&lt;/strong&gt;：本教程均在Spring4.0.4版本中实现&lt;/p&gt;
&lt;p&gt;目前Java EE主流的轻量级开发框架有SSH(Struts+spring+Hibernate)和SSM(Spring+Sprng MVC+Mybatis),从中可以看出，无论
      
    
    </summary>
    
      <category term="Spring" scheme="http://yoursite.com/categories/Spring/"/>
    
    
      <category term="Spring" scheme="http://yoursite.com/tags/Spring/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch Deep Learning for NLP with Pytorch</title>
    <link href="http://yoursite.com/2018/01/20/PyTorch-Deep-Learning-for-NLP-with-Pytorch/"/>
    <id>http://yoursite.com/2018/01/20/PyTorch-Deep-Learning-for-NLP-with-Pytorch/</id>
    <published>2018-01-20T14:04:52.000Z</published>
    <updated>2018-03-20T14:03:28.000Z</updated>
    
    <content type="html"><![CDATA[<p>此教程翻译自<a href="http://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html" target="_blank" rel="noopener">官方教程</a></p><p><strong>作者:</strong><a href="https://github.com/rguthrie3/DeepLearningForNLPInPytorch" target="_blank" rel="noopener">Robert Guthrie</a></p><p>本教程将引导您学习在使用 Pytorch 进行深度学习编程中的一些主要思想。许多概念（概念图抽象和autograd）并不是 Pytorch 所特有的，而是与任何深度学习工具有关。</p><p>我写这个教程，专门针对自然语言处理和那些从未用过任何深度学习框架（比如Tensorflow,Theano,Keras, Dynet)的写过自然语言处理的人。该教程假定你知道自然语言处理的核心知识： 词性标注、语言建模等。也假定你熟悉神经网络基础（例如来自 Russel 和 Norvig书）。通常，这些书籍涵盖基础的前馈神经网络的反向传播算法，并指出他们是线性和非线性组合的链。<br>本教程旨在让您开始编写深度学习的代码和让你知道这些必备知识。</p><p>请注意，这是关于模型而不是数据。对于所有的模型，我们只创建一个小维度的小的测试样例，以便您可以看到权重在它训练时是如何变化的。如果有一些真实数据你想去尝试，你应该能够撕掉这个笔记上的任何模型，并使用他们。</p><h1 id="介绍-PyTorch"><a href="#介绍-PyTorch" class="headerlink" title="介绍 PyTorch"></a>介绍 PyTorch</h1><h2 id="介绍-Torch-的张量库"><a href="#介绍-Torch-的张量库" class="headerlink" title="介绍 Torch 的张量库"></a>介绍 Torch 的张量库</h2><p>所有深度学习都是基于张量计算，这是矩阵的推广，它的维度可以超过两维。我们将在后面深入地看到这究竟意味着什么。首先，让我们看看我们可以使用张量做什么。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;此教程翻译自&lt;a href=&quot;http://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官方教程&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;str
      
    
    </summary>
    
      <category term="PyTorch" scheme="http://yoursite.com/categories/PyTorch/"/>
    
    
      <category term="PyTorch" scheme="http://yoursite.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Learn Progress</title>
    <link href="http://yoursite.com/2018/01/20/Learn-Progress/"/>
    <id>http://yoursite.com/2018/01/20/Learn-Progress/</id>
    <published>2018-01-20T05:14:25.000Z</published>
    <updated>2018-01-20T05:21:08.773Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目标检测-Object-Detection"><a href="#目标检测-Object-Detection" class="headerlink" title="目标检测(Object Detection)"></a>目标检测(Object Detection)</h1><ul><li><p>R-CNN 论文笔记 </p><ul><li>论文<a href="https://dl.dropboxusercontent.com/s/293tu0hh9ww08co/r-cnn-cvpr.pdf?dl=0" target="_blank" rel="noopener">《Rich feature hierarchies for accurate object detection and semantic segmentation》</a></li><li><a href="http://www.cnblogs.com/kingstrong/p/4969472.html" target="_blank" rel="noopener">http://www.cnblogs.com/kingstrong/p/4969472.html</a></li><li><a href="http://blog.gater.vip/articles/2015/11/02/1478607351098.html" target="_blank" rel="noopener">http://blog.gater.vip/articles/2015/11/02/1478607351098.html</a></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;目标检测-Object-Detection&quot;&gt;&lt;a href=&quot;#目标检测-Object-Detection&quot; class=&quot;headerlink&quot; title=&quot;目标检测(Object Detection)&quot;&gt;&lt;/a&gt;目标检测(Object Detection)
      
    
    </summary>
    
      <category term="Tutorial" scheme="http://yoursite.com/categories/Tutorial/"/>
    
    
      <category term="Tutorial" scheme="http://yoursite.com/tags/Tutorial/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch:数据加载和预处理</title>
    <link href="http://yoursite.com/2018/01/05/PyTorch-Data-Loading-and-Processing-TUtorial/"/>
    <id>http://yoursite.com/2018/01/05/PyTorch-Data-Loading-and-Processing-TUtorial/</id>
    <published>2018-01-05T11:56:09.000Z</published>
    <updated>2018-04-10T12:25:58.335Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2018/01/05/PyTorch-Data-Loading-and-Processing-TUtorial/" target="_blank" rel="noopener">Github地址</a><br><a href="https://www.jianshu.com/p/8b4cdd36a1b3" target="_blank" rel="noopener">简书地址</a><br><a href="https://blog.csdn.net/u014630987/article/details/79886956" target="_blank" rel="noopener">CSDN地址</a></p><p>此教程翻译自<a href="http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#" target="_blank" rel="noopener">PyTorch官方教程</a></p><p><strong>作者</strong>: <a href="https://chsasank.github.io/" target="_blank" rel="noopener">Sasank Chilamkurthy</a></p><p>在解决任何机器学习问题上，在准备数据上会付出很大努力。PyTorch 提供了许多工具， 使数据加载变得简单，希望能使你的代码更具可读性。本教程中，我们将看到图和从一个不重要的数据集中加载和预处理/增强数据。</p><p>要运行本教程，请确保已安装一下软件包：</p><ol><li><code>scikit-image</code>: 用于图像 IO 和 变换</li><li><code>pandas</code>： 更简单的 csv 解析</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function, division</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io, transform</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, utils</span><br><span class="line"></span><br><span class="line"><span class="comment"># Ignore warnings</span></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</span><br><span class="line"></span><br><span class="line">plt.ion()   <span class="comment"># interactive mode</span></span><br></pre></td></tr></table></figure><p>我们将要处理的数据集是面部姿势，意味着一张人脸将像下面这样被标注：<br><img src="/images/landmarked_face2.png" alt="landmarked_face2"></p><p>每张人脸总共有68个不同的地方被标注。</p><p><strong>注意</strong>:<br>数据下载地址为 <a href="https://download.pytorch.org/tutorial/faces.zip，" target="_blank" rel="noopener">https://download.pytorch.org/tutorial/faces.zip，</a> 图像位于名为“faces/“的目录中。这个数据集实际上是通过对来自 imagenet 的几张标注为 ‘face’ 的图片应用优秀的 <code>dlib</code> 的姿态估计来生成的。</p><p>数据集带有一个 csv 标注文件，里面的标注内容看起来像下面这样：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">image_name,part_0_x,part_0_y,part_1_x,part_1_y,part_2_x, ... ,part_67_x,part_67_y</span><br><span class="line">0805personali01.jpg,27,83,27,98, ... 84,134</span><br><span class="line">1084239450_e76e00b7e7.jpg,70,236,71,257, ... ,128,312</span><br></pre></td></tr></table></figure></p><p>让我们快速读取 csv 文件，并把标记数据保存在一个(N, 2)的数组中，其中 N 是特征点的数量。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">landmarks_frame = pd.read_csv(<span class="string">"./data//faces/face_landmarks.csv"</span>)</span><br><span class="line">n = <span class="number">65</span></span><br><span class="line">img_name = landmarks_frame.ix[n, <span class="number">0</span>]</span><br><span class="line">landmarks = landmarks_frame.ix[n, <span class="number">1</span>:].as_matrix().astype(<span class="string">'float'</span>)</span><br><span class="line">landmarks = landmarks.reshape(<span class="number">-1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Image name: &#123;&#125;"</span>.format(img_name))</span><br><span class="line">print(<span class="string">"Landmarks shape: &#123;&#125;"</span>.format(landmarks.shape))</span><br><span class="line">print(<span class="string">"First 4 Landmarks: &#123;&#125;"</span>.format(landmarks[:<span class="number">4</span>]))</span><br></pre></td></tr></table></figure></p><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Image name: person-7.jpg</span><br><span class="line">Landmarks shape: (68, 2)</span><br><span class="line">First 4 Landmarks: [[ 32.  65.]</span><br><span class="line"> [ 33.  76.]</span><br><span class="line"> [ 34.  86.]</span><br><span class="line"> [ 34.  97.]]</span><br></pre></td></tr></table></figure></p><p> 让我们写一个简单的帮主函数来显示图像及其特征点，并用他来显示一个样本。<br> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_landmarks</span><span class="params">(image, landmarks)</span>:</span></span><br><span class="line">    <span class="string">"""SHow image with landmarks"""</span></span><br><span class="line">    plt.imshow(image)</span><br><span class="line">    plt.scatter(landmarks[:, <span class="number">0</span>], landmarks[:, <span class="number">1</span>], s=<span class="number">10</span>, marker=<span class="string">"."</span>, c=<span class="string">"r"</span>)</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">img = io.imread(os.path.join(<span class="string">"./data/faces/"</span>, img_name))</span><br><span class="line">show_landmarks(io.imread(os.path.join(<span class="string">"./data/faces/"</span>, img_name)), landmarks)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p> 输出：<br> <img src="/images/sphx_glr_data_loading_tutorial_001.png" alt="sphx_glr_data_loading_tutorial_001"></p><p><strong>注意</strong>：要得到以上结果，请把 <code>plt.ion()</code> 注释掉。</p><h1 id="Dataset-类"><a href="#Dataset-类" class="headerlink" title="Dataset 类"></a>Dataset 类</h1><p><code>torch.utils.data.Dataset</code> 是一个表示数据集的抽象类。你自定义的数据集类应该继承自 <code>Dataset</code> 并重写如下方法：</p><ul><li><code>__len__</code>: 返回数据集的大小， <code>len(dataset)</code></li><li><code>__getitem__</code>: 是数据集支持索引操作, <code>dataset[i]</code></li></ul><p>让我们维我们的人脸特征点数据集创建一个数据集类。我们将在 <code>__init__</code> 中读取 csv， 但是让读取图片的操作在 <code>__getitem__</code> 中进行。这是内存高效的，因为所有的图像不是一次存储在内存中，而是根据需要进行读取。</p><p>我们数据集的样本将是一个字典<code>{&#39;image&#39;: image, &#39;landmarks&#39;: landmarks}</code>。我们的数据集将接受一个可选参数<code>transform’ 以便可以对样本应用任何需要的处理。我们将在下一节看到</code>transform` 的好处。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FaceLandmarksDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="string">"""Face Landmarks dataset."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, csv_file, root_dir, transform=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            csv_file (string): Path to the csv file with annotations.</span></span><br><span class="line"><span class="string">            root_dir (string): Directory with all the images.</span></span><br><span class="line"><span class="string">            transform (callable, optional): Optional transform to be applied</span></span><br><span class="line"><span class="string">                on a sample.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.landmarks_frame = pd.read_csv(csv_file)</span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.transform = transform</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.landmarks_frame)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        img_name = os.path.join(self.root_dir, self.landmarks_frame.ix[idx, <span class="number">0</span>])</span><br><span class="line">        image = io.imread(img_name)</span><br><span class="line">        landmarks = self.landmarks_frame.ix[idx, <span class="number">1</span>:].as_matrix().astype(<span class="string">'float'</span>)</span><br><span class="line">        landmarks = landmarks.reshape(<span class="number">-1</span>, <span class="number">2</span>)</span><br><span class="line">        sample = &#123;<span class="string">'image'</span>: image, <span class="string">'landmarks'</span>: landmarks&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            sample = self.transform(sample)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sample</span><br></pre></td></tr></table></figure><p>让我们初始化这个类的实例，并在数据样本上迭代。我们讲打印开始4个样本的大小并显示他们的特征点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">face_dataset = FaceLandmarksDataset(csv_file=<span class="string">'faces/face_landmarks.csv'</span>,</span><br><span class="line">                                    root_dir=<span class="string">'faces/'</span>)</span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(face_dataset)):</span><br><span class="line">    sample = face_dataset[i]</span><br><span class="line"></span><br><span class="line">    print(i, sample[<span class="string">'image'</span>].shape, sample[<span class="string">'landmarks'</span>].shape)</span><br><span class="line"></span><br><span class="line">    ax = plt.subplot(<span class="number">1</span>, <span class="number">4</span>, i + <span class="number">1</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    ax.set_title(<span class="string">'Sample #&#123;&#125;'</span>.format(i))</span><br><span class="line">    ax.axis(<span class="string">'off'</span>)</span><br><span class="line">    show_landmarks(**sample)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">3</span>:</span><br><span class="line">        plt.show()</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p><img src="/images/sphx_glr_data_loading_tutorial_002.png" alt="sphx_glr_data_loading_tutorial_002"></p><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">0 (324, 215, 3) (68, 2)</span><br><span class="line">1 (500, 333, 3) (68, 2)</span><br><span class="line">2 (250, 258, 3) (68, 2)</span><br><span class="line">3 (434, 290, 3) (68, 2)</span><br></pre></td></tr></table></figure></p><h1 id="Transform-变换"><a href="#Transform-变换" class="headerlink" title="Transform (变换)"></a>Transform (变换)</h1><p>从上面的例子我们可以看到一个问题：样本的尺寸不一样。大部分的神经网络希望一个固定大小的图像。因此，我们需要写一些预处理代码。让我们来创建三种变换：</p><ul><li><code>Rescale</code>: 缩放图像</li><li><code>RandomCrop</code>: 随机剪裁图像，这是一种数据增强的方法</li><li><code>ToTensor</code>: 把 numpy 图像转换为 PyTorch 图像（我们需要交换轴）</li></ul><p>我们将把它们写成一个可调用的类而不是函数，所以变换所需的参数不必在每次调用时都传递。为此，我们只需实现 <code>__call__</code> 方法，如果需要可以实现 <code>__init__</code> 方法。我们可以向下面这样使用他们：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tsfm = Transform(params)</span><br><span class="line">transformed_sample = tsfm(sample)</span><br></pre></td></tr></table></figure></p><p>请观察下面的变换是如何应用在图像和特征点上的。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Rescale</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Rescale the image in a sample to a given size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        output_size (tuple or int): Desired output size. If tuple, output is</span></span><br><span class="line"><span class="string">            matched to output_size. If int, smaller of image edges is matched</span></span><br><span class="line"><span class="string">            to output_size keeping aspect ratio the same.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, output_size)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> isinstance(output_size, (int, tuple))</span><br><span class="line">        self.output_size = output_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, sample)</span>:</span></span><br><span class="line">        image, landmarks = sample[<span class="string">'image'</span>], sample[<span class="string">'landmarks'</span>]</span><br><span class="line"></span><br><span class="line">        h, w = image.shape[:<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">if</span> isinstance(self.output_size, int):</span><br><span class="line">            <span class="keyword">if</span> h &gt; w:</span><br><span class="line">                new_h, new_w = self.output_size * h / w, self.output_size</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                new_h, new_w = self.output_size, self.output_size * w / h</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            new_h, new_w = self.output_size</span><br><span class="line"></span><br><span class="line">        new_h, new_w = int(new_h), int(new_w)</span><br><span class="line"></span><br><span class="line">        img = transform.resize(image, (new_h, new_w))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># h and w are swapped for landmarks because for images,</span></span><br><span class="line">        <span class="comment"># x and y axes are axis 1 and 0 respectively</span></span><br><span class="line">        landmarks = landmarks * [new_w / w, new_h / h]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'image'</span>: img, <span class="string">'landmarks'</span>: landmarks&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomCrop</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Crop randomly the image in a sample.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        output_size (tuple or int): Desired output size. If int, square crop</span></span><br><span class="line"><span class="string">            is made.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, output_size)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> isinstance(output_size, (int, tuple))</span><br><span class="line">        <span class="keyword">if</span> isinstance(output_size, int):</span><br><span class="line">            self.output_size = (output_size, output_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">assert</span> len(output_size) == <span class="number">2</span></span><br><span class="line">            self.output_size = output_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, sample)</span>:</span></span><br><span class="line">        image, landmarks = sample[<span class="string">'image'</span>], sample[<span class="string">'landmarks'</span>]</span><br><span class="line"></span><br><span class="line">        h, w = image.shape[:<span class="number">2</span>]</span><br><span class="line">        new_h, new_w = self.output_size</span><br><span class="line"></span><br><span class="line">        top = np.random.randint(<span class="number">0</span>, h - new_h)</span><br><span class="line">        left = np.random.randint(<span class="number">0</span>, w - new_w)</span><br><span class="line"></span><br><span class="line">        image = image[top: top + new_h,</span><br><span class="line">                      left: left + new_w]</span><br><span class="line"></span><br><span class="line">        landmarks = landmarks - [left, top]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'image'</span>: image, <span class="string">'landmarks'</span>: landmarks&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ToTensor</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Convert ndarrays in sample to Tensors."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, sample)</span>:</span></span><br><span class="line">        image, landmarks = sample[<span class="string">'image'</span>], sample[<span class="string">'landmarks'</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># swap color axis because</span></span><br><span class="line">        <span class="comment"># numpy image: H x W x C</span></span><br><span class="line">        <span class="comment"># torch image: C X H X W</span></span><br><span class="line">        image = image.transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'image'</span>: torch.from_numpy(image),</span><br><span class="line">                <span class="string">'landmarks'</span>: torch.from_numpy(landmarks)&#125;</span><br></pre></td></tr></table></figure></p><h2 id="组合变换"><a href="#组合变换" class="headerlink" title="组合变换"></a>组合变换</h2><p>现在，我们应用这些变换到我们的样本上。</p><p>假如我们想先把图像的较短的一边缩放到256，然后从中随机剪裁一个224*224大小的图像。即我们想要组合 <code>Rescale</code> 和 <code>RandomCrop</code> 两个变换。</p><p><code>torchvision.transforms.Compose</code> 是一个简单的可调用类，允许我们来组合多个变换<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scale = Rescale(<span class="number">256</span>)</span><br><span class="line">crop = RandomCrop(<span class="number">128</span>)</span><br><span class="line">composed = transforms.Compose([Rescale(<span class="number">256</span>),</span><br><span class="line">                               RandomCrop(<span class="number">224</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply each of the above transforms on sample.</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">sample = face_dataset[<span class="number">65</span>]</span><br><span class="line"><span class="keyword">for</span> i, tsfrm <span class="keyword">in</span> enumerate([scale, crop, composed]):</span><br><span class="line">    transformed_sample = tsfrm(sample)</span><br><span class="line"></span><br><span class="line">    ax = plt.subplot(<span class="number">1</span>, <span class="number">3</span>, i + <span class="number">1</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    ax.set_title(type(tsfrm).__name__)</span><br><span class="line">    show_landmarks(**transformed_sample)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="/images/sphx_glr_data_loading_tutorial_003.png" alt="sphx_glr_data_loading_tutorial_003"></p><h2 id="迭代数据集"><a href="#迭代数据集" class="headerlink" title="迭代数据集"></a>迭代数据集</h2><p>我们把这些放在一个来创建一个包含组合变换的数据集。总之，每当这个数据集被采样时执行一下操作：</p><ul><li>即时从文件中读取图像。</li><li>对图像应用变换。</li><li>由于其中一个变换是随机的，因此数据的采样得到增强。</li></ul><p>我们可以使用和之前一样的 <code>for i in range</code> 循环来迭代创建的数据集。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">transformed_dataset = FaceLandmarksDataset(csv_file=<span class="string">'faces/face_landmarks.csv'</span>,</span><br><span class="line">                                           root_dir=<span class="string">'faces/'</span>,</span><br><span class="line">                                           transform=transforms.Compose([</span><br><span class="line">                                               Rescale(<span class="number">256</span>),</span><br><span class="line">                                               RandomCrop(<span class="number">224</span>),</span><br><span class="line">                                               ToTensor()</span><br><span class="line">                                           ]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(transformed_dataset)):</span><br><span class="line">    sample = transformed_dataset[i]</span><br><span class="line"></span><br><span class="line">    print(i, sample[<span class="string">'image'</span>].size(), sample[<span class="string">'landmarks'</span>].size())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">3</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure></p><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">0 torch.Size([3, 224, 224]) torch.Size([68, 2])</span><br><span class="line">1 torch.Size([3, 224, 224]) torch.Size([68, 2])</span><br><span class="line">2 torch.Size([3, 224, 224]) torch.Size([68, 2])</span><br><span class="line">3 torch.Size([3, 224, 224]) torch.Size([68, 2])</span><br></pre></td></tr></table></figure></p><p>但是，通过使用简单的for循环遍历数据，我们将失去许多功能。特别是我们错过了：</p><ul><li>批处理数据</li><li>打乱数据</li><li>使用多线程并行加载数据</li></ul><p><code>torch.utils.data.DataLoader</code> 是一个提供以上所有的功能的迭代器。下面使用的参数应该是清楚的。其中一个又去的参数是 <code>collate_fn</code>。你可以指定如何使用 collate_fn 对样本进行批处理。但是，对大多数情况来说，默认的自动分页应该可以正常工作的很好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">dataloader = DataLoader(transformed_dataset, batch_size=<span class="number">4</span>,</span><br><span class="line">                        shuffle=<span class="keyword">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Helper function to show a batch</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_landmarks_batch</span><span class="params">(sample_batched)</span>:</span></span><br><span class="line">    <span class="string">"""Show image with landmarks for a batch of samples."""</span></span><br><span class="line">    images_batch, landmarks_batch = \</span><br><span class="line">            sample_batched[<span class="string">'image'</span>], sample_batched[<span class="string">'landmarks'</span>]</span><br><span class="line">    batch_size = len(images_batch)</span><br><span class="line">    im_size = images_batch.size(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    grid = utils.make_grid(images_batch)</span><br><span class="line">    plt.imshow(grid.numpy().transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size):</span><br><span class="line">        plt.scatter(landmarks_batch[i, :, <span class="number">0</span>].numpy() + i * im_size,</span><br><span class="line">                    landmarks_batch[i, :, <span class="number">1</span>].numpy(),</span><br><span class="line">                    s=<span class="number">10</span>, marker=<span class="string">'.'</span>, c=<span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line">        plt.title(<span class="string">'Batch from dataloader'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i_batch, sample_batched <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">    print(i_batch, sample_batched[<span class="string">'image'</span>].size(),</span><br><span class="line">          sample_batched[<span class="string">'landmarks'</span>].size())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># observe 4th batch and stop.</span></span><br><span class="line">    <span class="keyword">if</span> i_batch == <span class="number">3</span>:</span><br><span class="line">        plt.figure()</span><br><span class="line">        show_landmarks_batch(sample_batched)</span><br><span class="line">        plt.axis(<span class="string">'off'</span>)</span><br><span class="line">        plt.ioff()</span><br><span class="line">        plt.show()</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p><img src="/images/sphx_glr_data_loading_tutorial_004.png" alt="sphx_glr_data_loading_tutorial_004"></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">0 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])</span><br><span class="line">1 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])</span><br><span class="line">2 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])</span><br><span class="line">3 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])</span><br></pre></td></tr></table></figure></p><h1 id="后记：torchvision"><a href="#后记：torchvision" class="headerlink" title="后记：torchvision"></a>后记：torchvision</h1><p>在此教程中，我们看到了如何写和使用数据集（dataset），变换（transform）和数据加载器（dataloader）。<br><code>torchvision</code> 包提供了一些常见数据集和变换。你甚至可能不需要编写自定义类。<code>torchvision</code> 提供了一个更通过的数据集： ImageFolder。它假设图像按以下方式组织：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">root/ants/xxx.png</span><br><span class="line">root/ants/xxy.jpeg</span><br><span class="line">root/ants/xxz.png</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">root/bees/123.jpg</span><br><span class="line">root/bees/nsdf3.png</span><br><span class="line">root/bees/asd932_.png</span><br></pre></td></tr></table></figure></p><p>其中 ‘ant’,’bees’等是类别标签，类似的通用的操作于 <code>PIL.Image</code> 的变换，如<code>RandomHOrizontalFlip</code>， <code>Scale</code> 也是可以获取的。你可以向下面一样使用这些变换来写一个数据加载器。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, datasets</span><br><span class="line"></span><br><span class="line">data_transform = transforms.Compose([</span><br><span class="line">        transforms.RandomSizedCrop(<span class="number">224</span>),</span><br><span class="line">        transforms.RandomHorizontalFlip(),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">                             std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ])</span><br><span class="line">hymenoptera_dataset = datasets.ImageFolder(root=<span class="string">'hymenoptera_data/train'</span>,</span><br><span class="line">                                           transform=data_transform)</span><br><span class="line">dataset_loader = torch.utils.data.DataLoader(hymenoptera_dataset,</span><br><span class="line">                                             batch_size=<span class="number">4</span>, shuffle=<span class="keyword">True</span>,</span><br><span class="line">                                             num_workers=<span class="number">4</span>)</span><br></pre></td></tr></table></figure></p><p>有关训练代码的示例，请阅读 迁移学习章节。</p><p><a href="http://pytorch.org/tutorials/_downloads/data_loading_tutorial.py" target="_blank" rel="noopener">Python 源码</a><br><a href="http://pytorch.org/tutorials/_downloads/data_loading_tutorial.ipynb" target="_blank" rel="noopener">Jupyter</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2018/01/05/PyTorch-Data-Loading-and-Processing-TUtorial/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github地址&lt;/
      
    
    </summary>
    
      <category term="PyTorch" scheme="http://yoursite.com/categories/PyTorch/"/>
    
    
      <category term="PyTorch" scheme="http://yoursite.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>用例子学习 PyTorch</title>
    <link href="http://yoursite.com/2017/12/27/Learning-PyTorch-with-Examples/"/>
    <id>http://yoursite.com/2017/12/27/Learning-PyTorch-with-Examples/</id>
    <published>2017-12-27T04:59:35.000Z</published>
    <updated>2018-04-10T12:17:02.666Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2017/12/27/Learning-PyTorch-with-Examples/" target="_blank" rel="noopener">Github地址</a><br><a href="https://www.jianshu.com/p/52684285e335" target="_blank" rel="noopener">简书地址</a><br><a href="https://blog.csdn.net/u014630987/article/details/79886956" target="_blank" rel="noopener">CSDN地址</a></p><p>此教程翻译自 <code>PyTorch</code> <a href="http://pytorch.org/tutorials/beginner/pytorch_with_examples.html#tensors" target="_blank" rel="noopener">官方教程</a></p><p><strong>作者</strong><a href="https://github.com/jcjohnson/pytorch-examples" target="_blank" rel="noopener">Justin Johnson</a></p><p>本教程通过 <code>PyTorch</code> 自带的一些实例来介绍它的基本概念。<br><code>PyTorch</code> 核心提供了两个主要特征：</p><ol><li>n 维的张量，类似于 <code>numpy</code> ，但可以在 GPU 上运行</li><li>为构建和训练神经网络提供自动求导</li></ol><p>我们讲使用一个全连接的 <code>ReLU</code> 神经网络来作为我们运行的例子。这个神经网络只有一个隐藏层，并且通过梯度下降来最小化网络输出和真实值的欧几里得距离来训练神经网络拟合随机数据。</p><h1 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h1><h2 id="热身：-numpy"><a href="#热身：-numpy" class="headerlink" title="热身： numpy"></a>热身： numpy</h2><p>在介绍 <code>PyTorch</code> 之前，我们首先使用 <code>numpy</code> 来实现这个神经网络。<br><code>numpy</code> 提供了一个 n 维数组对象和许多操作这些数组的函数。<code>numpy</code> 是一个通用的科学计算框架；它没有计算图、深度学习和梯度的概念。然而我们可以很容易的使用 <code>numpy</code> 的操作手动实现前向传播和反向传播让一个两层的网络来拟合随机数据：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random input and output data</span></span><br><span class="line">x = np.random.randn(N, D_in)</span><br><span class="line">y = np.random.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Randomly initialize weights</span></span><br><span class="line">w1 = np.random.randn(D_in, H)</span><br><span class="line">w2 = np.random.randn(H, D_out)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y</span></span><br><span class="line">    h = x.dot(w1)</span><br><span class="line">    h_relu = np.maximum(h, <span class="number">0</span>)</span><br><span class="line">    y_pred = h_relu.dot(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = np.square(y_pred - y).sum()</span><br><span class="line">    print(t, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.T.dot(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.dot(w2.T)</span><br><span class="line">    grad_h = grad_h_relu.copy()</span><br><span class="line">    grad_h_relu[h&lt;<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.T.dot(grad_h)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># update weights</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure></p><h2 id="PyTorch-Tensors"><a href="#PyTorch-Tensors" class="headerlink" title="PyTorch: Tensors"></a>PyTorch: Tensors</h2><p><code>Numpy</code> 是一个伟大的框架，但是他不能利用 GPUs 来加快数值计算。对于现代深度神经网络， GPU 通常提供50倍或更高的加速，所以不幸的是 numpy 不足以用于现代深度学习。</p><p>这里，我介绍 PyTorch 中最基础的概念： 张量（Tensor）。Tensor 在概念上与 numpy 数组相同，它是一个 N 维的数组，并提供很多操作它的函数。和 numpy 类似，PyTorch 中的 Tensors 没有深度学习、计算图和梯度这些内容，他们是科学计算的通用工具。</p><p>然而，PyTorch的Tensor 和 numpy 不同，它可以利用 GPUs 来加快数值计算。为了让 Tensors 在 GPU上运行，你只需把它转型维一个新的类型即可。</p><p>这里我们使用 PyTorch 来训练一个两层的网络让它拟合随机数据。 和上面 numpy 的例子类似，我们需要手动实现网络的前向和反向传播：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">dtype = torch.FloatTensor</span><br><span class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimenssion</span></span><br><span class="line"><span class="comment"># H is hidden demenssion; D_out is output dimension</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create random input and output data</span></span><br><span class="line">x = torch.randn(N, D_in).type(dtype)</span><br><span class="line">y = torch.randn(N, D_out).type(dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Randomly initialize weights</span></span><br><span class="line">w1 = torch.randn(D_in, H).type(dtype)</span><br><span class="line">w2 = torch.randn(H, D_out).type(dtype)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pss: compute predicted y</span></span><br><span class="line">    h = x.mm(w1)</span><br><span class="line">    h_relu = h.clamp(min=<span class="number">0</span>)</span><br><span class="line">    y_pred = h_relu.mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute and print loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</span><br><span class="line">    print(t, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backprop to compute gradients of w1 and w2 with respect to loss</span></span><br><span class="line">    grad_y_pred = <span class="number">3.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.t().mm(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.mm(w2.t())</span><br><span class="line">    grad_h = grad_h_relu.clone()</span><br><span class="line">    grad_h[h&lt;<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.t().mm(grad_h)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># update weights using gradient descent</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure></p><h1 id="自动求导"><a href="#自动求导" class="headerlink" title="自动求导"></a>自动求导</h1><h2 id="PyTorch：-Variables-和-autograd"><a href="#PyTorch：-Variables-和-autograd" class="headerlink" title="PyTorch： Variables 和 autograd"></a>PyTorch： Variables 和 autograd</h2><p> 在上面的例子中，我们自己实现了神经网络的前向和反向传播。手动实现反向传播对于一个小型的两层网络来说并不是什么难题，但是对于大型复杂啊网络，很快就会变得棘手。</p><p>值得庆幸的是，我们可以使用自动微分来自动计算神经网络的反向传播。PyTorch 中的 <code>autograd</code> 包提供了这一功能。当我们使用自动求导时，你的神经网络的前向传播讲定义一个计算图；图中的节点将是 Tensors，边表示从输入张量产生输出张量的函数。通过图反向传播可以让你轻松的计算梯度。</p><p>这听起来很复杂，但是在实际使用却十分简单。我们用 <code>Variable</code> 对象来包装 Tensors。 一个 Variable 表示计算图中的一个节点。如果 <code>x</code> 是一个 <code>Variable</code>，那么 <code>x.data</code> 则是一个 Tensor，<code>x.grad</code> 是另一个用来保存 <code>x</code> 关于某个标量值的梯度。</p><p>PyTorch 的 Variable 拥有和 PyTorch 的 Tensors 一样的 API：几乎所有 Tensors 上的操作都可以在 Variable 上运行；不同之处在于使用 Variables 定义了一个计算图， 允许你自动的计算梯度。</p><p>这里我们使用 Variable 和自动求导来实现我们的两层网络；现在我们不再需要手动地实现反向传播。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dtype = torch.FloatTensor</span><br><span class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimenssion</span></span><br><span class="line"><span class="comment"># H is hidden demenssion; D_out is output dimension</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold input and outpus, and wrap them in Variables.</span></span><br><span class="line"><span class="comment"># Setting requires_grad=False indicates that we do not need to compute gradients</span></span><br><span class="line"><span class="comment"># with respect to these Variables during the backward pass</span></span><br><span class="line">x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=<span class="keyword">False</span>)</span><br><span class="line">y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors for weights, and wrap them in Variables.</span></span><br><span class="line"><span class="comment"># Setting requires_grad=True indicates that we want to compute gradients with</span></span><br><span class="line"><span class="comment"># respect to these Variables during the backward pass.</span></span><br><span class="line">w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=<span class="keyword">True</span>)</span><br><span class="line">w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y using operations on Variables; these</span></span><br><span class="line">    <span class="comment"># are exactly the same operations we used to compute the forward pass using</span></span><br><span class="line">    <span class="comment"># Tensors, but we do not need to keep references to intermediate values since</span></span><br><span class="line">    <span class="comment"># we are not implementing the backward pass by hand.</span></span><br><span class="line">    y_pred = x.mm(w1).clamp(min=<span class="number">0</span>).mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss using operations on Variables.</span></span><br><span class="line">    <span class="comment"># Now loss is a Variable of shape (1,) and loss.data is a Tensor of shape</span></span><br><span class="line">    <span class="comment"># (1,); loss.data[0] is a scalar value holding the loss.</span></span><br><span class="line"></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</span><br><span class="line">    print(t, loss.data[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use autograd to compute the backward pass. This call will compute the</span></span><br><span class="line">    <span class="comment"># gradient of loss with respect to all Variables with requires_grad=True.</span></span><br><span class="line">    <span class="comment"># After this call w1.grad and w2.grad will be Variables holding the gradient</span></span><br><span class="line">    <span class="comment"># of the loss with respect to w1 and w2 respectively.</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using gradient descent; w1.data and w2.data are Tensors,</span></span><br><span class="line">    <span class="comment"># w1.grad and w2.grad are Variables and w1.grad.data and w2.grad.data are</span></span><br><span class="line">    <span class="comment"># Tensors.</span></span><br><span class="line">    w1.data -= learning_rate * w1.grad.data</span><br><span class="line">    w2.data-= learning_rate * w2.grad.data</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Manually zero the gradients after updating weights</span></span><br><span class="line">    w1.grad.data.zero_()</span><br><span class="line">    w2.grad.data.zero_()</span><br></pre></td></tr></table></figure></p><h2 id="PyTorch-定义新的-autograd-函数"><a href="#PyTorch-定义新的-autograd-函数" class="headerlink" title="PyTorch:定义新的 autograd 函数"></a>PyTorch:定义新的 autograd 函数</h2><p>在底层，每个原始 autograd 操作符实际上是在 Tensors 上操作的两个函数。<strong>forward</strong> 函数从输入张量来计算输出张量。<strong>backward</strong>函数接收输出向量关于某个标量的梯度，然后计算输入张量关于关于同一个标量的梯度。</p><p>在 PyTorch 中， 我们可以通过定义一个 <code>torch.autograd.Function</code> 的子类并实现 <code>forward</code> 和 <code>backward</code>两个函数容易地实现我们自己的 autograd 操作符。我们可以通过定义一个该操作符的实例，并像函数一样给它传递一个包含输入数据的 Variable 来调用它，这样就使用我们新定义的 autograd 操作符。</p><p>在这个例子中我们， 我们自己定义了一个 autograd 函数来执行 ReLU 非线性映射，并使用它实现了一个两层的网络：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyReLU</span><span class="params">(torch.autograd.Function)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    We can implement our own custom autograd Function by subclassing</span></span><br><span class="line"><span class="string">    torch.autograd.Function and implementing forward and backward passed</span></span><br><span class="line"><span class="string">    which operate on tensors</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the forward pass we receive a  Tensor containing the input and return a</span></span><br><span class="line"><span class="string">        Tensor containing the output. You can cache arbitrary Tensors for use in the</span></span><br><span class="line"><span class="string">        backward pass using the save_for_backward method.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.save_for_backward(input)</span><br><span class="line">        <span class="keyword">return</span> input.clamp(min=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad_output)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the backward pass we receive a Tensor containing the gradient of loss </span></span><br><span class="line"><span class="string">        with respect to the output, and we need to compute the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the input</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        input, = self.saved_tensors</span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        grad_input[input&lt;<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dtype = torch.FloatTensor</span><br><span class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold input and outputs, and wrap them in Variables.</span></span><br><span class="line">x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=<span class="keyword">False</span>)</span><br><span class="line">y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors for weights, and wrap them in Variables.</span></span><br><span class="line">w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=<span class="keyword">True</span>)</span><br><span class="line">w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Construct an instance of our MyReLU class to use in our network</span></span><br><span class="line">    relu = MyReLU()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y using operations on Variables; we compute</span></span><br><span class="line">    <span class="comment"># ReLU using our custom autograd operation.</span></span><br><span class="line">    y_pred = relu(x.mm(w1)).mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</span><br><span class="line">    print(t, loss.data[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use autograd to compute the backward pass</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using gradient descent</span></span><br><span class="line">    w1.data -= learning_rate * w1.grad.data</span><br><span class="line">    w2.data -= learning_rate * w2.grad.data</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Manually zero the gradients after updateing weights</span></span><br><span class="line">    w1.grad.data.zero_()</span><br><span class="line">    w2.grad.data.zero_()</span><br></pre></td></tr></table></figure></p><h2 id="Tensorflow-静态图"><a href="#Tensorflow-静态图" class="headerlink" title="Tensorflow: 静态图"></a>Tensorflow: 静态图</h2><p>PyTorch 的 autograd 看起来很像 Tensorflow： 在两个框架中，我们都定义一个计算图，然后使用自动微分来计算梯度。最大的区别是： Tensorflow 中的计算图是静态的，而 PyTorch 使用的是动态计算图。</p><p>在 Tensorflow 中，我们只定义一次计算图，然后重复执行同一个图，可能将不同的输入数据提供给图。在 PyTorch 中，每次前向传播都会定义一个新的计算图。</p><p>静态图很好，因为您可以预先优化图; 例如，一个框架可能决定为了效率而融合某些图操作，或者想出一个在许多GPU或许多机器上的分布运行计算图的策略。 如果您一遍又一遍地重复使用同一个图，那么这个潜在的昂贵的前期优化可以在同一个图重复运行的情况下分摊。</p><p>静态图和动态图不同的一个方面是控制流。对于某些模型，我们可能希望对数据点执行不同的计算；例如，对于每个数据点，循环网络可以展开不同数量的时间步长，这个展开可以作为一个循环来实现。</p><p>使用静态图，循环结构需要成为图形的一部分；出于这个原因，Tensorflow 提供了像 <code>tf.scan</code> 这样的操作付来将循环嵌入到计算图中。使用动态图，这种情形就变得更简单了：由于我们为每个示例动态地都贱图，我们可以使用普通的命令式流控制来执行每个输入的不同计算。</p><p>为了与上面的 PyTorch 的 autograd 例子对比，这里我们使用 Tensorflow 来拟合一个简单的两层网络：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># First we set up the computational graph:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create placeholders for the input and target data; these will be filled</span></span><br><span class="line"><span class="comment"># with real data when we execute the graph.</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, D_in))</span><br><span class="line">y = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, D_out))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create Variables for the weights and initialize them with random data.</span></span><br><span class="line"><span class="comment"># A TensorFlow Variable persists its value across executions of the graph.</span></span><br><span class="line">w1 = tf.Variable(tf.random_normal((D_in, H)))</span><br><span class="line">w2 = tf.Variable(tf.random_normal((H, D_out)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Forward pass: Compute the predicted y using operations on TensorFlow Tensors.</span></span><br><span class="line"><span class="comment"># Note that this code does not actually perform any numeric operations; it</span></span><br><span class="line"><span class="comment"># merely sets up the computational graph that we will later execute.</span></span><br><span class="line">h = tf.matmul(x, w1)</span><br><span class="line">h_relu = tf.maximum(h, tf.zeros(<span class="number">1</span>))</span><br><span class="line">y_pred = tf.matmul(h_relu, w2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute loss using operations on TensorFlow Tensors</span></span><br><span class="line">loss = tf.reduce_sum((y - y_pred) ** <span class="number">2.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute gradient of the loss with respect to w1 and w2.</span></span><br><span class="line">grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Update the weights using gradient descent. To actually update the weights</span></span><br><span class="line"><span class="comment"># we need to evaluate new_w1 and new_w2 when executing the graph. Note that</span></span><br><span class="line"><span class="comment"># in TensorFlow the the act of updating the value of the weights is part of</span></span><br><span class="line"><span class="comment"># the computational graph; in PyTorch this happens outside the computational</span></span><br><span class="line"><span class="comment"># graph.</span></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line">new_w1 = w1.assign(w1 - learning_rate * grad_w1)</span><br><span class="line">new_w2 = w2.assign(w2 - learning_rate * grad_w2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now we have built our computational graph, so we enter a TensorFlow session to</span></span><br><span class="line"><span class="comment"># actually execute the graph.</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># Run the graph once to initialize the Variables w1 and w2.</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create numpy arrays holding the actual data for the inputs x and targets</span></span><br><span class="line">    <span class="comment"># y</span></span><br><span class="line">    x_value = np.random.randn(N, D_in)</span><br><span class="line">    y_value = np.random.randn(N, D_out)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">        <span class="comment"># Execute the graph many times. Each time it executes we want to bind</span></span><br><span class="line">        <span class="comment"># x_value to x and y_value to y, specified with the feed_dict argument.</span></span><br><span class="line">        <span class="comment"># Each time we execute the graph we want to compute the values for loss,</span></span><br><span class="line">        <span class="comment"># new_w1, and new_w2; the values of these Tensors are returned as numpy</span></span><br><span class="line">        <span class="comment"># arrays.</span></span><br><span class="line">        loss_value, _, _ = sess.run([loss, new_w1, new_w2],</span><br><span class="line">                                    feed_dict=&#123;x: x_value, y: y_value&#125;)</span><br><span class="line">        print(loss_value)</span><br></pre></td></tr></table></figure></p><h1 id="nn-模块"><a href="#nn-模块" class="headerlink" title="nn 模块"></a>nn 模块</h1><h2 id="PyTorch-nn"><a href="#PyTorch-nn" class="headerlink" title="PyTorch:nn"></a>PyTorch:nn</h2><p>计算图和 autograd 是定义复杂运算符和自动求导的的一个非常强大的范例。然而对于大规模的神经网络， 原始的 autograd 可能有点太低级了。</p><p>当构建神经网络时，我们经常想到把计算组织维层级结构，其中一些具有可学习的参数，这些参数将在学习期间被优化。</p><p>在 Tensorflow 中，像 <code>Keras</code>， <code>TensorFlow-Slim</code> 和 <code>TFLearn</code> 这样的软件包提供了对原始图的更高级的抽象，这对于构建神经网络很有用。</p><p>在 PyTorch 中， <code>nn</code> 包提供了同样的功能。 <code>nn</code> 包提供了一组模块，他们大致相当于神经网络层。一个模块接收一个输入变量并计算输出向量，也可能保存内部状态，如包含可学习的参数。 <code>nn</code> 包还定义了一组训练神经网络时有用的损失函数。</p><p>在这个例子中我们使用 <code>nn</code> 包来实现我们的两层网络：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables.</span></span><br><span class="line">x = Variable(torch.randn(N, D_in))</span><br><span class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the nn package to define our model as a sequence of layers. nn.Sequential</span></span><br><span class="line"><span class="comment"># is a Module which contains other Modules, and applies them in sequence to</span></span><br><span class="line"><span class="comment"># produce its output. Each Linear Module computes output from input using a</span></span><br><span class="line"><span class="comment"># linear function, and holds internal Variables for its weight and bias.</span></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(D_in, H),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(H, D_out)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The nn package also contains definitions of popular loss functions; in this</span></span><br><span class="line"><span class="comment"># case we will use Mean Squared Error (MSE) as our loss function.</span></span><br><span class="line">loss_fn = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-4</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y by passing x to the model. Module objects</span></span><br><span class="line">    <span class="comment"># override the __call__ operator so you can call them like functions. When</span></span><br><span class="line">    <span class="comment"># doing so you pass a Variable of input data to the Module and it produces</span></span><br><span class="line">    <span class="comment"># a Variable of output data.</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss. We pass Variables containing the predicted and true</span></span><br><span class="line">    <span class="comment"># values of y, and the loss function returns a Variable containing the</span></span><br><span class="line">    <span class="comment"># loss.</span></span><br><span class="line">    loss = loss_fn(y_pred, y)</span><br><span class="line">    print(t, loss.data[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero the gradients before running the backward pass.</span></span><br><span class="line">    model.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass: compute gradient of the loss with respect to all the learnable</span></span><br><span class="line">    <span class="comment"># parameters of the model. Internally, the parameters of each Module are stored</span></span><br><span class="line">    <span class="comment"># in Variables with requires_grad=True, so this call will compute gradients for</span></span><br><span class="line">    <span class="comment"># all learnable parameters in the model.</span></span><br><span class="line"></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">        param.data -= learning_rate * param.grad.data</span><br></pre></td></tr></table></figure></p><h2 id="PyTorch：-optim"><a href="#PyTorch：-optim" class="headerlink" title="PyTorch： optim"></a>PyTorch： optim</h2><p>到目前为止，我们已经更新了我们模型的权重，通过手动改变需要学习的参数变量的 <code>.data</code> 成员。这对于像随机梯度下降这种简单的优化算法来说不是很困难，但是在实际训练神经网络时，我们常常使用更复杂的优化算法，如 AdaGrad，RMSProp， Adam等。</p><p>PyTorch 的 <code>optim</code> 包抽象了优化算法的思想，并提供了常用算法的实现。</p><p>在这个例子中，我们讲使用 <code>nn</code> 包来重新定义我们之前的模型，但是我们讲使用 <code>optim</code> 包提供的 Adam 算法来优化我们的模型：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span>  torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables.</span></span><br><span class="line">x = Variable(torch.randn(N, D_in))</span><br><span class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the nn package to define our model and loss function</span></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(D_in, H),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(H, D_out)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">loss_fn = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the optim package to define an Optimizer that will update the weights of</span></span><br><span class="line"><span class="comment"># the model for us. Here we will use Adam; the optim package contains many other</span></span><br><span class="line"><span class="comment"># optimization algoriths. The first argument to the Adam constructor tells the</span></span><br><span class="line"><span class="comment"># optimizer which Variables it should update.</span></span><br><span class="line">learning_rate = <span class="number">1e-4</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y by passing x to the model.</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y)</span><br><span class="line">    print(t, loss.data[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Before the backward pass, use the optimizer object to zero all of the</span></span><br><span class="line">    <span class="comment"># gradients for the variables it will update (which are the learnable weights</span></span><br><span class="line">    <span class="comment"># of the model)</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass: compute gradient of the loss with respect to model</span></span><br><span class="line">    <span class="comment"># parameters</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calling the step function on an Optimizer makes an update to its</span></span><br><span class="line">    <span class="comment"># parameters</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure></p><h2 id="PyTorch：-自定义模块"><a href="#PyTorch：-自定义模块" class="headerlink" title="PyTorch： 自定义模块"></a>PyTorch： 自定义模块</h2><p>有时你想要指定比现有模块序列更复杂的模型；对于这种情况，你可以通过继承 <code>nn.Module</code> 来定义自己的模块，并实现 <code>forward</code> 函数，他接收一个输入变量，并使用其他模块或 autograd 操作符来生成输出变量。</p><p>在这个例子中，我们实现了一个两层网络来作为自定义模块：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoLayerNet</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, D_in, H, D_out)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the constructor we instantiate two nn.Linear modules and assign them as member variables.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(TwoLayerNet, self).__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(D_in, H)</span><br><span class="line">        self.linear2 = torch.nn.Linear(H, D_out)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the forward function we accept a Variable of input data and we must return</span></span><br><span class="line"><span class="string">        a Variable of output data. We can use Modules defined in the constructor as</span></span><br><span class="line"><span class="string">        well as arbitrary operators on Variables.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        h_relu = self.linear1(x).clamp(min=<span class="number">0</span>)</span><br><span class="line">        y_pred = self.linear2(h_relu)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables</span></span><br><span class="line">x = Variable(torch.randn(N, D_in))</span><br><span class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct our model by instantiating the class defined above</span></span><br><span class="line">model = TwoLayerNet(D_in, H, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct our loss function and an Optimizer. The call to model.parameters()</span></span><br><span class="line"><span class="comment"># in the SGD constructor will contain the learnable parameters of the two</span></span><br><span class="line"><span class="comment"># nn.Linear modules which are members of the model.</span></span><br><span class="line">criterion = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-4</span>)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: Compute predicted y by passing x to model</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = criterion(y_pred, y)</span><br><span class="line">    print(t, loss.data[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero gradients, perform a backward pass, and update the weights.</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure></p><h2 id="PyTorch-控制流-共享权重"><a href="#PyTorch-控制流-共享权重" class="headerlink" title="PyTorch: 控制流 + 共享权重"></a>PyTorch: 控制流 + 共享权重</h2><p>作为动态图和权值共享的例子，我们实现了一个不同的模型： 一个全连接的 ReLU 网络，每次前向传播时， 从1到4随机选择一个数来作为隐藏层的层数。=，多次重复使用相同的权重来计算最内层的隐藏层。</p><p>对于这个模型，我们可以使用普通的 Python 控制流来实现循环，在定义前向传播时，通过简单的重复使用同一个模块，我们可以实现最内层之间的权重共享。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DynamicNet</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, D_in, H, D_out)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the constructor we construct three nn.Linear instances that we will use</span></span><br><span class="line"><span class="string">        in the forward pass.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(DynamicNet, self).__init__()</span><br><span class="line">        self.input_linear = torch.nn.Linear(D_in, H)</span><br><span class="line">        self.middle_linear = torch.nn.Linear(H, H)</span><br><span class="line">        self.output_linear = torch.nn.Linear(H, D_out)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3</span></span><br><span class="line"><span class="string">        and reuse the middle_linear Module that many times to compute hidden layer</span></span><br><span class="line"><span class="string">        representations.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Since each forward pass builds a dynamic computation graph, we can use normal</span></span><br><span class="line"><span class="string">        Python control-flow operators like loops or conditional statements when</span></span><br><span class="line"><span class="string">        defining the forward pass of the model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Here we also see that it is perfectly safe to reuse the same Module many</span></span><br><span class="line"><span class="string">        times when defining a computational graph. This is a big improvement from Lua</span></span><br><span class="line"><span class="string">        Torch, where each Module could be used only once.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        h_relu = self.input_linear(x).clamp(min=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(random.randint(<span class="number">0</span>, <span class="number">3</span>)):</span><br><span class="line">            h_relu = self.middle_linear(h_relu).clamp(min=<span class="number">0</span>)</span><br><span class="line">        y_pred = self.output_linear(h_relu)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables</span></span><br><span class="line">x = Variable(torch.randn(N, D_in))</span><br><span class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct our model by instantiating the class defined above</span></span><br><span class="line">model = DynamicNet(D_in, H, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct our loss function and an Optimizer. Training this strange model with</span></span><br><span class="line"><span class="comment"># vanilla stochastic gradient descent is tough, so we use momentum</span></span><br><span class="line">criterion = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-4</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: Compute predicted y by passing x to the model</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = criterion(y_pred, y)</span><br><span class="line">    print(t, loss.data[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero gradients, perform a backward pass, and update the weights.</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure></p><h1 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h1><p>以上实例的代码地址如下：</p><p><strong>Tensors</strong></p><p><a href="http://pytorch.org/tutorials/beginner/examples_tensor/two_layer_net_numpy.html#sphx-glr-beginner-examples-tensor-two-layer-net-numpy-py" target="_blank" rel="noopener">Warm-up: numpy</a></p><p><a href="http://pytorch.org/tutorials/beginner/examples_tensor/two_layer_net_tensor.html#sphx-glr-beginner-examples-tensor-two-layer-net-tensor-py" target="_blank" rel="noopener">PyTorch: Tensors</a></p><p><strong>autograd</strong><br><a href="http://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_autograd.html#sphx-glr-beginner-examples-autograd-two-layer-net-autograd-py" target="_blank" rel="noopener">PyTorch: Variables and autograd</a></p><p><a href="http://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html#sphx-glr-beginner-examples-autograd-two-layer-net-custom-function-py" target="_blank" rel="noopener">PyTorch: Defining new autograd functions</a></p><p><a href="http://pytorch.org/tutorials/beginner/examples_autograd/tf_two_layer_net.html#sphx-glr-beginner-examples-autograd-tf-two-layer-net-py" target="_blank" rel="noopener">Tensorflow: Static Graphs</a></p><p><strong>nn Module</strong><br><a href="http://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_nn.html#sphx-glr-beginner-examples-nn-two-layer-net-nn-py" target="_blank" rel="noopener">PyTorch: nn</a></p><p><a href="http://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html#sphx-glr-beginner-examples-nn-two-layer-net-optim-py" target="_blank" rel="noopener">PyTorch: optim</a></p><p><a href="http://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html#sphx-glr-beginner-examples-nn-two-layer-net-module-py" target="_blank" rel="noopener">PyTorch: Custom nn Modules</a></p><p><a href="http://pytorch.org/tutorials/beginner/examples_nn/dynamic_net.html#sphx-glr-beginner-examples-nn-dynamic-net-py" target="_blank" rel="noopener">PyTorch: Control FLow + Weights Sharing</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2017/12/27/Learning-PyTorch-with-Examples/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github地址&lt;/a&gt;&lt;br&gt;&lt;a href=
      
    
    </summary>
    
      <category term="PyTorch" scheme="http://yoursite.com/categories/PyTorch/"/>
    
    
      <category term="PyTorch" scheme="http://yoursite.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Android 四大组件之 Service</title>
    <link href="http://yoursite.com/2017/12/16/Android-Service/"/>
    <id>http://yoursite.com/2017/12/16/Android-Service/</id>
    <published>2017-12-16T05:20:33.000Z</published>
    <updated>2018-03-20T14:02:04.000Z</updated>
    
    <content type="html"><![CDATA[<p><code>Service</code> 是 Android 应用的四大组件之一(其余的为<code>Activity</code>, <code>Broadcatst Receiver</code>, <code>Content Provider</code>),它不同于 <code>Activity</code>, 它有两个特点:</p><ol><li><strong>在后台执行长时间运行的操作</strong></li><li><strong>没有用户界面</strong></li></ol><p>Android 应用中的任何组件都可以启动和绑定 <code>Service</code>,有2种形式的 <code>Service</code>:</p><ol><li>启动式(Started)</li></ol><blockquote><p>当一个组件(比如 <code>Activity</code>)通过调用<code>startService()</code>时,就会启动一个 <code>Service</code>。一旦启动,这个 <code>service</code> 就会在后台无限执行下去,即使创建这个 <code>Service</code> 的组件已经销毁。通常启动式(started)的 <code>Service</code> 执行单一操作切不给调用者返回结果。比如,通过网络下载文件。<strong>注意</strong>,当操作执行完后,应该手动停止 <code>Service</code>。</p></blockquote><ol><li>绑定式(Bound)</li></ol><blockquote><p>当一个组件通过调用 <code>bindService()</code> 时,就会绑定到一个 <code>Service</code>。一个绑定的 <code>Service</code> 类似于”客户端-服务器”的结构,它允许组件与它进行交互,还可以进行跨进程交互(IPC)。它不同于启动式的 <code>Service</code> 会在后台无限执行知道手动被停止,绑定式的 <code>Service</code>只要有组件与之绑定,该 <code>Service</code> 就会运行,反之,则会被自动销毁。</p></blockquote><p>任何一个 <code>Service</code> 都可以同时以这两种方式运行，关键在于你是否实现了以下两个方法：</p><ol><li><code>onStartCommand()</code>: 允许组件启动该 ’Service‘；</li><li><code>onBind()</code>： 允许组件与之绑定；</li></ol><p><strong>注意</strong>：<code>Service</code> 和 <code>Activity</code> 一样，运行在主线程（UI线程）中， 它不会创建独立的线程，因此，当在 <code>Service</code> 中执行 CPU 密集操作或者阻塞操作时（比如网络或MP3播放），应该在 <code>service</code> 中创建一个新的线程，来避免出现 “Application Not Responding（ANR）”。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;code&gt;Service&lt;/code&gt; 是 Android 应用的四大组件之一(其余的为&lt;code&gt;Activity&lt;/code&gt;, &lt;code&gt;Broadcatst Receiver&lt;/code&gt;, &lt;code&gt;Content Provider&lt;/code&gt;),它不同于
      
    
    </summary>
    
      <category term="Android" scheme="http://yoursite.com/categories/Android/"/>
    
    
      <category term="Android" scheme="http://yoursite.com/tags/Android/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch:Torch 用户</title>
    <link href="http://yoursite.com/2017/12/03/PyTorch-for-former-Torch-users/"/>
    <id>http://yoursite.com/2017/12/03/PyTorch-for-former-Torch-users/</id>
    <published>2017-12-03T03:12:02.000Z</published>
    <updated>2018-03-20T14:03:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>此教程翻译自 <code>PyTorch</code> <a href="http://pytorch.org/tutorials/beginner/former_torchies_tutorial.html" target="_blank" rel="noopener">官方教程</a></p><p><strong>作者:</strong><a href="http://soumith.ch/" target="_blank" rel="noopener">Soumith Chintala</a></p><p>在这个教程中,你将学习如下知识:</p><ol><li>使学习用 <code>torch</code> 张量, 和与 <code>(Lua) Torch</code> 的不同;</li><li>使用自动求导包;</li><li><p>构建神经网络</p><ul><li>构建卷积网络(‘ConvNet`)</li><li>构建 循环神经网络(`Recurrent Net)</li></ul></li><li>使用多GPU</li></ol><h1 id="一-张量-Tensors"><a href="#一-张量-Tensors" class="headerlink" title="一. 张量(Tensors)"></a>一. 张量(<code>Tensors</code>)</h1><p><code>PyTorch</code> 中的张量和 <code>Torch</code> 中的张量完全一样的.</p><p>创建一个未初始化的大小为 5*7 的张量:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.FloatTensor(<span class="number">5</span>, <span class="number">7</span>)</span><br></pre></td></tr></table></figure></p><p>用均值为 0, 方差为 1 的正态分布随机初始化一个张量:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">5</span>, <span class="number">7</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(a.size())</span><br></pre></td></tr></table></figure></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> 0.6608  0.0584 -1.9318  1.6068 -1.0049  1.3115 -0.3893</span><br><span class="line"> 0.5118  1.7707 -0.3681  0.2345 -0.0295  2.0229  0.2815</span><br><span class="line"> 1.2753 -1.2878  1.3112 -0.0965  1.4672  0.5731  2.7306</span><br><span class="line"> 1.2824  0.5972  1.4461  1.1568  0.1307  0.0381 -1.1927</span><br><span class="line"> 0.7658  0.1555  0.2385  1.0136 -0.6666 -0.3318 -0.8383</span><br><span class="line">[torch.FloatTensor of size 5x7]</span><br><span class="line">torch.Size([5, 7])</span><br></pre></td></tr></table></figure></p><p><strong>注意</strong><br><code>torch.Size</code> 本质上是一个元组,所以它支持和元组一样的操作.</p><h2 id="Inplace-Out-of-place"><a href="#Inplace-Out-of-place" class="headerlink" title="Inplace/Out-of-place"></a>Inplace/Out-of-place</h2><p>第一个不同就是张量上的所有就地(Inplace)操作都有一个’_’后缀,比如,<code>add</code>不是一个原地(Out-of-place)操作版本,<code>add_</code>是一个原地操作版本.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a.fill_(<span class="number">3.5</span>)</span><br><span class="line"><span class="comment"># a has now been filled with the value 3.5</span></span><br><span class="line"></span><br><span class="line">b = a.add(<span class="number">4.0</span>)</span><br><span class="line"><span class="comment"># is still filled with 3.5</span></span><br><span class="line"><span class="comment"># new tensor b is returned with 3.5 + 4.0 = 7.0</span></span><br><span class="line">print(a, b)</span><br></pre></td></tr></table></figure></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">3.5000  3.5000  3.5000  3.5000  3.5000  3.5000  3.5000</span><br><span class="line"> 3.5000  3.5000  3.5000  3.5000  3.5000  3.5000  3.5000</span><br><span class="line"> 3.5000  3.5000  3.5000  3.5000  3.5000  3.5000  3.5000</span><br><span class="line"> 3.5000  3.5000  3.5000  3.5000  3.5000  3.5000  3.5000</span><br><span class="line"> 3.5000  3.5000  3.5000  3.5000  3.5000  3.5000  3.5000</span><br><span class="line">[torch.FloatTensor of size 5x7]</span><br><span class="line"></span><br><span class="line"> 7.5000  7.5000  7.5000  7.5000  7.5000  7.5000  7.5000</span><br><span class="line"> 7.5000  7.5000  7.5000  7.5000  7.5000  7.5000  7.5000</span><br><span class="line"> 7.5000  7.5000  7.5000  7.5000  7.5000  7.5000  7.5000</span><br><span class="line"> 7.5000  7.5000  7.5000  7.5000  7.5000  7.5000  7.5000</span><br><span class="line"> 7.5000  7.5000  7.5000  7.5000  7.5000  7.5000  7.5000</span><br><span class="line">[torch.FloatTensor of size 5x7]</span><br></pre></td></tr></table></figure></p><p>某些操作,比如 <code>narrow</code> 没有原地操作版本, 因此 <code>narrow_</code>不存在.<br>同样地,某些操作, 比如 <code>fill_</code> 没有 <code>out-of-place</code> 版本,所以 <code>.fill</code> 不存在.</p><h2 id="零索引-Zero-Indexing"><a href="#零索引-Zero-Indexing" class="headerlink" title="零索引(Zero Indexing)"></a>零索引(Zero Indexing)</h2><p>另一个不同是 PyTorch 中的张量是从 0 开始索引,而在 lua 中, 它是从1 开始索引.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b = a[<span class="number">0</span>, <span class="number">3</span>] <span class="comment"># 选择 a 中的第一行,第四列的元素</span></span><br></pre></td></tr></table></figure></p><p>张量也可以用 Python 的切片来索引<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b = a[:, <span class="number">3</span>:<span class="number">5</span>]</span><br></pre></td></tr></table></figure></p><h2 id="无驼峰写法"><a href="#无驼峰写法" class="headerlink" title="无驼峰写法"></a>无驼峰写法</h2><p>下一个微小的不同是所有的函数现在不再是驼峰命名.比如 <code>indexAdd</code> 现在写为 <code>index_add_</code>.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1  1  1  1  1</span><br><span class="line"> 1  1  1  1  1</span><br><span class="line"> 1  1  1  1  1</span><br><span class="line"> 1  1  1  1  1</span><br><span class="line"> 1  1  1  1  1</span><br><span class="line">[torch.FloatTensor of size 5x5]</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">z = torch.Tensor(<span class="number">5</span>, <span class="number">2</span>)</span><br><span class="line">z[:, <span class="number">0</span>] = <span class="number">10</span></span><br><span class="line">z[:, <span class="number">1</span>] = <span class="number">100</span></span><br><span class="line">print(z)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">10  100</span><br><span class="line">  10  100</span><br><span class="line">  10  100</span><br><span class="line">  10  100</span><br><span class="line">  10  100</span><br><span class="line">[torch.FloatTensor of size 5x2]</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x.index_add_(<span class="number">1</span>, torch.LongTensor([<span class="number">4</span>, <span class="number">0</span>]), z)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">101    1    1    1   11</span><br><span class="line"> 101    1    1    1   11</span><br><span class="line"> 101    1    1    1   11</span><br><span class="line"> 101    1    1    1   11</span><br><span class="line"> 101    1    1    1   11</span><br><span class="line">[torch.FloatTensor of size 5x5]</span><br></pre></td></tr></table></figure></p><h2 id="numpy-桥"><a href="#numpy-桥" class="headerlink" title="numpy 桥"></a>numpy 桥</h2><p>把一个torch张量转换为numpy数组或者反过来都是很简单的。Torch张量和numpy数组将共享潜在的内存，改变其中一个也将改变另一个。</p><h3 id="把-Torch-张量转换为-numpy-数组"><a href="#把-Torch-张量转换为-numpy-数组" class="headerlink" title="把 Torch 张量转换为 numpy 数组"></a>把 Torch 张量转换为 numpy 数组</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">5</span>)</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line"> 1</span><br><span class="line"> 1</span><br><span class="line"> 1</span><br><span class="line"> 1</span><br><span class="line">[torch.FloatTensor of size 5]</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = a.numpy()</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ 1.  1.  1.  1.  1.]</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a.add_(1)</span><br><span class="line">print(a)</span><br><span class="line">print(b)    # 观察 numpy 数组的值如何在变化</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">2</span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line">[torch.FloatTensor of size 5]</span><br><span class="line"></span><br><span class="line">[ 2.  2.  2.  2.  2.]</span><br></pre></td></tr></table></figure></p><h3 id="把-numpy-数组转换为-torch-张量"><a href="#把-numpy-数组转换为-torch-张量" class="headerlink" title="把 numpy 数组转换为 torch 张量"></a>把 numpy 数组转换为 torch 张量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">np.add(a, <span class="number">1</span>, out=a)</span><br><span class="line">print(a)</span><br><span class="line">print(b)  <span class="comment"># 观察改变numpy数组的值如何在自动改变torch张量的值</span></span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[ 2.  2.  2.  2.  2.]</span><br><span class="line"></span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line"> 2</span><br><span class="line">[torch.DoubleTensor of size 5]</span><br></pre></td></tr></table></figure></p><p>除了 <code>CharTensor</code>, 所有在 CPU 上的张量都支持在 numpy 之间来回转换.</p><h2 id="CUDA-张量"><a href="#CUDA-张量" class="headerlink" title="CUDA 张量"></a>CUDA 张量</h2><p>在 PyTorch 中, CUDA 张量是很容易实现的,并且从cpu 到 GPU 来转换一个 CUDA 张量将保存它的潜在类型.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 让我们在只有CUDA可用时才运行这个单元</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    <span class="comment"># 创建一个LongTensor,并把它转换为 GPU 版的 torch.cuda.LongTensor</span></span><br><span class="line">    a = torch.LongTensor(<span class="number">10</span>).fill_(<span class="number">3</span>).cuda()</span><br><span class="line">    print(type(a))</span><br><span class="line">    b = a.cpu()</span><br><span class="line">    <span class="comment"># 把它转换为 cpu 版本,还原为torch.LongTensor</span></span><br></pre></td></tr></table></figure></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;class &apos;torch.cuda.LongTensor&apos;&gt;</span><br></pre></td></tr></table></figure></p><p><strong>脚本总运行时间:</strong> 0.002 秒</p><p><a href="http://pytorch.org/tutorials/_downloads/tensor_tutorial1.py" target="_blank" rel="noopener">Python 源码</a></p><p><a href="http://pytorch.org/tutorials/_downloads/tensor_tutorial1.ipynb" target="_blank" rel="noopener">Jupyter 源码</a></p><h1 id="自动求导-Autograd"><a href="#自动求导-Autograd" class="headerlink" title="自动求导(Autograd)"></a>自动求导(Autograd)</h1><p>自动求导是 torch 中自动微分的核心包,它使用一个基于磁带的系统来进行自动微分.</p><p>在前向阶段,这个磁带会记住所有执行的操作,在反向阶段, 它重新执行这些操作.</p><h2 id="变量-Variable"><a href="#变量-Variable" class="headerlink" title="变量(Variable)"></a>变量(Variable)</h2><p>在 <code>autograd</code> 中,我们引入一个 <code>Variable</code> 类, 它是一个 <code>Tensor</code> 的包装器.你可以通过 <code>.data</code> 属性来访问原始的张量,在执行完反向过程后,关于这个变量的梯度被累积到 <code>.grad</code> 属性中.</p><p>![Variable][/images/Variable.png]</p><p>还有一个类对于自动求导的实验非常重要,即 <code>Function</code>类.<br><code>Variable</code> 和 <code>Function</code> 是相互连接的并构成一个无环图来编码整个计算历史.每个 <code>Variable</code> 有一个 <code>grad_fn</code> 属性,它指向创建该变量的一个<code>Function</code>,用户自己创建的变量除外,它的<code>grad_fn</code>属性为None.</p><p>如果你想计算导数,你可以在一个 <code>Variable</code> 上调用 <code>.backward()</code>. 如果 <code>Variable</code> 是一个标量(他只有一个元素),你不必给 <code>backward()</code>指定任何的参数,然而如果他拥有多个元素, 你需要指定一个 <code>grad_output</code> 参数,他是一个和该 <code>Variable</code> 大小相匹配的张量.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span>  torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line">x = Variable(torch.ones(<span class="number">2</span>, <span class="number">2</span>), requires_grad=<span class="keyword">True</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Variable containing:</span><br><span class="line"> 1  1</span><br><span class="line"> 1  1</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1  1</span><br><span class="line"> 1  1</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">None</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x.grad_fn)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">None</span><br></pre></td></tr></table></figure><p>在 x 上执行一个操作:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = x + 2</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Variable containing:</span><br><span class="line"> 3  3</span><br><span class="line"> 3  3</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br></pre></td></tr></table></figure></p><p>y 作为一个操作的结果, 因此它有<code>grad_fn</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.grad_fn)</span><br></pre></td></tr></table></figure></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;torch.autograd.function.AddConstantBackward object at 0x7faa6602d7c8&gt;</span><br></pre></td></tr></table></figure></p><p>在 y 上执行更多的操作:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line">print(z, out)</span><br></pre></td></tr></table></figure></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Variable containing:</span><br><span class="line"> 27  27</span><br><span class="line"> 27  27</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br><span class="line"> Variable containing:</span><br><span class="line"> 27</span><br><span class="line">[torch.FloatTensor of size 1]</span><br></pre></td></tr></table></figure></p><h2 id="梯度-Gradients"><a href="#梯度-Gradients" class="headerlink" title="梯度(Gradients)"></a>梯度(Gradients)</h2><p>现在我们进行反向机选,并打印出梯度 $d(out)/dx$<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">out.backward()</span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure></p><p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Variable containing:</span><br><span class="line"> 4.5000  4.5000</span><br><span class="line"> 4.5000  4.5000</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br></pre></td></tr></table></figure></p><p><strong>未完待续…</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;此教程翻译自 &lt;code&gt;PyTorch&lt;/code&gt; &lt;a href=&quot;http://pytorch.org/tutorials/beginner/former_torchies_tutorial.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
      
    
    </summary>
    
      <category term="PyTorch" scheme="http://yoursite.com/categories/PyTorch/"/>
    
    
      <category term="PyTorch" scheme="http://yoursite.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>如何在 hexo 中支持 Mathjax？</title>
    <link href="http://yoursite.com/2017/11/29/hexo-support-mathjax/"/>
    <id>http://yoursite.com/2017/11/29/hexo-support-mathjax/</id>
    <published>2017-11-29T12:52:28.000Z</published>
    <updated>2018-03-20T14:02:25.178Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ranmaosong.github.io/2017/11/29/hexo-support-mathjax/" target="_blank" rel="noopener">Github 地址</a><br><a href="http://www.jianshu.com/p/e8d433a2c5b7" target="_blank" rel="noopener">简书地址</a><br><a href="http://blog.csdn.net/u014630987/article/details/78670258" target="_blank" rel="noopener">CSDN地址</a></p><p>在 hexo 中，你会发现我们不能用 <code>Latex</code> 语法来书写数学公式，这对于书写学术博客来说是很大的不便，因为我们会经常碰到很多的数学公式推导，但是我们可以通过安装第三方库来解决这一问题。</p><h2 id="第一步：-使用Kramed代替-Marked"><a href="#第一步：-使用Kramed代替-Marked" class="headerlink" title="第一步： 使用Kramed代替 Marked"></a>第一步： 使用Kramed代替 Marked</h2><p><code>hexo</code> 默认的渲染引擎是 <code>marked</code>，但是 <code>marked</code> 不支持 <code>mathjax</code>。 <code>kramed</code> 是在 <code>marked</code> 的基础上进行修改。我们在工程目录下执行以下命令来安装 <code>kramed</code>.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure></p><p>然后，更改<your-project-dir>/node_modules/hexo-renderer-kramed/lib/renderer.js，更改：<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Change inline math rule</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">formatText</span>(<span class="params">text</span>) </span>&#123;</span><br><span class="line">    <span class="comment">// Fit kramed's rule: $$ + \1 + $$</span></span><br><span class="line">    <span class="keyword">return</span> text.replace(<span class="regexp">/`\$(.*?)\$`/g</span>, <span class="string">'$$$$$1$$$$'</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></your-project-dir></p><p>为：<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Change inline math rule</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">formatText</span>(<span class="params">text</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> text;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="第二步-停止使用-hexo-math"><a href="#第二步-停止使用-hexo-math" class="headerlink" title="第二步: 停止使用 hexo-math"></a>第二步: 停止使用 hexo-math</h2><p>首先，如果你已经安装 <code>hexo-math</code>, 请卸载它：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-math --save</span><br></pre></td></tr></table></figure></p><p>然后安装 <a href="https://github.com/phoenixcw/hexo-renderer-mathjax" target="_blank" rel="noopener">hexo-renderer-mathjax</a> 包：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-renderer-mathjax --save</span><br></pre></td></tr></table></figure></p><h2 id="第三步-更新-Mathjax-的-CDN-链接"><a href="#第三步-更新-Mathjax-的-CDN-链接" class="headerlink" title="第三步: 更新 Mathjax 的 CDN 链接"></a>第三步: 更新 Mathjax 的 CDN 链接</h2><p>首先，打开<path-to-your-project>/node_modules/hexo-renderer-mathjax/mathjax.html</path-to-your-project></p><p>然后，把<code>&lt;script&gt;</code>更改为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure></p><h2 id="第四步-更改默认转义规则"><a href="#第四步-更改默认转义规则" class="headerlink" title="第四步: 更改默认转义规则"></a>第四步: 更改默认转义规则</h2><p>因为 <code>hexo</code> 默认的转义规则会将一些字符进行转义，比如 <code>_</code> 转为 <code>&lt;em&gt;</code>, 所以我们需要对默认的规则进行修改.<br>首先， 打开&lt;path-to-your-project/node_modules/kramed/lib/rules、inline.js,</p><p>然后，把:<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">escape</span>: <span class="regexp">/^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/</span>,</span><br></pre></td></tr></table></figure></p><p>更改为:<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">escape</span>: <span class="regexp">/^\\([`*\[\]()# +\-.!_&gt;])/</span>,</span><br></pre></td></tr></table></figure></p><p>把<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">em: <span class="regexp">/^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br></pre></td></tr></table></figure></p><p>更改为:<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">em: <span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br></pre></td></tr></table></figure></p><h2 id="第五步-开启mathjax"><a href="#第五步-开启mathjax" class="headerlink" title="第五步: 开启mathjax"></a>第五步: 开启mathjax</h2><p>在主题 <code>_config.yml</code> 中开启 Mathjax， 找到 <code>mathjax</code> 字段添加如下代码：<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">mathjax:</span></span><br><span class="line"><span class="attr">    enable:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></p><p>这一步可选，在博客中开启 <code>Mathjax</code>，， 添加以下内容：<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: Testing Mathjax with Hexo</span><br><span class="line">category: Uncategorized</span><br><span class="line">date: 2017/05/03</span><br><span class="line">mathjax: true</span><br><span class="line">---</span><br></pre></td></tr></table></figure></p><p>通过以上步骤，我们就可以在 <code>hexo</code> 中使用 <code>Mathjax</code> 来书写数学公式。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://ranmaosong.github.io/2017/11/29/hexo-support-mathjax/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github 地址&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://w
      
    
    </summary>
    
      <category term="Hexo" scheme="http://yoursite.com/categories/Hexo/"/>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
  </entry>
  
</feed>
